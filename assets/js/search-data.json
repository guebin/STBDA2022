{
  
    
        "post0": {
            "title": "빅데이터분석 특강 기말고사",
            "content": "imports . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . %load_ext tensorboard . The tensorboard extension is already loaded. To reload it, use: %reload_ext tensorboard . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . 1. Fashion_mnist, DNN (30&#51216;) . (1) tf.keras.datasets.fashion_mnist.load_data()을 이용하여 fashion_mnist 자료를 불러온 뒤 아래의 네트워크를 이용하여 적합하라. . 평가지표로 accuracy를 이용할 것 | epoch은 10으로 설정할 것 | optimizer는 adam을 이용할 것 | . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node20&quot; &quot;x2&quot; -&gt; &quot;node20&quot; &quot;..&quot; -&gt; &quot;node20&quot; &quot;x784&quot; -&gt; &quot;node20&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;node1 &quot; &quot;node2&quot; -&gt; &quot;node1 &quot; &quot;...&quot; -&gt; &quot;node1 &quot; &quot;node20&quot; -&gt; &quot;node1 &quot; &quot;node1&quot; -&gt; &quot;node2 &quot; &quot;node2&quot; -&gt; &quot;node2 &quot; &quot;...&quot; -&gt; &quot;node2 &quot; &quot;node20&quot; -&gt; &quot;node2 &quot; &quot;node1&quot; -&gt; &quot;... &quot; &quot;node2&quot; -&gt; &quot;... &quot; &quot;...&quot; -&gt; &quot;... &quot; &quot;node20&quot; -&gt; &quot;... &quot; &quot;node1&quot; -&gt; &quot;node30 &quot; &quot;node2&quot; -&gt; &quot;node30 &quot; &quot;...&quot; -&gt; &quot;node30 &quot; &quot;node20&quot; -&gt; &quot;node30 &quot; label = &quot;Layer 2: relu&quot; } subgraph cluster_4{ style=filled; color=lightgrey; &quot;node1 &quot; -&gt; &quot;y10&quot; &quot;node2 &quot; -&gt; &quot;y10&quot; &quot;... &quot; -&gt; &quot;y10&quot; &quot;node30 &quot; -&gt; &quot;y10&quot; &quot;node1 &quot; -&gt; &quot;y1&quot; &quot;node2 &quot; -&gt; &quot;y1&quot; &quot;... &quot; -&gt; &quot;y1&quot; &quot;node30 &quot; -&gt; &quot;y1&quot; &quot;node1 &quot; -&gt; &quot;.&quot; &quot;node2 &quot; -&gt; &quot;.&quot; &quot;... &quot; -&gt; &quot;.&quot; &quot;node30 &quot; -&gt; &quot;.&quot; label = &quot;Layer 3: softmax&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: relu cluster_3 Layer 2: relu cluster_4 Layer 3: softmax x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node20 node20 x1&#45;&gt;node20 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node20 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node20 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node20 node1 node1 node1&#45;&gt;node1 node2 node2 node1&#45;&gt;node2 ... ... node1&#45;&gt;... node30 node30 node1&#45;&gt;node30 node2&#45;&gt;node1 node2&#45;&gt;node2 node2&#45;&gt;... node2&#45;&gt;node30 ...&#45;&gt;node1 ...&#45;&gt;node2 ...&#45;&gt;... ...&#45;&gt;node30 node20&#45;&gt;node1 node20&#45;&gt;node2 node20&#45;&gt;... node20&#45;&gt;node30 y10 y10 node1 &#45;&gt;y10 y1 y1 node1 &#45;&gt;y1 . . node1 &#45;&gt;. node2 &#45;&gt;y10 node2 &#45;&gt;y1 node2 &#45;&gt;. ... &#45;&gt;y10 ... &#45;&gt;y1 ... &#45;&gt;. node30 &#45;&gt;y10 node30 &#45;&gt;y1 node30 &#45;&gt;. (2) (1)에서 적합된 네트워크를 이용하여 test data의 accuracy를 구하라. . (3) train set에서 20%의 자료를 validation 으로 분리하여 50에폭동안 학습하라. 텐서보드를 이용하여 train accuracy와 validation accuracy를 시각화 하고 결과를 해석하라. 오버피팅이라고 볼 수 있는가? . (4) (3)에서 적합된 네트워크를 이용하여 test data의 accuracy를 구하라. (2)의 결과와 비교하라. . (5) 조기종료기능을 이용하여 (3)의 네트워크를 다시 학습하라. 학습결과를 텐서보드를 이용하여 시각화 하라. . patience=3 으로 설정할 것 | . 2. Fashion_mnist, CNN (30&#51216;) . (1) tf.keras.datasets.fashion_mnist.load_data()을 이용하여 fashion_mnist 자료를 불러온 뒤 아래의 네트워크를 이용하여 적합하라. . 이때 n1=6, n2=16, n3=120 으로 설정한다, 드랍아웃비율은 20%로 설정한다. | net.summary()를 출력하여 설계결과를 확인하라. | . . (2) n1=(6,64,128), n2=(16,256)에 대하여 test set의 loss가 최소화되는 조합을 찾아라. 결과를 텐서보드로 시각화하는 코드를 작성하라. . epoc은 3회로 한정한다. | validation_split은 0.2로 설정한다. | . 3. CIFAR10 (30&#51216;) . tf.keras.datasets.cifar10.load_data()을 이용하여 CIFAR10을 불러온 뒤 적당한 네트워크를 사용하여 적합하라. . 결과를 텐서보드로 시각화할 필요는 없다. | 자유롭게 모형을 설계하여 적합하라. | test set의 accuracy가 70%이상인 경우만 정답으로 인정한다. | . 4. &#45796;&#51020;&#51012; &#51069;&#44256; &#47932;&#51020;&#50640; &#45813;&#54616;&#46972;. (10&#51216;) . (1) (1,128,128,3)의 shape을 가진 텐서가 tf.keras.layers.Conv2D(5,(2,2))으로 만들어진 커널을 통과할시 나오는 shape은? . (2) (1,24,24,16)의 shape을 가진 텐서가 tf.keras.layers.Flatten()을 통과할때 나오는 텐서의 shape은? . (3)-(4) . 아래와 같은 모형을 고려하자. . $$y_i= beta_0 + sum_{k=1}^{5} beta_k cos(k t_i)+ epsilon_i$$ . 여기에서 $t=(t_1, dots,t_{1000})=$ np.linspace(0,5,1000) 이다. 그리고 $ epsilon_i sim i.i.d~ N(0, sigma^2)$, 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자. ($ beta_0, dots, beta_5$의 참값은 각각 -2, 3, 1, 0, 0, 0.5 이다.) . np.random.seed(43052) t= np.linspace(0,5,1000) y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2 plt.plot(t,y,&#39;.&#39;,alpha=0.1) . [&lt;matplotlib.lines.Line2D at 0x7f6f63d9b0a0&gt;] . (3) 모형에 대한 설명 중 옳은 것을 모두 골라라. . (하영) 이 모형의 경우 MSEloss를 최소화하는 $ hat{ beta}_0, dots, hat{ beta}_5$를 구하는것은 최대우도함수를 최대화하는 $ hat{ beta}_0, dots, hat{ beta}_5$를 구하는 것과 같다. . (재인) 하영의 말이 옳은 이유는 오차항이 정규분포를 따른다는 가정이 있기 때문이다. . (서연) 이 모형에서 적절한 학습률이 선택되더라도 경사하강법을 이용하면 MSEloss를 최소화하는 $ hat{ beta}_0, dots, hat{ beta}_5$를 종종 구할 수 없는 문제가 생긴다. 왜냐하면 손실함수가 convex하지 않아서 local minimum에 빠질 위험이 있기 때문이다. . (규빈) 만약에 경사하강법 대신 확률적 경사하강법을 쓴다면 local minimum을 언제나 탈출 할 수 있다. 따라서 서연이 언급한 문제점은 생기지 않는다. . (4) 다음은 위의 모형을 학습한 결과이다. 옳게 해석한 것을 모두 고르시오. . y = y.reshape(1000,1) x1 = np.cos(t) x2 = np.cos(2*t) x3 = np.cos(3*t) x4 = np.cos(4*t) x5 = np.cos(5*t) X = tf.stack([x1,x2,x3,x4,x5],axis=1) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) net.fit(X,y,epochs=500,batch_size=100, validation_split=0.45,verbose=0) . &lt;keras.callbacks.History at 0x7f6ee0696380&gt; . plt.plot(y,&#39;.&#39;,alpha=0.1) plt.plot(net(X),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f6ee05367a0&gt;] . (재인) 처음 550개의 데이터만 학습하고 이후의 450개의 데이터는 학습하지 않고 validation으로 이용하였다. . (서연) validation에서의 적합결과가 좋지 않다. . (규빈) validation의 적합결과가 좋지 않기 때문에 오버피팅을 의심할 수 있다. 따라서 만약에 네트워크에 드랍아웃층을 추가한다면 오버피팅을 방지하는 효과가 있어 validation의 loss가 줄어들 것이다. . (하영) 이 모형의 경우 더 많은 epoch으로 학습한다면 train loss와 validation loss를 둘 다 줄일 수 있다. . (5) 다음을 잘 읽고 참 거짓을 판별하라. . Convolution은 선형변환이다. | CNN을 이용하면 언제나 손실함수를 MSEloss로 선택해야 한다. | CNN은 adam optimizer를 통해서만 최적화할 수 있다. | 이미지자료는 CNN을 이용하여서만 분석할 수 있으며 DNN으로는 분석불가능하다. | CNN은 칼라이미지일 경우에만 적용가능하다. | .",
            "url": "https://guebin.github.io/STBDA2022/2022/06/13/%EA%B8%B0%EB%A7%90%EA%B3%A0%EC%82%AC.html",
            "relUrl": "/2022/06/13/%EA%B8%B0%EB%A7%90%EA%B3%A0%EC%82%AC.html",
            "date": " • Jun 13, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "(14주차) 6월9일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . GAN (Generative Adversarial Network) . - 저자: 이안굿펠로우 (이름이 특이함. 좋은친구..) . 제가 추천한 딥러닝 교재의 저자 | 천재임 | 지도교수가 요수아 벤지오 | . - 논문 NIPS, 저는 이 논문 읽고 소름돋았어요.. . https://arxiv.org/abs/1406.2661 (현재시점, 38751회 인용되었음) | . - 최근 10년간 머신러닝 분야에서 가장 혁신적인 아이디어이다. (얀르쿤, 2014년 시점..) . - 무슨내용? 생성모형 . &#49373;&#49457;&#47784;&#54805;&#51060;&#46976;? (&#49772;&#50868; &#49444;&#47749;) . 만들수 없다면 이해하지 못한 것이다, 리처드 파인만 (천재 물리학자) . - 사진속에 들어있는 동물이 개인지 고양이인지 맞출수 있는 기계와 개와 고양이를 그릴수 있는 기계중 어떤것이 더 시각적보에 대한 이해가 깊다고 볼수 있는가? . - 진정으로 인공지능이 이미지를 이해했다면, 이미지를 만들수도 있어야 한다. $ to$ 이미지를 생성하는 모형을 만들어보자 $ to$ 성공 . . GAN&#51032; &#51025;&#50857;&#48516;&#50556; . - 내가 찍은 사진이 피카소의 화풍으로 표현된다면? . https://www.lgsl.kr/sto/stories/60/ALMA2020070001 | . - 퀸의 라이브에이드가 4k로 나온다면? . - 1920년대 서울의 모습이 칼라로 복원된다면? . - 딥페이크: 유명인의 가짜 포르노, 가짜뉴스, 협박(거짓기소) . - 게임영상 (파이널판타지) . - 거북이의 커버.. . - 너무 많아요..... . &#49373;&#49457;&#47784;&#54805;&#51060;&#46976;? &#53685;&#44228;&#54617;&#44284; &#48260;&#51204;&#51032; &#49444;&#47749; . 제한된 정보만으로 어떤 문제를 풀 떄, 그 과정에서 원래의 문제보다 일반적인 문제를 풀지 말고, 가능한 원래의 문제를 직접 풀어야한다. 배프닉 (SVM 창시자) . - 이미지 $ boldsymbol{x}$가 주어졌을 경우 라벨을 $y$라고 하자. . - 이미지를 보고 라벨을 맞추는 일은 $p(y| boldsymbol{x})$에 관심이 있다. . - 이미지를 생성하는 일은 $p( boldsymbol{x},y)$에 관심이 있는것이다. . - 데이터의 생성확률 $p( boldsymbol{x},y)$을 알면 클래스의 사후확률 $p(y| boldsymbol{x})$를 알 수 있음. (아래의 수식 참고) 하지만 역은 불가능 . $$p(y|x) = frac{p(x,y)}{p(x)} = frac{p(x,y)}{ sum_{y}p(x,y)} $$ . 즉 이미지를 생성하는일은 분류문제보다 더 어려운 일이라 해석가능 | . - 따라서 배프닉의 원리에 의하면 식별적 분류가 생성적 분류보다 바람직한 접근법이라 할 수 있음. . - 하지만 다양한 현실문제에서 생성모형이 유용할떄가 많다. . GAN&#51032; &#50896;&#47532; . - GAN은 생성모형중 하나임 . - GAN의 원리는 경찰과 위조지폐범이 서로 선의의(?) 경쟁을 통하여 서로 발전하는 모형으로 설명할 수 있다. . The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles. . - 서로 적대적인(adversarial) 네트워크(network)를 동시에 학습시켜 가짜이미지를 만든다(generate) . - 무식한 상황극.. . 위조범: 가짜돈을 만들어서 부자가 되어야지! (가짜돈을 그림) | 경찰: (위조범이 만든 돈을 보고) 이건 가짜다! | 위조범: 걸렸군.. 더 정교하게 만들어야지.. | 경찰: 이건 진짠가?... --&gt; 상사에게 혼남. 그것도 구분못하냐고 | 위조범: 더 정교하게 만들자.. | 경찰: 더 판별능력을 업그레이드 하자! | 반복.. | . - 굉장히 우수한 경찰조차도 진짜와 가짜를 구분하지 못할때(=진짜 이미지를 0.5의 확률로만 진짜라고 말할때 = 가짜 이미지를 0.5의 확률로만 가짜라고 말할때) 학습을 멈춘다. . &#44396;&#54788; . - 목표: 노이즈에서 mnist자료의 이미지를 생성하여 보자. . import . import tensorflow as tf import tensorflow.experimental.numpy as tnp import matplotlib.pyplot as plt import numpy as np from tqdm import tqdm . tnp.experimental_enable_numpy_behavior() . &#45936;&#51060;&#53552; . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() . Xreal = x_train.reshape(-1,784)/255 . &#50948;&#51312;&#51648;&#54224;&#48276;&#51032; &#49444;&#44228;: noise -&gt; &#44032;&#51676;&#51060;&#48120;&#51648;&#47484; &#47564;&#46308;&#50612;&#45236;&#45716; &#45348;&#53944;&#50892;&#53356;&#47484; &#47564;&#46308;&#51088;. . - 네트워크의 입력: 적당한 벡터 혹은 매트릭스에 노이즈 (랜덤으로 생성한 어떠한 숫자) 를 채운 것 . - 네트워크의 출력: (28,28) shape의 매트릭스 혹은 784개의 원소를 가지는 벡터 . net_counterfeiter= tf.keras.Sequential() net_counterfeiter.add(tf.keras.layers.Dense(256,activation=&#39;relu&#39;)) net_counterfeiter.add(tf.keras.layers.Dense(512,activation=&#39;relu&#39;)) net_counterfeiter.add(tf.keras.layers.Dense(1024,activation=&#39;relu&#39;)) net_counterfeiter.add(tf.keras.layers.Dense(784)) . 2022-06-09 12:43:21.685836: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . &#44221;&#52272;&#51032; &#49444;&#44228;: &#51652;&#51676;&#51060;&#48120;&#51648;&#45716; 1, &#44032;&#51676;&#51060;&#48120;&#51648;&#45716; 0&#51004;&#47196; &#54032;&#48324;&#54616;&#45716; DNN&#51012; &#47564;&#46308;&#51088;. . - 네트워크의 입력? . X: (28,28) shape의 matrix 혹은 784개의 원소를 가지는 벡터 | . - 네트워크의 출력? yhat . yhat은 진짜이미지일수록1, 가짜이미지일수록 0이 되어야 한다. (왜냐하면 y가 진짜이미지이면 1, 가짜이미지이면 0 이므로) | . net_police = tf.keras.Sequential() net_police.add(tf.keras.layers.Dense(1024,activation=&#39;relu&#39;)) net_police.add(tf.keras.layers.Dropout(0.3)) net_police.add(tf.keras.layers.Dense(512,activation=&#39;relu&#39;)) net_police.add(tf.keras.layers.Dropout(0.3)) net_police.add(tf.keras.layers.Dense(256,activation=&#39;relu&#39;)) net_police.add(tf.keras.layers.Dropout(0.3)) net_police.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) . &#51068;&#45800; &#49828;&#53664;&#47532;&#47484; &#44228;&#49549; &#51652;&#54665;&#54644;&#48372;&#44192;&#49845;&#45768;&#45796;. . - 진짜 이미지가 아래와 같이 있다. . plt.imshow(Xreal[1].reshape(28,28)) . &lt;matplotlib.image.AxesImage at 0x7f549c0232b0&gt; . - 이 이미지를 경찰이 봤습니다 -&gt; yhat이 나와야 하고, yhat $ approx$ 1 이어야 한다. (왜? 진짜 이미지니까) . policehat_from_realimage = net_police(Xreal) policehat_from_realimage . &lt;tf.Tensor: shape=(60000, 1), dtype=float32, numpy= array([[0.4638827 ], [0.45302874], [0.50195265], ..., [0.4384173 ], [0.48654714], [0.53714067]], dtype=float32)&gt; . 진짜 이미지이므로 위의 값들이 모두 1이어야함. 즉 yhat $ approx$ 1 이어야 좋은 것임 | 하지만 그렇지 못함 (배운것이 없는 무능한 경찰) | . - 이번에는 가짜 이미지를 경찰이 봤다고 생각하자. . (step1) 랜덤으로 아무숫자나 생성한다. . Noise1=tnp.random.randn(10).reshape(1,10) . (step2) 위조범을 시켜서 이미지를 생성시킨다. . Xfake1 = net_counterfeiter(Noise1) plt.imshow(Xfake1.reshape(28,28)) . &lt;matplotlib.image.AxesImage at 0x7f549c0eb5b0&gt; . (step3) 위조범이 생성한 이미지를 경찰한테 넘겨본다. . policehat_from_Xfake1 = net_police(Xfake1) policehat_from_Xfake1 . &lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.49095842]], dtype=float32)&gt; . - 경찰의 실력도 형편없고 위조범의 실력도 형편없다. . &#44221;&#52272;&#45348;&#53944;&#50892;&#53356;&#51032; &#49892;&#47141;&#51012; &#54693;&#49345;&#54616;&#51088; . - 데이터정리 . 원래 n=60000개의 real image가 있음. 이 자료중 일부를 batchsize=500 만큼 뽑아 이것을 ${ bf X}{batch}$라고 하자. 따라서 ${ bf X}_{batch}$의 차원은 (500,784) 이다. | 위조범이 만든 가짜자료를 원래자료의 batchsize와 같은 숫자인 500만큼 만듬. 그리고 이것을 ${ bf tilde X}{batch}$라고 하자. 그러면 ${ bf tilde X}_{batch}$의 차원은 (500,784)이다. | 진짜자료는 1, 가짜자료는 0으로 라벨링 | . batch_size = 500 . Noise_batch = np.random.normal(0,1,size=(batch_size,10)) Xfake_batch = net_counterfeiter(Noise_batch) . Xreal_batch = Xreal[:batch_size] Xpolice_batch = tf.concat([Xreal_batch,Xfake_batch],axis=0) . ypolice_batch = np.zeros(2*batch_size) ypolice_batch[:batch_size] = 1 . - 학습전: yhat(경찰의 예측)을 관찰 . net_police(Xreal_batch)[:5] . &lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy= array([[0.46388364], [0.45302677], [0.50195265], [0.543617 ], [0.44746903]], dtype=float32)&gt; . net_police(Xfake_batch)[:5] . &lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy= array([[0.49146503], [0.49262333], [0.49713606], [0.494118 ], [0.50205135]], dtype=float32)&gt; . - 학습하자: compile and fit . net_police.compile(loss=tf.losses.binary_crossentropy, optimizer=&#39;adam&#39;) . net_police.fit(Xpolice_batch,ypolice_batch) . 32/32 [==============================] - 0s 1ms/step - loss: 0.0591 . &lt;keras.callbacks.History at 0x7f549041d360&gt; . - 훈련된 경찰의 성능을 살펴보자. . net_police(Xreal_batch)[:5] . &lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy= array([[1.], [1.], [1.], [1.], [1.]], dtype=float32)&gt; . net_police(Xfake_batch)[:5] . &lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy= array([[6.4868483e-13], [1.3337571e-19], [2.0222801e-11], [1.7586674e-22], [5.3691273e-20]], dtype=float32)&gt; . &#50948;&#51312;&#48276;&#45348;&#53944;&#50892;&#53356;&#51032; &#49457;&#45733;&#51012; &#54693;&#49345;&#49884;&#53412;&#51088;. . - 자료구조: 네트워크의 입력은 임의의 노이즈, 네트워크의 출력은 fakeimage . - 손실함수의 설계? . 위조범 네트워크의 출력은 fakeimage, 위조범 네트워크의 yhat은 fakeimage 이다! | 이 가짜이미지를 (위조범네트워크의 yhat을) 경찰이 진짜라고 판단해야 위조범 입장에서는 좋은것. 즉 &quot;경찰네트워크(위조범네트워크의yhat) $ approx$ 1&quot; 이어야 함 | . def loss_counterfeiter(y,yhat): # note that yhat is fake image! return tf.losses.binary_crossentropy(y,net_police(yhat)) # here label should be 1 . ycounterfeiter_batch = np.ones(batch_size) net_counterfeiter.compile(loss=loss_counterfeiter,optimizer = &#39;adam&#39;) . net_counterfeiter.fit(Noise_batch, ycounterfeiter_batch) . 16/16 [==============================] - 0s 1ms/step - loss: 2.0207 . &lt;keras.callbacks.History at 0x7f54902cded0&gt; . plt.imshow(net_counterfeiter(Noise1).reshape(28,28)) . &lt;matplotlib.image.AxesImage at 0x7f542c24cfa0&gt; . 학습된 이미지의 하나의 샘플 (아직 노이즈같음) | . net_police(net_counterfeiter(Noise_batch))[:5] . &lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy= array([[1.], [1.], [1.], [1.], [1.]], dtype=float32)&gt; . 노이즈같지만 아무튼 이정도로도 경찰은 속는다! | . &#46160; &#51201;&#45824;&#51201; &#45348;&#53944;&#50892;&#53356;&#47484; &#44221;&#51137;&#49884;&#53412;&#51088;. . for _ in tqdm(range(500)): # step1: 가짜이미지 생성, 데이터정리 Noise_batch = np.random.normal(0,1,size=(batch_size,10)) Xfake_batch = net_counterfeiter(Noise_batch) Xreal_batch = Xreal[np.random.randint(0,Xreal.shape[0],size=batch_size)] # step2: 경찰네트워크용 데이터 정리 Xpolice_batch = tf.concat([Xreal_batch,Xfake_batch],axis=0) ypolice_batch = np.zeros(2*batch_size) ypolice_batch[:batch_size] = 1 # step3: 경찰네트워크 훈련 net_police.fit(Xpolice_batch,ypolice_batch,verbose=0) # step4: 위조범네트워크 훈련 Xcounterfeiter_batch = Noise_batch # &lt;- 위조범 네트워크의 X ycounterfeiter_batch = np.ones(batch_size) # &lt;- 위조범네트워크의 y net_counterfeiter.fit(Noise_batch, ycounterfeiter_batch, verbose=0) . 100%|██████████| 500/500 [00:48&lt;00:00, 10.25it/s] . fig, ax = plt.subplots(5,5) k=0 for i in range(5): for j in range(5): ax[i][j].imshow(net_counterfeiter.predict(Noise_batch)[k].reshape(28,28),cmap=&#39;gray&#39;) k=k+1 fig.set_figwidth(16) fig.set_figheight(16) fig.tight_layout() .",
            "url": "https://guebin.github.io/STBDA2022/2022/06/09/(14%EC%A3%BC%EC%B0%A8)-6%EC%9B%949%EC%9D%BC.html",
            "relUrl": "/2022/06/09/(14%EC%A3%BC%EC%B0%A8)-6%EC%9B%949%EC%9D%BC.html",
            "date": " • Jun 9, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "(13주차) 5월30일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import numpy as np import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import matplotlib.pyplot as plt . %load_ext tensorboard . &#50724;&#48260;&#54588;&#54021; . &#50724;&#48260;&#54588;&#54021;&#51004;&#47196; &#52265;&#44033;&#54616;&#44592; &#49772;&#50868; &#49345;&#54889; . 3-(1) 아래와 같은 모형을 고려하자. . $$y_i= beta_0 + sum_{k=1}^{5} beta_k cos(k t_i)+ epsilon_i$$ . 여기에서 $t=(t_1, dots,t_{1000})=$ np.linspace(0,5,1000) 이다. 그리고 $ epsilon_i sim i.i.d~ N(0, sigma^2)$, 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자. . np.random.seed(43052) t= np.linspace(0,5,1000) y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2 plt.plot(t,y,&#39;.&#39;,alpha=0.1) . [&lt;matplotlib.lines.Line2D at 0x7f6fcdee93c0&gt;] . tf.keras를 이용하여 $ beta_0, dots, beta_5$를 추정하라. ($ beta_0, dots, beta_5$의 참값은 각각 -2,3,1,0,0,0.5 이다) . (풀이) . - 다시 풀어보자 . y = y.reshape(1000,1) x1 = np.cos(t) x2 = np.cos(2*t) x3 = np.cos(3*t) x4 = np.cos(4*t) x5 = np.cos(5*t) X = tf.stack([x1,x2,x3,x4,x5],axis=1) . 2022-05-25 13:34:57.890242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . !rm -rf logs net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) net.fit(X,y,epochs=500,batch_size=100, validation_split=0.45, callbacks=tf.keras.callbacks.TensorBoard()) # 텐서보드를 이용한 시각화기능 추가 # validation_split 이용 . Epoch 1/500 6/6 [==============================] - 0s 9ms/step - loss: 6.2200 - val_loss: 16.9707 Epoch 2/500 6/6 [==============================] - 0s 3ms/step - loss: 6.1682 - val_loss: 16.9098 Epoch 3/500 6/6 [==============================] - 0s 3ms/step - loss: 6.1168 - val_loss: 16.8487 Epoch 4/500 6/6 [==============================] - 0s 3ms/step - loss: 6.0665 - val_loss: 16.7881 Epoch 5/500 6/6 [==============================] - 0s 3ms/step - loss: 6.0170 - val_loss: 16.7266 Epoch 6/500 6/6 [==============================] - 0s 4ms/step - loss: 5.9663 - val_loss: 16.6675 Epoch 7/500 6/6 [==============================] - 0s 4ms/step - loss: 5.9185 - val_loss: 16.6086 Epoch 8/500 6/6 [==============================] - 0s 3ms/step - loss: 5.8679 - val_loss: 16.5515 Epoch 9/500 6/6 [==============================] - 0s 4ms/step - loss: 5.8200 - val_loss: 16.4926 Epoch 10/500 6/6 [==============================] - 0s 3ms/step - loss: 5.7724 - val_loss: 16.4347 Epoch 11/500 6/6 [==============================] - 0s 3ms/step - loss: 5.7247 - val_loss: 16.3748 Epoch 12/500 6/6 [==============================] - 0s 3ms/step - loss: 5.6759 - val_loss: 16.3188 Epoch 13/500 6/6 [==============================] - 0s 3ms/step - loss: 5.6296 - val_loss: 16.2615 Epoch 14/500 6/6 [==============================] - 0s 3ms/step - loss: 5.5824 - val_loss: 16.2049 Epoch 15/500 6/6 [==============================] - 0s 3ms/step - loss: 5.5361 - val_loss: 16.1485 Epoch 16/500 6/6 [==============================] - 0s 4ms/step - loss: 5.4899 - val_loss: 16.0936 Epoch 17/500 6/6 [==============================] - 0s 3ms/step - loss: 5.4436 - val_loss: 16.0364 Epoch 18/500 6/6 [==============================] - 0s 3ms/step - loss: 5.3988 - val_loss: 15.9806 Epoch 19/500 6/6 [==============================] - 0s 3ms/step - loss: 5.3545 - val_loss: 15.9251 Epoch 20/500 6/6 [==============================] - 0s 3ms/step - loss: 5.3087 - val_loss: 15.8708 Epoch 21/500 6/6 [==============================] - 0s 3ms/step - loss: 5.2653 - val_loss: 15.8172 Epoch 22/500 6/6 [==============================] - 0s 3ms/step - loss: 5.2204 - val_loss: 15.7660 Epoch 23/500 6/6 [==============================] - 0s 3ms/step - loss: 5.1774 - val_loss: 15.7140 Epoch 24/500 6/6 [==============================] - 0s 3ms/step - loss: 5.1352 - val_loss: 15.6609 Epoch 25/500 6/6 [==============================] - 0s 3ms/step - loss: 5.0921 - val_loss: 15.6077 Epoch 26/500 6/6 [==============================] - 0s 3ms/step - loss: 5.0496 - val_loss: 15.5556 Epoch 27/500 6/6 [==============================] - 0s 3ms/step - loss: 5.0074 - val_loss: 15.5049 Epoch 28/500 6/6 [==============================] - 0s 3ms/step - loss: 4.9658 - val_loss: 15.4536 Epoch 29/500 6/6 [==============================] - 0s 3ms/step - loss: 4.9243 - val_loss: 15.4012 Epoch 30/500 6/6 [==============================] - 0s 3ms/step - loss: 4.8829 - val_loss: 15.3500 Epoch 31/500 6/6 [==============================] - 0s 3ms/step - loss: 4.8414 - val_loss: 15.2996 Epoch 32/500 6/6 [==============================] - 0s 3ms/step - loss: 4.8004 - val_loss: 15.2499 Epoch 33/500 6/6 [==============================] - 0s 3ms/step - loss: 4.7603 - val_loss: 15.1995 Epoch 34/500 6/6 [==============================] - 0s 4ms/step - loss: 4.7195 - val_loss: 15.1498 Epoch 35/500 6/6 [==============================] - 0s 3ms/step - loss: 4.6804 - val_loss: 15.1006 Epoch 36/500 6/6 [==============================] - 0s 3ms/step - loss: 4.6396 - val_loss: 15.0534 Epoch 37/500 6/6 [==============================] - 0s 3ms/step - loss: 4.6005 - val_loss: 15.0050 Epoch 38/500 6/6 [==============================] - 0s 3ms/step - loss: 4.5618 - val_loss: 14.9572 Epoch 39/500 6/6 [==============================] - 0s 3ms/step - loss: 4.5234 - val_loss: 14.9099 Epoch 40/500 6/6 [==============================] - 0s 3ms/step - loss: 4.4840 - val_loss: 14.8627 Epoch 41/500 6/6 [==============================] - 0s 3ms/step - loss: 4.4467 - val_loss: 14.8146 Epoch 42/500 6/6 [==============================] - 0s 3ms/step - loss: 4.4087 - val_loss: 14.7684 Epoch 43/500 6/6 [==============================] - 0s 3ms/step - loss: 4.3711 - val_loss: 14.7220 Epoch 44/500 6/6 [==============================] - 0s 3ms/step - loss: 4.3334 - val_loss: 14.6753 Epoch 45/500 6/6 [==============================] - 0s 3ms/step - loss: 4.2970 - val_loss: 14.6294 Epoch 46/500 6/6 [==============================] - 0s 3ms/step - loss: 4.2603 - val_loss: 14.5826 Epoch 47/500 6/6 [==============================] - 0s 3ms/step - loss: 4.2232 - val_loss: 14.5386 Epoch 48/500 6/6 [==============================] - 0s 4ms/step - loss: 4.1877 - val_loss: 14.4930 Epoch 49/500 6/6 [==============================] - 0s 4ms/step - loss: 4.1513 - val_loss: 14.4489 Epoch 50/500 6/6 [==============================] - 0s 3ms/step - loss: 4.1169 - val_loss: 14.4047 Epoch 51/500 6/6 [==============================] - 0s 3ms/step - loss: 4.0811 - val_loss: 14.3595 Epoch 52/500 6/6 [==============================] - 0s 3ms/step - loss: 4.0459 - val_loss: 14.3128 Epoch 53/500 6/6 [==============================] - 0s 4ms/step - loss: 4.0111 - val_loss: 14.2681 Epoch 54/500 6/6 [==============================] - 0s 3ms/step - loss: 3.9770 - val_loss: 14.2218 Epoch 55/500 6/6 [==============================] - 0s 4ms/step - loss: 3.9416 - val_loss: 14.1764 Epoch 56/500 6/6 [==============================] - 0s 3ms/step - loss: 3.9088 - val_loss: 14.1324 Epoch 57/500 6/6 [==============================] - 0s 4ms/step - loss: 3.8748 - val_loss: 14.0885 Epoch 58/500 6/6 [==============================] - 0s 3ms/step - loss: 3.8411 - val_loss: 14.0468 Epoch 59/500 6/6 [==============================] - 0s 3ms/step - loss: 3.8085 - val_loss: 14.0052 Epoch 60/500 6/6 [==============================] - 0s 3ms/step - loss: 3.7749 - val_loss: 13.9633 Epoch 61/500 6/6 [==============================] - 0s 3ms/step - loss: 3.7425 - val_loss: 13.9234 Epoch 62/500 6/6 [==============================] - 0s 3ms/step - loss: 3.7106 - val_loss: 13.8829 Epoch 63/500 6/6 [==============================] - 0s 3ms/step - loss: 3.6800 - val_loss: 13.8406 Epoch 64/500 6/6 [==============================] - 0s 3ms/step - loss: 3.6468 - val_loss: 13.8024 Epoch 65/500 6/6 [==============================] - 0s 3ms/step - loss: 3.6158 - val_loss: 13.7614 Epoch 66/500 6/6 [==============================] - 0s 3ms/step - loss: 3.5847 - val_loss: 13.7200 Epoch 67/500 6/6 [==============================] - 0s 3ms/step - loss: 3.5536 - val_loss: 13.6788 Epoch 68/500 6/6 [==============================] - 0s 3ms/step - loss: 3.5231 - val_loss: 13.6385 Epoch 69/500 6/6 [==============================] - 0s 3ms/step - loss: 3.4927 - val_loss: 13.6000 Epoch 70/500 6/6 [==============================] - 0s 3ms/step - loss: 3.4631 - val_loss: 13.5597 Epoch 71/500 6/6 [==============================] - 0s 3ms/step - loss: 3.4322 - val_loss: 13.5206 Epoch 72/500 6/6 [==============================] - 0s 2ms/step - loss: 3.4026 - val_loss: 13.4812 Epoch 73/500 6/6 [==============================] - 0s 3ms/step - loss: 3.3729 - val_loss: 13.4425 Epoch 74/500 6/6 [==============================] - 0s 3ms/step - loss: 3.3431 - val_loss: 13.4058 Epoch 75/500 6/6 [==============================] - 0s 3ms/step - loss: 3.3148 - val_loss: 13.3676 Epoch 76/500 6/6 [==============================] - 0s 3ms/step - loss: 3.2857 - val_loss: 13.3286 Epoch 77/500 6/6 [==============================] - 0s 2ms/step - loss: 3.2565 - val_loss: 13.2909 Epoch 78/500 6/6 [==============================] - 0s 2ms/step - loss: 3.2284 - val_loss: 13.2534 Epoch 79/500 6/6 [==============================] - 0s 3ms/step - loss: 3.1992 - val_loss: 13.2167 Epoch 80/500 6/6 [==============================] - 0s 4ms/step - loss: 3.1722 - val_loss: 13.1812 Epoch 81/500 6/6 [==============================] - 0s 4ms/step - loss: 3.1444 - val_loss: 13.1452 Epoch 82/500 6/6 [==============================] - 0s 3ms/step - loss: 3.1169 - val_loss: 13.1083 Epoch 83/500 6/6 [==============================] - 0s 2ms/step - loss: 3.0901 - val_loss: 13.0695 Epoch 84/500 6/6 [==============================] - 0s 3ms/step - loss: 3.0625 - val_loss: 13.0314 Epoch 85/500 6/6 [==============================] - 0s 3ms/step - loss: 3.0359 - val_loss: 12.9948 Epoch 86/500 6/6 [==============================] - 0s 3ms/step - loss: 3.0088 - val_loss: 12.9592 Epoch 87/500 6/6 [==============================] - 0s 4ms/step - loss: 2.9823 - val_loss: 12.9249 Epoch 88/500 6/6 [==============================] - 0s 3ms/step - loss: 2.9566 - val_loss: 12.8900 Epoch 89/500 6/6 [==============================] - 0s 3ms/step - loss: 2.9307 - val_loss: 12.8534 Epoch 90/500 6/6 [==============================] - 0s 3ms/step - loss: 2.9050 - val_loss: 12.8185 Epoch 91/500 6/6 [==============================] - 0s 3ms/step - loss: 2.8792 - val_loss: 12.7831 Epoch 92/500 6/6 [==============================] - 0s 3ms/step - loss: 2.8542 - val_loss: 12.7482 Epoch 93/500 6/6 [==============================] - 0s 3ms/step - loss: 2.8289 - val_loss: 12.7135 Epoch 94/500 6/6 [==============================] - 0s 3ms/step - loss: 2.8041 - val_loss: 12.6790 Epoch 95/500 6/6 [==============================] - 0s 3ms/step - loss: 2.7795 - val_loss: 12.6438 Epoch 96/500 6/6 [==============================] - 0s 3ms/step - loss: 2.7546 - val_loss: 12.6110 Epoch 97/500 6/6 [==============================] - 0s 3ms/step - loss: 2.7307 - val_loss: 12.5763 Epoch 98/500 6/6 [==============================] - 0s 3ms/step - loss: 2.7060 - val_loss: 12.5430 Epoch 99/500 6/6 [==============================] - 0s 3ms/step - loss: 2.6825 - val_loss: 12.5093 Epoch 100/500 6/6 [==============================] - 0s 3ms/step - loss: 2.6584 - val_loss: 12.4760 Epoch 101/500 6/6 [==============================] - 0s 3ms/step - loss: 2.6350 - val_loss: 12.4419 Epoch 102/500 6/6 [==============================] - 0s 3ms/step - loss: 2.6117 - val_loss: 12.4088 Epoch 103/500 6/6 [==============================] - 0s 3ms/step - loss: 2.5882 - val_loss: 12.3750 Epoch 104/500 6/6 [==============================] - 0s 3ms/step - loss: 2.5657 - val_loss: 12.3433 Epoch 105/500 6/6 [==============================] - 0s 3ms/step - loss: 2.5425 - val_loss: 12.3114 Epoch 106/500 6/6 [==============================] - 0s 3ms/step - loss: 2.5202 - val_loss: 12.2804 Epoch 107/500 6/6 [==============================] - 0s 3ms/step - loss: 2.4979 - val_loss: 12.2476 Epoch 108/500 6/6 [==============================] - 0s 3ms/step - loss: 2.4755 - val_loss: 12.2145 Epoch 109/500 6/6 [==============================] - 0s 3ms/step - loss: 2.4537 - val_loss: 12.1828 Epoch 110/500 6/6 [==============================] - 0s 3ms/step - loss: 2.4314 - val_loss: 12.1527 Epoch 111/500 6/6 [==============================] - 0s 3ms/step - loss: 2.4102 - val_loss: 12.1212 Epoch 112/500 6/6 [==============================] - 0s 3ms/step - loss: 2.3888 - val_loss: 12.0904 Epoch 113/500 6/6 [==============================] - 0s 3ms/step - loss: 2.3681 - val_loss: 12.0618 Epoch 114/500 6/6 [==============================] - 0s 3ms/step - loss: 2.3466 - val_loss: 12.0304 Epoch 115/500 6/6 [==============================] - 0s 3ms/step - loss: 2.3263 - val_loss: 12.0002 Epoch 116/500 6/6 [==============================] - 0s 3ms/step - loss: 2.3053 - val_loss: 11.9704 Epoch 117/500 6/6 [==============================] - 0s 3ms/step - loss: 2.2849 - val_loss: 11.9400 Epoch 118/500 6/6 [==============================] - 0s 3ms/step - loss: 2.2647 - val_loss: 11.9106 Epoch 119/500 6/6 [==============================] - 0s 3ms/step - loss: 2.2438 - val_loss: 11.8820 Epoch 120/500 6/6 [==============================] - 0s 3ms/step - loss: 2.2241 - val_loss: 11.8520 Epoch 121/500 6/6 [==============================] - 0s 3ms/step - loss: 2.2039 - val_loss: 11.8224 Epoch 122/500 6/6 [==============================] - 0s 3ms/step - loss: 2.1843 - val_loss: 11.7940 Epoch 123/500 6/6 [==============================] - 0s 3ms/step - loss: 2.1648 - val_loss: 11.7655 Epoch 124/500 6/6 [==============================] - 0s 4ms/step - loss: 2.1454 - val_loss: 11.7361 Epoch 125/500 6/6 [==============================] - 0s 3ms/step - loss: 2.1262 - val_loss: 11.7072 Epoch 126/500 6/6 [==============================] - 0s 3ms/step - loss: 2.1071 - val_loss: 11.6805 Epoch 127/500 6/6 [==============================] - 0s 3ms/step - loss: 2.0886 - val_loss: 11.6536 Epoch 128/500 6/6 [==============================] - 0s 3ms/step - loss: 2.0695 - val_loss: 11.6254 Epoch 129/500 6/6 [==============================] - 0s 2ms/step - loss: 2.0515 - val_loss: 11.5969 Epoch 130/500 6/6 [==============================] - 0s 3ms/step - loss: 2.0327 - val_loss: 11.5688 Epoch 131/500 6/6 [==============================] - 0s 3ms/step - loss: 2.0150 - val_loss: 11.5409 Epoch 132/500 6/6 [==============================] - 0s 3ms/step - loss: 1.9966 - val_loss: 11.5148 Epoch 133/500 6/6 [==============================] - 0s 3ms/step - loss: 1.9790 - val_loss: 11.4873 Epoch 134/500 6/6 [==============================] - 0s 3ms/step - loss: 1.9609 - val_loss: 11.4611 Epoch 135/500 6/6 [==============================] - 0s 3ms/step - loss: 1.9438 - val_loss: 11.4336 Epoch 136/500 6/6 [==============================] - 0s 3ms/step - loss: 1.9268 - val_loss: 11.4073 Epoch 137/500 6/6 [==============================] - 0s 3ms/step - loss: 1.9093 - val_loss: 11.3819 Epoch 138/500 6/6 [==============================] - 0s 3ms/step - loss: 1.8922 - val_loss: 11.3557 Epoch 139/500 6/6 [==============================] - 0s 3ms/step - loss: 1.8756 - val_loss: 11.3285 Epoch 140/500 6/6 [==============================] - 0s 3ms/step - loss: 1.8586 - val_loss: 11.3015 Epoch 141/500 6/6 [==============================] - 0s 2ms/step - loss: 1.8418 - val_loss: 11.2759 Epoch 142/500 6/6 [==============================] - 0s 3ms/step - loss: 1.8256 - val_loss: 11.2502 Epoch 143/500 6/6 [==============================] - 0s 4ms/step - loss: 1.8090 - val_loss: 11.2256 Epoch 144/500 6/6 [==============================] - 0s 3ms/step - loss: 1.7927 - val_loss: 11.1994 Epoch 145/500 6/6 [==============================] - 0s 3ms/step - loss: 1.7769 - val_loss: 11.1727 Epoch 146/500 6/6 [==============================] - 0s 3ms/step - loss: 1.7608 - val_loss: 11.1471 Epoch 147/500 6/6 [==============================] - 0s 3ms/step - loss: 1.7451 - val_loss: 11.1210 Epoch 148/500 6/6 [==============================] - 0s 3ms/step - loss: 1.7294 - val_loss: 11.0955 Epoch 149/500 6/6 [==============================] - 0s 4ms/step - loss: 1.7142 - val_loss: 11.0686 Epoch 150/500 6/6 [==============================] - 0s 3ms/step - loss: 1.6985 - val_loss: 11.0437 Epoch 151/500 6/6 [==============================] - 0s 3ms/step - loss: 1.6833 - val_loss: 11.0186 Epoch 152/500 6/6 [==============================] - 0s 3ms/step - loss: 1.6682 - val_loss: 10.9943 Epoch 153/500 6/6 [==============================] - 0s 4ms/step - loss: 1.6531 - val_loss: 10.9703 Epoch 154/500 6/6 [==============================] - 0s 3ms/step - loss: 1.6386 - val_loss: 10.9462 Epoch 155/500 6/6 [==============================] - 0s 3ms/step - loss: 1.6239 - val_loss: 10.9234 Epoch 156/500 6/6 [==============================] - 0s 3ms/step - loss: 1.6093 - val_loss: 10.9001 Epoch 157/500 6/6 [==============================] - 0s 3ms/step - loss: 1.5949 - val_loss: 10.8762 Epoch 158/500 6/6 [==============================] - 0s 3ms/step - loss: 1.5811 - val_loss: 10.8518 Epoch 159/500 6/6 [==============================] - 0s 3ms/step - loss: 1.5667 - val_loss: 10.8276 Epoch 160/500 6/6 [==============================] - 0s 3ms/step - loss: 1.5530 - val_loss: 10.8025 Epoch 161/500 6/6 [==============================] - 0s 3ms/step - loss: 1.5387 - val_loss: 10.7807 Epoch 162/500 6/6 [==============================] - 0s 3ms/step - loss: 1.5250 - val_loss: 10.7566 Epoch 163/500 6/6 [==============================] - 0s 3ms/step - loss: 1.5115 - val_loss: 10.7325 Epoch 164/500 6/6 [==============================] - 0s 3ms/step - loss: 1.4977 - val_loss: 10.7099 Epoch 165/500 6/6 [==============================] - 0s 3ms/step - loss: 1.4846 - val_loss: 10.6866 Epoch 166/500 6/6 [==============================] - 0s 3ms/step - loss: 1.4714 - val_loss: 10.6642 Epoch 167/500 6/6 [==============================] - 0s 3ms/step - loss: 1.4582 - val_loss: 10.6410 Epoch 168/500 6/6 [==============================] - 0s 3ms/step - loss: 1.4451 - val_loss: 10.6192 Epoch 169/500 6/6 [==============================] - 0s 3ms/step - loss: 1.4321 - val_loss: 10.5966 Epoch 170/500 6/6 [==============================] - 0s 3ms/step - loss: 1.4194 - val_loss: 10.5742 Epoch 171/500 6/6 [==============================] - 0s 3ms/step - loss: 1.4066 - val_loss: 10.5521 Epoch 172/500 6/6 [==============================] - 0s 3ms/step - loss: 1.3942 - val_loss: 10.5301 Epoch 173/500 6/6 [==============================] - 0s 3ms/step - loss: 1.3817 - val_loss: 10.5074 Epoch 174/500 6/6 [==============================] - 0s 3ms/step - loss: 1.3695 - val_loss: 10.4855 Epoch 175/500 6/6 [==============================] - 0s 3ms/step - loss: 1.3575 - val_loss: 10.4629 Epoch 176/500 6/6 [==============================] - 0s 4ms/step - loss: 1.3451 - val_loss: 10.4417 Epoch 177/500 6/6 [==============================] - 0s 3ms/step - loss: 1.3332 - val_loss: 10.4193 Epoch 178/500 6/6 [==============================] - 0s 3ms/step - loss: 1.3214 - val_loss: 10.3978 Epoch 179/500 6/6 [==============================] - 0s 3ms/step - loss: 1.3094 - val_loss: 10.3760 Epoch 180/500 6/6 [==============================] - 0s 3ms/step - loss: 1.2979 - val_loss: 10.3535 Epoch 181/500 6/6 [==============================] - 0s 3ms/step - loss: 1.2867 - val_loss: 10.3313 Epoch 182/500 6/6 [==============================] - 0s 3ms/step - loss: 1.2751 - val_loss: 10.3107 Epoch 183/500 6/6 [==============================] - 0s 3ms/step - loss: 1.2640 - val_loss: 10.2911 Epoch 184/500 6/6 [==============================] - 0s 4ms/step - loss: 1.2527 - val_loss: 10.2700 Epoch 185/500 6/6 [==============================] - 0s 3ms/step - loss: 1.2418 - val_loss: 10.2492 Epoch 186/500 6/6 [==============================] - 0s 4ms/step - loss: 1.2310 - val_loss: 10.2279 Epoch 187/500 6/6 [==============================] - 0s 3ms/step - loss: 1.2201 - val_loss: 10.2073 Epoch 188/500 6/6 [==============================] - 0s 2ms/step - loss: 1.2098 - val_loss: 10.1864 Epoch 189/500 6/6 [==============================] - 0s 3ms/step - loss: 1.1989 - val_loss: 10.1660 Epoch 190/500 6/6 [==============================] - 0s 3ms/step - loss: 1.1886 - val_loss: 10.1447 Epoch 191/500 6/6 [==============================] - 0s 3ms/step - loss: 1.1780 - val_loss: 10.1253 Epoch 192/500 6/6 [==============================] - 0s 3ms/step - loss: 1.1680 - val_loss: 10.1046 Epoch 193/500 6/6 [==============================] - 0s 4ms/step - loss: 1.1579 - val_loss: 10.0847 Epoch 194/500 6/6 [==============================] - 0s 3ms/step - loss: 1.1477 - val_loss: 10.0651 Epoch 195/500 6/6 [==============================] - 0s 3ms/step - loss: 1.1379 - val_loss: 10.0455 Epoch 196/500 6/6 [==============================] - 0s 3ms/step - loss: 1.1280 - val_loss: 10.0270 Epoch 197/500 6/6 [==============================] - 0s 4ms/step - loss: 1.1183 - val_loss: 10.0083 Epoch 198/500 6/6 [==============================] - 0s 4ms/step - loss: 1.1086 - val_loss: 9.9880 Epoch 199/500 6/6 [==============================] - 0s 3ms/step - loss: 1.0990 - val_loss: 9.9680 Epoch 200/500 6/6 [==============================] - 0s 3ms/step - loss: 1.0896 - val_loss: 9.9479 Epoch 201/500 6/6 [==============================] - 0s 3ms/step - loss: 1.0802 - val_loss: 9.9284 Epoch 202/500 6/6 [==============================] - 0s 3ms/step - loss: 1.0708 - val_loss: 9.9092 Epoch 203/500 6/6 [==============================] - 0s 3ms/step - loss: 1.0616 - val_loss: 9.8902 Epoch 204/500 6/6 [==============================] - 0s 3ms/step - loss: 1.0527 - val_loss: 9.8713 Epoch 205/500 6/6 [==============================] - 0s 3ms/step - loss: 1.0435 - val_loss: 9.8533 Epoch 206/500 6/6 [==============================] - 0s 3ms/step - loss: 1.0346 - val_loss: 9.8335 Epoch 207/500 6/6 [==============================] - 0s 3ms/step - loss: 1.0260 - val_loss: 9.8141 Epoch 208/500 6/6 [==============================] - 0s 3ms/step - loss: 1.0172 - val_loss: 9.7948 Epoch 209/500 6/6 [==============================] - 0s 3ms/step - loss: 1.0084 - val_loss: 9.7757 Epoch 210/500 6/6 [==============================] - 0s 3ms/step - loss: 1.0000 - val_loss: 9.7570 Epoch 211/500 6/6 [==============================] - 0s 3ms/step - loss: 0.9918 - val_loss: 9.7374 Epoch 212/500 6/6 [==============================] - 0s 3ms/step - loss: 0.9832 - val_loss: 9.7190 Epoch 213/500 6/6 [==============================] - 0s 3ms/step - loss: 0.9752 - val_loss: 9.6991 Epoch 214/500 6/6 [==============================] - 0s 3ms/step - loss: 0.9669 - val_loss: 9.6805 Epoch 215/500 6/6 [==============================] - 0s 3ms/step - loss: 0.9588 - val_loss: 9.6630 Epoch 216/500 6/6 [==============================] - 0s 3ms/step - loss: 0.9509 - val_loss: 9.6449 Epoch 217/500 6/6 [==============================] - 0s 3ms/step - loss: 0.9429 - val_loss: 9.6260 Epoch 218/500 6/6 [==============================] - 0s 3ms/step - loss: 0.9351 - val_loss: 9.6078 Epoch 219/500 6/6 [==============================] - 0s 3ms/step - loss: 0.9272 - val_loss: 9.5889 Epoch 220/500 6/6 [==============================] - 0s 2ms/step - loss: 0.9194 - val_loss: 9.5704 Epoch 221/500 6/6 [==============================] - 0s 3ms/step - loss: 0.9117 - val_loss: 9.5517 Epoch 222/500 6/6 [==============================] - 0s 3ms/step - loss: 0.9043 - val_loss: 9.5328 Epoch 223/500 6/6 [==============================] - 0s 3ms/step - loss: 0.8967 - val_loss: 9.5144 Epoch 224/500 6/6 [==============================] - 0s 3ms/step - loss: 0.8893 - val_loss: 9.4963 Epoch 225/500 6/6 [==============================] - 0s 3ms/step - loss: 0.8820 - val_loss: 9.4785 Epoch 226/500 6/6 [==============================] - 0s 3ms/step - loss: 0.8748 - val_loss: 9.4600 Epoch 227/500 6/6 [==============================] - 0s 3ms/step - loss: 0.8675 - val_loss: 9.4414 Epoch 228/500 6/6 [==============================] - 0s 3ms/step - loss: 0.8605 - val_loss: 9.4222 Epoch 229/500 6/6 [==============================] - 0s 4ms/step - loss: 0.8535 - val_loss: 9.4037 Epoch 230/500 6/6 [==============================] - 0s 3ms/step - loss: 0.8464 - val_loss: 9.3854 Epoch 231/500 6/6 [==============================] - 0s 3ms/step - loss: 0.8396 - val_loss: 9.3677 Epoch 232/500 6/6 [==============================] - 0s 3ms/step - loss: 0.8328 - val_loss: 9.3485 Epoch 233/500 6/6 [==============================] - 0s 3ms/step - loss: 0.8262 - val_loss: 9.3306 Epoch 234/500 6/6 [==============================] - 0s 4ms/step - loss: 0.8196 - val_loss: 9.3123 Epoch 235/500 6/6 [==============================] - 0s 3ms/step - loss: 0.8130 - val_loss: 9.2945 Epoch 236/500 6/6 [==============================] - 0s 3ms/step - loss: 0.8064 - val_loss: 9.2777 Epoch 237/500 6/6 [==============================] - 0s 3ms/step - loss: 0.8002 - val_loss: 9.2597 Epoch 238/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7938 - val_loss: 9.2424 Epoch 239/500 6/6 [==============================] - 0s 4ms/step - loss: 0.7875 - val_loss: 9.2246 Epoch 240/500 6/6 [==============================] - 0s 4ms/step - loss: 0.7812 - val_loss: 9.2078 Epoch 241/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7752 - val_loss: 9.1906 Epoch 242/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7690 - val_loss: 9.1731 Epoch 243/500 6/6 [==============================] - 0s 4ms/step - loss: 0.7630 - val_loss: 9.1547 Epoch 244/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7570 - val_loss: 9.1369 Epoch 245/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7511 - val_loss: 9.1204 Epoch 246/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7454 - val_loss: 9.1022 Epoch 247/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7395 - val_loss: 9.0852 Epoch 248/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7338 - val_loss: 9.0677 Epoch 249/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7282 - val_loss: 9.0500 Epoch 250/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7227 - val_loss: 9.0341 Epoch 251/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7172 - val_loss: 9.0179 Epoch 252/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7118 - val_loss: 9.0014 Epoch 253/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7064 - val_loss: 8.9845 Epoch 254/500 6/6 [==============================] - 0s 3ms/step - loss: 0.7011 - val_loss: 8.9669 Epoch 255/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6958 - val_loss: 8.9504 Epoch 256/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6907 - val_loss: 8.9342 Epoch 257/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6855 - val_loss: 8.9176 Epoch 258/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6804 - val_loss: 8.9012 Epoch 259/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6755 - val_loss: 8.8848 Epoch 260/500 6/6 [==============================] - 0s 4ms/step - loss: 0.6705 - val_loss: 8.8678 Epoch 261/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6655 - val_loss: 8.8516 Epoch 262/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6606 - val_loss: 8.8351 Epoch 263/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6559 - val_loss: 8.8178 Epoch 264/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6510 - val_loss: 8.8016 Epoch 265/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6464 - val_loss: 8.7851 Epoch 266/500 6/6 [==============================] - 0s 4ms/step - loss: 0.6419 - val_loss: 8.7681 Epoch 267/500 6/6 [==============================] - 0s 4ms/step - loss: 0.6372 - val_loss: 8.7525 Epoch 268/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6328 - val_loss: 8.7359 Epoch 269/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6284 - val_loss: 8.7178 Epoch 270/500 6/6 [==============================] - 0s 2ms/step - loss: 0.6240 - val_loss: 8.7002 Epoch 271/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6195 - val_loss: 8.6830 Epoch 272/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6153 - val_loss: 8.6657 Epoch 273/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6109 - val_loss: 8.6506 Epoch 274/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6067 - val_loss: 8.6332 Epoch 275/500 6/6 [==============================] - 0s 3ms/step - loss: 0.6025 - val_loss: 8.6157 Epoch 276/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5983 - val_loss: 8.5999 Epoch 277/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5942 - val_loss: 8.5836 Epoch 278/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5901 - val_loss: 8.5670 Epoch 279/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5861 - val_loss: 8.5503 Epoch 280/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5822 - val_loss: 8.5338 Epoch 281/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5782 - val_loss: 8.5173 Epoch 282/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5743 - val_loss: 8.5005 Epoch 283/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5705 - val_loss: 8.4849 Epoch 284/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5666 - val_loss: 8.4695 Epoch 285/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5630 - val_loss: 8.4534 Epoch 286/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5593 - val_loss: 8.4368 Epoch 287/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5556 - val_loss: 8.4202 Epoch 288/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5521 - val_loss: 8.4029 Epoch 289/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5485 - val_loss: 8.3865 Epoch 290/500 6/6 [==============================] - 0s 4ms/step - loss: 0.5449 - val_loss: 8.3706 Epoch 291/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5416 - val_loss: 8.3542 Epoch 292/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5380 - val_loss: 8.3383 Epoch 293/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5346 - val_loss: 8.3220 Epoch 294/500 6/6 [==============================] - 0s 4ms/step - loss: 0.5313 - val_loss: 8.3061 Epoch 295/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5279 - val_loss: 8.2898 Epoch 296/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5245 - val_loss: 8.2738 Epoch 297/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5213 - val_loss: 8.2570 Epoch 298/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5180 - val_loss: 8.2404 Epoch 299/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5149 - val_loss: 8.2240 Epoch 300/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5117 - val_loss: 8.2085 Epoch 301/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5086 - val_loss: 8.1927 Epoch 302/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5055 - val_loss: 8.1769 Epoch 303/500 6/6 [==============================] - 0s 3ms/step - loss: 0.5026 - val_loss: 8.1606 Epoch 304/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4995 - val_loss: 8.1443 Epoch 305/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4966 - val_loss: 8.1277 Epoch 306/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4936 - val_loss: 8.1108 Epoch 307/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4907 - val_loss: 8.0935 Epoch 308/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4879 - val_loss: 8.0770 Epoch 309/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4850 - val_loss: 8.0609 Epoch 310/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4822 - val_loss: 8.0452 Epoch 311/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4794 - val_loss: 8.0294 Epoch 312/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4768 - val_loss: 8.0134 Epoch 313/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4741 - val_loss: 7.9976 Epoch 314/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4715 - val_loss: 7.9818 Epoch 315/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4689 - val_loss: 7.9666 Epoch 316/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4663 - val_loss: 7.9510 Epoch 317/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4638 - val_loss: 7.9347 Epoch 318/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4612 - val_loss: 7.9191 Epoch 319/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4587 - val_loss: 7.9027 Epoch 320/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4561 - val_loss: 7.8862 Epoch 321/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4536 - val_loss: 7.8708 Epoch 322/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4512 - val_loss: 7.8552 Epoch 323/500 6/6 [==============================] - 0s 2ms/step - loss: 0.4487 - val_loss: 7.8394 Epoch 324/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4463 - val_loss: 7.8227 Epoch 325/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4439 - val_loss: 7.8063 Epoch 326/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4416 - val_loss: 7.7906 Epoch 327/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4393 - val_loss: 7.7742 Epoch 328/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4370 - val_loss: 7.7577 Epoch 329/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4348 - val_loss: 7.7415 Epoch 330/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4326 - val_loss: 7.7239 Epoch 331/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4304 - val_loss: 7.7076 Epoch 332/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4282 - val_loss: 7.6921 Epoch 333/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4261 - val_loss: 7.6763 Epoch 334/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4240 - val_loss: 7.6604 Epoch 335/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4220 - val_loss: 7.6445 Epoch 336/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4198 - val_loss: 7.6292 Epoch 337/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4179 - val_loss: 7.6131 Epoch 338/500 6/6 [==============================] - 0s 2ms/step - loss: 0.4158 - val_loss: 7.5977 Epoch 339/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4138 - val_loss: 7.5814 Epoch 340/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4118 - val_loss: 7.5642 Epoch 341/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4099 - val_loss: 7.5479 Epoch 342/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4079 - val_loss: 7.5325 Epoch 343/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4060 - val_loss: 7.5169 Epoch 344/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4040 - val_loss: 7.5016 Epoch 345/500 6/6 [==============================] - 0s 4ms/step - loss: 0.4022 - val_loss: 7.4851 Epoch 346/500 6/6 [==============================] - 0s 3ms/step - loss: 0.4003 - val_loss: 7.4683 Epoch 347/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3985 - val_loss: 7.4519 Epoch 348/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3966 - val_loss: 7.4350 Epoch 349/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3948 - val_loss: 7.4181 Epoch 350/500 6/6 [==============================] - 0s 2ms/step - loss: 0.3930 - val_loss: 7.4023 Epoch 351/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3912 - val_loss: 7.3859 Epoch 352/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3895 - val_loss: 7.3701 Epoch 353/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3877 - val_loss: 7.3544 Epoch 354/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3860 - val_loss: 7.3373 Epoch 355/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3842 - val_loss: 7.3210 Epoch 356/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3825 - val_loss: 7.3045 Epoch 357/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3808 - val_loss: 7.2885 Epoch 358/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3791 - val_loss: 7.2724 Epoch 359/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3775 - val_loss: 7.2565 Epoch 360/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3758 - val_loss: 7.2409 Epoch 361/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3742 - val_loss: 7.2244 Epoch 362/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3727 - val_loss: 7.2077 Epoch 363/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3710 - val_loss: 7.1918 Epoch 364/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3695 - val_loss: 7.1761 Epoch 365/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3679 - val_loss: 7.1599 Epoch 366/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3664 - val_loss: 7.1431 Epoch 367/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3649 - val_loss: 7.1263 Epoch 368/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3634 - val_loss: 7.1100 Epoch 369/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3619 - val_loss: 7.0939 Epoch 370/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3604 - val_loss: 7.0774 Epoch 371/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3589 - val_loss: 7.0615 Epoch 372/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3575 - val_loss: 7.0453 Epoch 373/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3561 - val_loss: 7.0284 Epoch 374/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3547 - val_loss: 7.0119 Epoch 375/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3533 - val_loss: 6.9952 Epoch 376/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3519 - val_loss: 6.9800 Epoch 377/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3505 - val_loss: 6.9635 Epoch 378/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3491 - val_loss: 6.9466 Epoch 379/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3477 - val_loss: 6.9295 Epoch 380/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3464 - val_loss: 6.9124 Epoch 381/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3452 - val_loss: 6.8957 Epoch 382/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3438 - val_loss: 6.8792 Epoch 383/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3425 - val_loss: 6.8631 Epoch 384/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3412 - val_loss: 6.8465 Epoch 385/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3399 - val_loss: 6.8299 Epoch 386/500 6/6 [==============================] - 0s 4ms/step - loss: 0.3387 - val_loss: 6.8137 Epoch 387/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3373 - val_loss: 6.7981 Epoch 388/500 6/6 [==============================] - 0s 4ms/step - loss: 0.3361 - val_loss: 6.7825 Epoch 389/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3348 - val_loss: 6.7660 Epoch 390/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3336 - val_loss: 6.7500 Epoch 391/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3323 - val_loss: 6.7337 Epoch 392/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3311 - val_loss: 6.7167 Epoch 393/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3299 - val_loss: 6.7001 Epoch 394/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3287 - val_loss: 6.6835 Epoch 395/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3275 - val_loss: 6.6664 Epoch 396/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3263 - val_loss: 6.6488 Epoch 397/500 6/6 [==============================] - 0s 4ms/step - loss: 0.3252 - val_loss: 6.6320 Epoch 398/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3240 - val_loss: 6.6150 Epoch 399/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3228 - val_loss: 6.5981 Epoch 400/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3217 - val_loss: 6.5811 Epoch 401/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3207 - val_loss: 6.5645 Epoch 402/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3195 - val_loss: 6.5477 Epoch 403/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3184 - val_loss: 6.5309 Epoch 404/500 6/6 [==============================] - 0s 4ms/step - loss: 0.3173 - val_loss: 6.5144 Epoch 405/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3161 - val_loss: 6.4976 Epoch 406/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3150 - val_loss: 6.4804 Epoch 407/500 6/6 [==============================] - 0s 4ms/step - loss: 0.3139 - val_loss: 6.4638 Epoch 408/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3128 - val_loss: 6.4458 Epoch 409/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3117 - val_loss: 6.4281 Epoch 410/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3106 - val_loss: 6.4111 Epoch 411/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3096 - val_loss: 6.3941 Epoch 412/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3085 - val_loss: 6.3764 Epoch 413/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3074 - val_loss: 6.3594 Epoch 414/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3065 - val_loss: 6.3420 Epoch 415/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3054 - val_loss: 6.3241 Epoch 416/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3044 - val_loss: 6.3073 Epoch 417/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3034 - val_loss: 6.2903 Epoch 418/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3025 - val_loss: 6.2734 Epoch 419/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3013 - val_loss: 6.2575 Epoch 420/500 6/6 [==============================] - 0s 3ms/step - loss: 0.3004 - val_loss: 6.2409 Epoch 421/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2994 - val_loss: 6.2242 Epoch 422/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2984 - val_loss: 6.2075 Epoch 423/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2974 - val_loss: 6.1904 Epoch 424/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2964 - val_loss: 6.1737 Epoch 425/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2954 - val_loss: 6.1569 Epoch 426/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2945 - val_loss: 6.1407 Epoch 427/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2936 - val_loss: 6.1236 Epoch 428/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2926 - val_loss: 6.1073 Epoch 429/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2916 - val_loss: 6.0908 Epoch 430/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2908 - val_loss: 6.0741 Epoch 431/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2898 - val_loss: 6.0571 Epoch 432/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2889 - val_loss: 6.0399 Epoch 433/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2879 - val_loss: 6.0221 Epoch 434/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2870 - val_loss: 6.0051 Epoch 435/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2861 - val_loss: 5.9884 Epoch 436/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2851 - val_loss: 5.9713 Epoch 437/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2843 - val_loss: 5.9546 Epoch 438/500 6/6 [==============================] - 0s 4ms/step - loss: 0.2833 - val_loss: 5.9381 Epoch 439/500 6/6 [==============================] - 0s 4ms/step - loss: 0.2824 - val_loss: 5.9211 Epoch 440/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2815 - val_loss: 5.9039 Epoch 441/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2806 - val_loss: 5.8861 Epoch 442/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2797 - val_loss: 5.8693 Epoch 443/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2789 - val_loss: 5.8528 Epoch 444/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2781 - val_loss: 5.8350 Epoch 445/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2772 - val_loss: 5.8184 Epoch 446/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2763 - val_loss: 5.8015 Epoch 447/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2754 - val_loss: 5.7853 Epoch 448/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2746 - val_loss: 5.7684 Epoch 449/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2737 - val_loss: 5.7513 Epoch 450/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2729 - val_loss: 5.7344 Epoch 451/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2720 - val_loss: 5.7173 Epoch 452/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2711 - val_loss: 5.7008 Epoch 453/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2703 - val_loss: 5.6843 Epoch 454/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2695 - val_loss: 5.6665 Epoch 455/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2687 - val_loss: 5.6494 Epoch 456/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2678 - val_loss: 5.6318 Epoch 457/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2670 - val_loss: 5.6147 Epoch 458/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2662 - val_loss: 5.5980 Epoch 459/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2654 - val_loss: 5.5813 Epoch 460/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2646 - val_loss: 5.5641 Epoch 461/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2638 - val_loss: 5.5476 Epoch 462/500 6/6 [==============================] - 0s 4ms/step - loss: 0.2630 - val_loss: 5.5310 Epoch 463/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2622 - val_loss: 5.5145 Epoch 464/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2613 - val_loss: 5.4979 Epoch 465/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2605 - val_loss: 5.4806 Epoch 466/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2597 - val_loss: 5.4643 Epoch 467/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2589 - val_loss: 5.4476 Epoch 468/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2581 - val_loss: 5.4310 Epoch 469/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2574 - val_loss: 5.4142 Epoch 470/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2566 - val_loss: 5.3978 Epoch 471/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2558 - val_loss: 5.3805 Epoch 472/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2550 - val_loss: 5.3637 Epoch 473/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2542 - val_loss: 5.3466 Epoch 474/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2535 - val_loss: 5.3306 Epoch 475/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2527 - val_loss: 5.3135 Epoch 476/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2519 - val_loss: 5.2972 Epoch 477/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2511 - val_loss: 5.2801 Epoch 478/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2504 - val_loss: 5.2632 Epoch 479/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2495 - val_loss: 5.2459 Epoch 480/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2487 - val_loss: 5.2288 Epoch 481/500 6/6 [==============================] - 0s 4ms/step - loss: 0.2480 - val_loss: 5.2122 Epoch 482/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2472 - val_loss: 5.1952 Epoch 483/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2464 - val_loss: 5.1784 Epoch 484/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2456 - val_loss: 5.1620 Epoch 485/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2449 - val_loss: 5.1452 Epoch 486/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2441 - val_loss: 5.1281 Epoch 487/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2434 - val_loss: 5.1113 Epoch 488/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2426 - val_loss: 5.0950 Epoch 489/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2419 - val_loss: 5.0791 Epoch 490/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2411 - val_loss: 5.0621 Epoch 491/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2404 - val_loss: 5.0454 Epoch 492/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2396 - val_loss: 5.0286 Epoch 493/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2389 - val_loss: 5.0118 Epoch 494/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2381 - val_loss: 4.9955 Epoch 495/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2374 - val_loss: 4.9795 Epoch 496/500 6/6 [==============================] - 0s 4ms/step - loss: 0.2367 - val_loss: 4.9630 Epoch 497/500 6/6 [==============================] - 0s 4ms/step - loss: 0.2360 - val_loss: 4.9465 Epoch 498/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2352 - val_loss: 4.9300 Epoch 499/500 6/6 [==============================] - 0s 4ms/step - loss: 0.2344 - val_loss: 4.9141 Epoch 500/500 6/6 [==============================] - 0s 3ms/step - loss: 0.2337 - val_loss: 4.8977 . &lt;keras.callbacks.History at 0x7f70988bbe50&gt; . . - 결과시각화 . plt.plot(y,&#39;.&#39;,alpha=0.1) plt.plot(net(X),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f708bd1e650&gt;] . - 보여준 데이터에서는 잘 맞추는것 같지만 validation에서는 엉망이다. -&gt; 오버피팅인가? -&gt; 텐서보드로 확인 . #%tensorboard --logdir logs --host 0.0.0.0 . 확인결과: 에폭마다 val_loss 가 줄어들고 있기는 하다 (늦게 줄어들뿐) -&gt; 오버피팅이라기보다 val_set에 들어있는 자료를 예측하기에는 보여준 데이터가 불충분하다라고 해석하는것이 더 옳음 (모형자체의 문제는 아님) | . - 해결하는 방법? 그냥 더 학습시키면된다. . !rm -rf logs net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) net.fit(X,y,epochs=2000,batch_size=100, validation_split=0.45, callbacks=tf.keras.callbacks.TensorBoard()) # 텐서보드를 이용한 시각화기능 추가 # validation_split 이용 . Epoch 1/2000 6/6 [==============================] - 0s 9ms/step - loss: 7.9563 - val_loss: 17.0027 Epoch 2/2000 6/6 [==============================] - 0s 4ms/step - loss: 7.9013 - val_loss: 16.9412 Epoch 3/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.8463 - val_loss: 16.8810 Epoch 4/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.7913 - val_loss: 16.8207 Epoch 5/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.7380 - val_loss: 16.7617 Epoch 6/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.6829 - val_loss: 16.7032 Epoch 7/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.6308 - val_loss: 16.6446 Epoch 8/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.5769 - val_loss: 16.5886 Epoch 9/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.5245 - val_loss: 16.5321 Epoch 10/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.4717 - val_loss: 16.4760 Epoch 11/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.4201 - val_loss: 16.4201 Epoch 12/2000 6/6 [==============================] - 0s 4ms/step - loss: 7.3686 - val_loss: 16.3636 Epoch 13/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.3162 - val_loss: 16.3079 Epoch 14/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.2647 - val_loss: 16.2527 Epoch 15/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.2148 - val_loss: 16.1978 Epoch 16/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.1634 - val_loss: 16.1427 Epoch 17/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.1142 - val_loss: 16.0877 Epoch 18/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.0632 - val_loss: 16.0335 Epoch 19/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.0148 - val_loss: 15.9786 Epoch 20/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.9651 - val_loss: 15.9238 Epoch 21/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.9164 - val_loss: 15.8699 Epoch 22/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.8670 - val_loss: 15.8155 Epoch 23/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.8191 - val_loss: 15.7617 Epoch 24/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.7714 - val_loss: 15.7093 Epoch 25/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.7231 - val_loss: 15.6568 Epoch 26/2000 6/6 [==============================] - 0s 4ms/step - loss: 6.6762 - val_loss: 15.6048 Epoch 27/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.6286 - val_loss: 15.5528 Epoch 28/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.5816 - val_loss: 15.5012 Epoch 29/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.5349 - val_loss: 15.4495 Epoch 30/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.4885 - val_loss: 15.3982 Epoch 31/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.4417 - val_loss: 15.3463 Epoch 32/2000 6/6 [==============================] - 0s 4ms/step - loss: 6.3956 - val_loss: 15.2957 Epoch 33/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.3508 - val_loss: 15.2442 Epoch 34/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.3053 - val_loss: 15.1941 Epoch 35/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.2593 - val_loss: 15.1447 Epoch 36/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.2142 - val_loss: 15.0943 Epoch 37/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.1706 - val_loss: 15.0446 Epoch 38/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.1261 - val_loss: 14.9966 Epoch 39/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.0825 - val_loss: 14.9488 Epoch 40/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.0394 - val_loss: 14.9001 Epoch 41/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.9965 - val_loss: 14.8519 Epoch 42/2000 6/6 [==============================] - 0s 4ms/step - loss: 5.9530 - val_loss: 14.8035 Epoch 43/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.9106 - val_loss: 14.7552 Epoch 44/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.8678 - val_loss: 14.7074 Epoch 45/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.8262 - val_loss: 14.6600 Epoch 46/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.7841 - val_loss: 14.6128 Epoch 47/2000 6/6 [==============================] - 0s 4ms/step - loss: 5.7422 - val_loss: 14.5659 Epoch 48/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.7007 - val_loss: 14.5194 Epoch 49/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.6598 - val_loss: 14.4737 Epoch 50/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.6183 - val_loss: 14.4275 Epoch 51/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.5779 - val_loss: 14.3826 Epoch 52/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.5376 - val_loss: 14.3368 Epoch 53/2000 6/6 [==============================] - 0s 4ms/step - loss: 5.4973 - val_loss: 14.2902 Epoch 54/2000 6/6 [==============================] - 0s 4ms/step - loss: 5.4572 - val_loss: 14.2453 Epoch 55/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.4176 - val_loss: 14.2002 Epoch 56/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.3785 - val_loss: 14.1551 Epoch 57/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.3387 - val_loss: 14.1109 Epoch 58/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.2996 - val_loss: 14.0679 Epoch 59/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.2613 - val_loss: 14.0241 Epoch 60/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.2221 - val_loss: 13.9811 Epoch 61/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.1839 - val_loss: 13.9382 Epoch 62/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.1467 - val_loss: 13.8949 Epoch 63/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.1086 - val_loss: 13.8506 Epoch 64/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.0710 - val_loss: 13.8065 Epoch 65/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.0336 - val_loss: 13.7637 Epoch 66/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.9956 - val_loss: 13.7212 Epoch 67/2000 6/6 [==============================] - 0s 4ms/step - loss: 4.9589 - val_loss: 13.6790 Epoch 68/2000 6/6 [==============================] - 0s 2ms/step - loss: 4.9218 - val_loss: 13.6360 Epoch 69/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.8858 - val_loss: 13.5936 Epoch 70/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.8495 - val_loss: 13.5514 Epoch 71/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.8133 - val_loss: 13.5099 Epoch 72/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.7778 - val_loss: 13.4699 Epoch 73/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.7419 - val_loss: 13.4284 Epoch 74/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.7071 - val_loss: 13.3875 Epoch 75/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.6717 - val_loss: 13.3462 Epoch 76/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.6372 - val_loss: 13.3066 Epoch 77/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.6033 - val_loss: 13.2665 Epoch 78/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.5686 - val_loss: 13.2260 Epoch 79/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.5349 - val_loss: 13.1872 Epoch 80/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.5012 - val_loss: 13.1474 Epoch 81/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.4672 - val_loss: 13.1093 Epoch 82/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.4336 - val_loss: 13.0714 Epoch 83/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.4009 - val_loss: 13.0337 Epoch 84/2000 6/6 [==============================] - 0s 4ms/step - loss: 4.3675 - val_loss: 12.9957 Epoch 85/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.3348 - val_loss: 12.9574 Epoch 86/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.3025 - val_loss: 12.9182 Epoch 87/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.2700 - val_loss: 12.8800 Epoch 88/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.2382 - val_loss: 12.8425 Epoch 89/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.2063 - val_loss: 12.8047 Epoch 90/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.1745 - val_loss: 12.7663 Epoch 91/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.1428 - val_loss: 12.7295 Epoch 92/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.1119 - val_loss: 12.6919 Epoch 93/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.0804 - val_loss: 12.6557 Epoch 94/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.0493 - val_loss: 12.6196 Epoch 95/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.0188 - val_loss: 12.5834 Epoch 96/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.9878 - val_loss: 12.5464 Epoch 97/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.9580 - val_loss: 12.5099 Epoch 98/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.9274 - val_loss: 12.4748 Epoch 99/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.8977 - val_loss: 12.4391 Epoch 100/2000 6/6 [==============================] - 0s 4ms/step - loss: 3.8678 - val_loss: 12.4017 Epoch 101/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.8381 - val_loss: 12.3662 Epoch 102/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.8084 - val_loss: 12.3317 Epoch 103/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.7793 - val_loss: 12.2967 Epoch 104/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.7504 - val_loss: 12.2616 Epoch 105/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.7214 - val_loss: 12.2255 Epoch 106/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.6928 - val_loss: 12.1913 Epoch 107/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.6639 - val_loss: 12.1572 Epoch 108/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.6359 - val_loss: 12.1229 Epoch 109/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.6079 - val_loss: 12.0890 Epoch 110/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.5798 - val_loss: 12.0553 Epoch 111/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.5520 - val_loss: 12.0209 Epoch 112/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.5245 - val_loss: 11.9872 Epoch 113/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.4968 - val_loss: 11.9544 Epoch 114/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.4703 - val_loss: 11.9212 Epoch 115/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.4430 - val_loss: 11.8894 Epoch 116/2000 6/6 [==============================] - 0s 4ms/step - loss: 3.4164 - val_loss: 11.8567 Epoch 117/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.3899 - val_loss: 11.8250 Epoch 118/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.3633 - val_loss: 11.7929 Epoch 119/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.3368 - val_loss: 11.7604 Epoch 120/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.3108 - val_loss: 11.7281 Epoch 121/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.2852 - val_loss: 11.6949 Epoch 122/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.2593 - val_loss: 11.6615 Epoch 123/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.2339 - val_loss: 11.6295 Epoch 124/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.2084 - val_loss: 11.5968 Epoch 125/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.1835 - val_loss: 11.5650 Epoch 126/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.1585 - val_loss: 11.5338 Epoch 127/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.1335 - val_loss: 11.5018 Epoch 128/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.1093 - val_loss: 11.4705 Epoch 129/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.0847 - val_loss: 11.4392 Epoch 130/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.0604 - val_loss: 11.4076 Epoch 131/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.0360 - val_loss: 11.3776 Epoch 132/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.0123 - val_loss: 11.3460 Epoch 133/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.9884 - val_loss: 11.3158 Epoch 134/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.9648 - val_loss: 11.2869 Epoch 135/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.9414 - val_loss: 11.2564 Epoch 136/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.9179 - val_loss: 11.2282 Epoch 137/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.8950 - val_loss: 11.1989 Epoch 138/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.8720 - val_loss: 11.1695 Epoch 139/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.8492 - val_loss: 11.1400 Epoch 140/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.8265 - val_loss: 11.1121 Epoch 141/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.8047 - val_loss: 11.0833 Epoch 142/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7819 - val_loss: 11.0539 Epoch 143/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7598 - val_loss: 11.0244 Epoch 144/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7381 - val_loss: 10.9954 Epoch 145/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.7157 - val_loss: 10.9659 Epoch 146/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6944 - val_loss: 10.9374 Epoch 147/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6728 - val_loss: 10.9100 Epoch 148/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6517 - val_loss: 10.8820 Epoch 149/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6303 - val_loss: 10.8545 Epoch 150/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6098 - val_loss: 10.8258 Epoch 151/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5886 - val_loss: 10.7982 Epoch 152/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5681 - val_loss: 10.7702 Epoch 153/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5477 - val_loss: 10.7426 Epoch 154/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5268 - val_loss: 10.7159 Epoch 155/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5070 - val_loss: 10.6879 Epoch 156/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4865 - val_loss: 10.6615 Epoch 157/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4665 - val_loss: 10.6347 Epoch 158/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.4471 - val_loss: 10.6074 Epoch 159/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4269 - val_loss: 10.5801 Epoch 160/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4074 - val_loss: 10.5535 Epoch 161/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.3880 - val_loss: 10.5256 Epoch 162/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.3687 - val_loss: 10.4981 Epoch 163/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3497 - val_loss: 10.4711 Epoch 164/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3308 - val_loss: 10.4447 Epoch 165/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3117 - val_loss: 10.4198 Epoch 166/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2933 - val_loss: 10.3949 Epoch 167/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2749 - val_loss: 10.3696 Epoch 168/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2567 - val_loss: 10.3437 Epoch 169/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2385 - val_loss: 10.3177 Epoch 170/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2204 - val_loss: 10.2909 Epoch 171/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2022 - val_loss: 10.2653 Epoch 172/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1842 - val_loss: 10.2391 Epoch 173/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1669 - val_loss: 10.2139 Epoch 174/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1493 - val_loss: 10.1893 Epoch 175/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1320 - val_loss: 10.1643 Epoch 176/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1146 - val_loss: 10.1393 Epoch 177/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0978 - val_loss: 10.1148 Epoch 178/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0802 - val_loss: 10.0900 Epoch 179/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0637 - val_loss: 10.0662 Epoch 180/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.0470 - val_loss: 10.0430 Epoch 181/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.0304 - val_loss: 10.0192 Epoch 182/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0140 - val_loss: 9.9947 Epoch 183/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.9974 - val_loss: 9.9698 Epoch 184/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9812 - val_loss: 9.9468 Epoch 185/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.9653 - val_loss: 9.9227 Epoch 186/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9492 - val_loss: 9.8996 Epoch 187/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9333 - val_loss: 9.8767 Epoch 188/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9175 - val_loss: 9.8542 Epoch 189/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9018 - val_loss: 9.8322 Epoch 190/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8864 - val_loss: 9.8097 Epoch 191/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8710 - val_loss: 9.7886 Epoch 192/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.8560 - val_loss: 9.7661 Epoch 193/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8408 - val_loss: 9.7440 Epoch 194/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8261 - val_loss: 9.7216 Epoch 195/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8109 - val_loss: 9.6998 Epoch 196/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7963 - val_loss: 9.6770 Epoch 197/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.7817 - val_loss: 9.6547 Epoch 198/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7671 - val_loss: 9.6326 Epoch 199/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7529 - val_loss: 9.6101 Epoch 200/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7385 - val_loss: 9.5889 Epoch 201/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7247 - val_loss: 9.5673 Epoch 202/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.7103 - val_loss: 9.5462 Epoch 203/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6965 - val_loss: 9.5249 Epoch 204/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6832 - val_loss: 9.5034 Epoch 205/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6688 - val_loss: 9.4818 Epoch 206/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6554 - val_loss: 9.4604 Epoch 207/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6420 - val_loss: 9.4388 Epoch 208/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.6284 - val_loss: 9.4175 Epoch 209/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6153 - val_loss: 9.3953 Epoch 210/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6020 - val_loss: 9.3753 Epoch 211/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5890 - val_loss: 9.3550 Epoch 212/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5760 - val_loss: 9.3347 Epoch 213/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5630 - val_loss: 9.3146 Epoch 214/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5504 - val_loss: 9.2938 Epoch 215/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.5378 - val_loss: 9.2736 Epoch 216/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.5254 - val_loss: 9.2528 Epoch 217/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5129 - val_loss: 9.2324 Epoch 218/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5007 - val_loss: 9.2127 Epoch 219/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4883 - val_loss: 9.1932 Epoch 220/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4761 - val_loss: 9.1740 Epoch 221/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4642 - val_loss: 9.1550 Epoch 222/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4522 - val_loss: 9.1361 Epoch 223/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4407 - val_loss: 9.1162 Epoch 224/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4289 - val_loss: 9.0964 Epoch 225/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4172 - val_loss: 9.0773 Epoch 226/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4058 - val_loss: 9.0589 Epoch 227/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.3944 - val_loss: 9.0388 Epoch 228/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3833 - val_loss: 9.0196 Epoch 229/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3721 - val_loss: 9.0016 Epoch 230/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3607 - val_loss: 8.9826 Epoch 231/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3498 - val_loss: 8.9648 Epoch 232/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3390 - val_loss: 8.9451 Epoch 233/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3282 - val_loss: 8.9263 Epoch 234/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3173 - val_loss: 8.9072 Epoch 235/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3065 - val_loss: 8.8890 Epoch 236/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2960 - val_loss: 8.8710 Epoch 237/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2857 - val_loss: 8.8527 Epoch 238/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2753 - val_loss: 8.8343 Epoch 239/2000 6/6 [==============================] - 0s 2ms/step - loss: 1.2650 - val_loss: 8.8167 Epoch 240/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2549 - val_loss: 8.7986 Epoch 241/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2446 - val_loss: 8.7807 Epoch 242/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2348 - val_loss: 8.7634 Epoch 243/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2248 - val_loss: 8.7452 Epoch 244/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2149 - val_loss: 8.7272 Epoch 245/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2051 - val_loss: 8.7098 Epoch 246/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.1953 - val_loss: 8.6919 Epoch 247/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1859 - val_loss: 8.6734 Epoch 248/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1761 - val_loss: 8.6563 Epoch 249/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.1668 - val_loss: 8.6384 Epoch 250/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1575 - val_loss: 8.6211 Epoch 251/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1482 - val_loss: 8.6034 Epoch 252/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1391 - val_loss: 8.5858 Epoch 253/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.1299 - val_loss: 8.5696 Epoch 254/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1210 - val_loss: 8.5526 Epoch 255/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1121 - val_loss: 8.5356 Epoch 256/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1033 - val_loss: 8.5179 Epoch 257/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0945 - val_loss: 8.5020 Epoch 258/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0857 - val_loss: 8.4856 Epoch 259/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0772 - val_loss: 8.4692 Epoch 260/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0686 - val_loss: 8.4531 Epoch 261/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0601 - val_loss: 8.4360 Epoch 262/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0518 - val_loss: 8.4194 Epoch 263/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0433 - val_loss: 8.4037 Epoch 264/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0351 - val_loss: 8.3865 Epoch 265/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0271 - val_loss: 8.3690 Epoch 266/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0190 - val_loss: 8.3524 Epoch 267/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0108 - val_loss: 8.3361 Epoch 268/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0029 - val_loss: 8.3203 Epoch 269/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.9951 - val_loss: 8.3041 Epoch 270/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9873 - val_loss: 8.2878 Epoch 271/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9796 - val_loss: 8.2707 Epoch 272/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9719 - val_loss: 8.2545 Epoch 273/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.9642 - val_loss: 8.2390 Epoch 274/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.9568 - val_loss: 8.2218 Epoch 275/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9494 - val_loss: 8.2058 Epoch 276/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9420 - val_loss: 8.1896 Epoch 277/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9347 - val_loss: 8.1732 Epoch 278/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9275 - val_loss: 8.1561 Epoch 279/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9202 - val_loss: 8.1401 Epoch 280/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9131 - val_loss: 8.1239 Epoch 281/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9060 - val_loss: 8.1088 Epoch 282/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8990 - val_loss: 8.0931 Epoch 283/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8922 - val_loss: 8.0769 Epoch 284/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8853 - val_loss: 8.0603 Epoch 285/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8784 - val_loss: 8.0448 Epoch 286/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8718 - val_loss: 8.0294 Epoch 287/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8652 - val_loss: 8.0138 Epoch 288/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8584 - val_loss: 7.9985 Epoch 289/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8520 - val_loss: 7.9826 Epoch 290/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.8454 - val_loss: 7.9671 Epoch 291/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.8390 - val_loss: 7.9519 Epoch 292/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8326 - val_loss: 7.9362 Epoch 293/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8263 - val_loss: 7.9212 Epoch 294/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8201 - val_loss: 7.9061 Epoch 295/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8138 - val_loss: 7.8905 Epoch 296/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8079 - val_loss: 7.8750 Epoch 297/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8017 - val_loss: 7.8607 Epoch 298/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7958 - val_loss: 7.8459 Epoch 299/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7899 - val_loss: 7.8306 Epoch 300/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7840 - val_loss: 7.8159 Epoch 301/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7783 - val_loss: 7.8007 Epoch 302/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7725 - val_loss: 7.7864 Epoch 303/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.7669 - val_loss: 7.7719 Epoch 304/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7611 - val_loss: 7.7579 Epoch 305/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7556 - val_loss: 7.7428 Epoch 306/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7500 - val_loss: 7.7285 Epoch 307/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7445 - val_loss: 7.7141 Epoch 308/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7391 - val_loss: 7.6997 Epoch 309/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7337 - val_loss: 7.6853 Epoch 310/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7283 - val_loss: 7.6718 Epoch 311/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7231 - val_loss: 7.6580 Epoch 312/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7179 - val_loss: 7.6447 Epoch 313/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7128 - val_loss: 7.6302 Epoch 314/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7077 - val_loss: 7.6174 Epoch 315/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7026 - val_loss: 7.6038 Epoch 316/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6975 - val_loss: 7.5901 Epoch 317/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6925 - val_loss: 7.5768 Epoch 318/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6876 - val_loss: 7.5627 Epoch 319/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6827 - val_loss: 7.5483 Epoch 320/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6779 - val_loss: 7.5341 Epoch 321/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6731 - val_loss: 7.5198 Epoch 322/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6683 - val_loss: 7.5051 Epoch 323/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6636 - val_loss: 7.4905 Epoch 324/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6588 - val_loss: 7.4768 Epoch 325/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6543 - val_loss: 7.4629 Epoch 326/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6497 - val_loss: 7.4488 Epoch 327/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6452 - val_loss: 7.4349 Epoch 328/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6407 - val_loss: 7.4209 Epoch 329/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6361 - val_loss: 7.4075 Epoch 330/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.6318 - val_loss: 7.3936 Epoch 331/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6273 - val_loss: 7.3804 Epoch 332/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6230 - val_loss: 7.3673 Epoch 333/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6187 - val_loss: 7.3541 Epoch 334/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6146 - val_loss: 7.3411 Epoch 335/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6104 - val_loss: 7.3282 Epoch 336/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6063 - val_loss: 7.3151 Epoch 337/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.6021 - val_loss: 7.3019 Epoch 338/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5982 - val_loss: 7.2878 Epoch 339/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.5941 - val_loss: 7.2744 Epoch 340/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5901 - val_loss: 7.2609 Epoch 341/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5862 - val_loss: 7.2473 Epoch 342/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5822 - val_loss: 7.2341 Epoch 343/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5785 - val_loss: 7.2205 Epoch 344/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5746 - val_loss: 7.2082 Epoch 345/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5709 - val_loss: 7.1954 Epoch 346/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5672 - val_loss: 7.1823 Epoch 347/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5635 - val_loss: 7.1690 Epoch 348/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5599 - val_loss: 7.1551 Epoch 349/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5561 - val_loss: 7.1426 Epoch 350/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5527 - val_loss: 7.1290 Epoch 351/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5491 - val_loss: 7.1156 Epoch 352/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5456 - val_loss: 7.1031 Epoch 353/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.5421 - val_loss: 7.0905 Epoch 354/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5387 - val_loss: 7.0773 Epoch 355/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5352 - val_loss: 7.0639 Epoch 356/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5319 - val_loss: 7.0513 Epoch 357/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5285 - val_loss: 7.0387 Epoch 358/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.5251 - val_loss: 7.0263 Epoch 359/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5218 - val_loss: 7.0130 Epoch 360/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5186 - val_loss: 7.0000 Epoch 361/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.5154 - val_loss: 6.9864 Epoch 362/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5121 - val_loss: 6.9732 Epoch 363/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5090 - val_loss: 6.9603 Epoch 364/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5057 - val_loss: 6.9482 Epoch 365/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5027 - val_loss: 6.9357 Epoch 366/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4997 - val_loss: 6.9228 Epoch 367/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4966 - val_loss: 6.9100 Epoch 368/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4936 - val_loss: 6.8972 Epoch 369/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4906 - val_loss: 6.8840 Epoch 370/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4876 - val_loss: 6.8711 Epoch 371/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4847 - val_loss: 6.8580 Epoch 372/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4818 - val_loss: 6.8453 Epoch 373/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4790 - val_loss: 6.8319 Epoch 374/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4760 - val_loss: 6.8195 Epoch 375/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4732 - val_loss: 6.8068 Epoch 376/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4704 - val_loss: 6.7937 Epoch 377/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4676 - val_loss: 6.7809 Epoch 378/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4649 - val_loss: 6.7677 Epoch 379/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4622 - val_loss: 6.7545 Epoch 380/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4595 - val_loss: 6.7420 Epoch 381/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4569 - val_loss: 6.7292 Epoch 382/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4543 - val_loss: 6.7164 Epoch 383/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4517 - val_loss: 6.7039 Epoch 384/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4492 - val_loss: 6.6910 Epoch 385/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4467 - val_loss: 6.6781 Epoch 386/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4441 - val_loss: 6.6659 Epoch 387/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4416 - val_loss: 6.6535 Epoch 388/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4392 - val_loss: 6.6404 Epoch 389/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4367 - val_loss: 6.6274 Epoch 390/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4342 - val_loss: 6.6145 Epoch 391/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4318 - val_loss: 6.6011 Epoch 392/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4295 - val_loss: 6.5875 Epoch 393/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4270 - val_loss: 6.5747 Epoch 394/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4248 - val_loss: 6.5615 Epoch 395/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4224 - val_loss: 6.5482 Epoch 396/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4202 - val_loss: 6.5350 Epoch 397/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4178 - val_loss: 6.5221 Epoch 398/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4156 - val_loss: 6.5091 Epoch 399/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4134 - val_loss: 6.4958 Epoch 400/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4112 - val_loss: 6.4832 Epoch 401/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4090 - val_loss: 6.4701 Epoch 402/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4068 - val_loss: 6.4580 Epoch 403/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4047 - val_loss: 6.4458 Epoch 404/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4026 - val_loss: 6.4328 Epoch 405/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4005 - val_loss: 6.4201 Epoch 406/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3984 - val_loss: 6.4078 Epoch 407/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3964 - val_loss: 6.3951 Epoch 408/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3943 - val_loss: 6.3820 Epoch 409/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3923 - val_loss: 6.3687 Epoch 410/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3904 - val_loss: 6.3556 Epoch 411/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3884 - val_loss: 6.3432 Epoch 412/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3864 - val_loss: 6.3316 Epoch 413/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3845 - val_loss: 6.3191 Epoch 414/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3826 - val_loss: 6.3064 Epoch 415/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3807 - val_loss: 6.2937 Epoch 416/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3788 - val_loss: 6.2801 Epoch 417/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3770 - val_loss: 6.2673 Epoch 418/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3751 - val_loss: 6.2552 Epoch 419/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3734 - val_loss: 6.2428 Epoch 420/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3716 - val_loss: 6.2297 Epoch 421/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3697 - val_loss: 6.2176 Epoch 422/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3680 - val_loss: 6.2058 Epoch 423/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3662 - val_loss: 6.1930 Epoch 424/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3645 - val_loss: 6.1798 Epoch 425/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3628 - val_loss: 6.1656 Epoch 426/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3611 - val_loss: 6.1524 Epoch 427/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3593 - val_loss: 6.1409 Epoch 428/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3577 - val_loss: 6.1283 Epoch 429/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3560 - val_loss: 6.1157 Epoch 430/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3543 - val_loss: 6.1029 Epoch 431/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3527 - val_loss: 6.0910 Epoch 432/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3511 - val_loss: 6.0784 Epoch 433/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3495 - val_loss: 6.0659 Epoch 434/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3479 - val_loss: 6.0532 Epoch 435/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3463 - val_loss: 6.0409 Epoch 436/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3449 - val_loss: 6.0271 Epoch 437/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3433 - val_loss: 6.0139 Epoch 438/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3417 - val_loss: 6.0021 Epoch 439/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3403 - val_loss: 5.9891 Epoch 440/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3387 - val_loss: 5.9761 Epoch 441/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3373 - val_loss: 5.9631 Epoch 442/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3358 - val_loss: 5.9510 Epoch 443/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3343 - val_loss: 5.9384 Epoch 444/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3329 - val_loss: 5.9251 Epoch 445/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3315 - val_loss: 5.9124 Epoch 446/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3300 - val_loss: 5.9006 Epoch 447/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3286 - val_loss: 5.8884 Epoch 448/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3273 - val_loss: 5.8753 Epoch 449/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3259 - val_loss: 5.8638 Epoch 450/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3246 - val_loss: 5.8502 Epoch 451/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3232 - val_loss: 5.8370 Epoch 452/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3219 - val_loss: 5.8248 Epoch 453/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3205 - val_loss: 5.8125 Epoch 454/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.3192 - val_loss: 5.8001 Epoch 455/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3179 - val_loss: 5.7872 Epoch 456/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3166 - val_loss: 5.7746 Epoch 457/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3153 - val_loss: 5.7613 Epoch 458/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3141 - val_loss: 5.7485 Epoch 459/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3128 - val_loss: 5.7363 Epoch 460/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3115 - val_loss: 5.7242 Epoch 461/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3102 - val_loss: 5.7113 Epoch 462/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3091 - val_loss: 5.6971 Epoch 463/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3078 - val_loss: 5.6843 Epoch 464/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3066 - val_loss: 5.6719 Epoch 465/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3054 - val_loss: 5.6590 Epoch 466/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3042 - val_loss: 5.6457 Epoch 467/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3030 - val_loss: 5.6332 Epoch 468/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3018 - val_loss: 5.6205 Epoch 469/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3007 - val_loss: 5.6081 Epoch 470/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2995 - val_loss: 5.5948 Epoch 471/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2984 - val_loss: 5.5822 Epoch 472/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2973 - val_loss: 5.5698 Epoch 473/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2962 - val_loss: 5.5573 Epoch 474/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2950 - val_loss: 5.5454 Epoch 475/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2939 - val_loss: 5.5330 Epoch 476/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2929 - val_loss: 5.5206 Epoch 477/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2918 - val_loss: 5.5077 Epoch 478/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2907 - val_loss: 5.4955 Epoch 479/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2897 - val_loss: 5.4826 Epoch 480/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2886 - val_loss: 5.4703 Epoch 481/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2875 - val_loss: 5.4572 Epoch 482/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2865 - val_loss: 5.4437 Epoch 483/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2854 - val_loss: 5.4312 Epoch 484/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2844 - val_loss: 5.4180 Epoch 485/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2833 - val_loss: 5.4055 Epoch 486/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2823 - val_loss: 5.3928 Epoch 487/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2812 - val_loss: 5.3801 Epoch 488/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2803 - val_loss: 5.3680 Epoch 489/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2793 - val_loss: 5.3553 Epoch 490/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2784 - val_loss: 5.3432 Epoch 491/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2774 - val_loss: 5.3311 Epoch 492/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2764 - val_loss: 5.3187 Epoch 493/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2755 - val_loss: 5.3060 Epoch 494/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2745 - val_loss: 5.2937 Epoch 495/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2735 - val_loss: 5.2808 Epoch 496/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2726 - val_loss: 5.2676 Epoch 497/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2717 - val_loss: 5.2543 Epoch 498/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2707 - val_loss: 5.2419 Epoch 499/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2697 - val_loss: 5.2294 Epoch 500/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2688 - val_loss: 5.2162 Epoch 501/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2679 - val_loss: 5.2035 Epoch 502/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2670 - val_loss: 5.1906 Epoch 503/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2661 - val_loss: 5.1787 Epoch 504/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2653 - val_loss: 5.1656 Epoch 505/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2643 - val_loss: 5.1536 Epoch 506/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2635 - val_loss: 5.1414 Epoch 507/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2626 - val_loss: 5.1286 Epoch 508/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2618 - val_loss: 5.1159 Epoch 509/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2609 - val_loss: 5.1031 Epoch 510/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2600 - val_loss: 5.0902 Epoch 511/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2592 - val_loss: 5.0771 Epoch 512/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2583 - val_loss: 5.0634 Epoch 513/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2574 - val_loss: 5.0508 Epoch 514/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2566 - val_loss: 5.0378 Epoch 515/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2558 - val_loss: 5.0251 Epoch 516/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2549 - val_loss: 5.0124 Epoch 517/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2541 - val_loss: 5.0009 Epoch 518/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2533 - val_loss: 4.9884 Epoch 519/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2524 - val_loss: 4.9745 Epoch 520/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2516 - val_loss: 4.9611 Epoch 521/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2508 - val_loss: 4.9477 Epoch 522/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2500 - val_loss: 4.9343 Epoch 523/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2492 - val_loss: 4.9215 Epoch 524/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2483 - val_loss: 4.9094 Epoch 525/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2476 - val_loss: 4.8964 Epoch 526/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2468 - val_loss: 4.8839 Epoch 527/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2460 - val_loss: 4.8708 Epoch 528/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2452 - val_loss: 4.8576 Epoch 529/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2444 - val_loss: 4.8448 Epoch 530/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2437 - val_loss: 4.8320 Epoch 531/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2429 - val_loss: 4.8194 Epoch 532/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2421 - val_loss: 4.8058 Epoch 533/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2413 - val_loss: 4.7929 Epoch 534/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2406 - val_loss: 4.7801 Epoch 535/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2398 - val_loss: 4.7671 Epoch 536/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2390 - val_loss: 4.7535 Epoch 537/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2382 - val_loss: 4.7403 Epoch 538/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2375 - val_loss: 4.7273 Epoch 539/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2367 - val_loss: 4.7137 Epoch 540/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2360 - val_loss: 4.7006 Epoch 541/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2352 - val_loss: 4.6876 Epoch 542/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2345 - val_loss: 4.6743 Epoch 543/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2337 - val_loss: 4.6610 Epoch 544/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2330 - val_loss: 4.6474 Epoch 545/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2323 - val_loss: 4.6331 Epoch 546/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2316 - val_loss: 4.6199 Epoch 547/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2308 - val_loss: 4.6071 Epoch 548/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2301 - val_loss: 4.5937 Epoch 549/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2294 - val_loss: 4.5801 Epoch 550/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2287 - val_loss: 4.5669 Epoch 551/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2280 - val_loss: 4.5541 Epoch 552/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2273 - val_loss: 4.5418 Epoch 553/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2266 - val_loss: 4.5291 Epoch 554/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2259 - val_loss: 4.5153 Epoch 555/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2252 - val_loss: 4.5024 Epoch 556/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2245 - val_loss: 4.4886 Epoch 557/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2238 - val_loss: 4.4760 Epoch 558/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2231 - val_loss: 4.4632 Epoch 559/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2225 - val_loss: 4.4498 Epoch 560/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2218 - val_loss: 4.4371 Epoch 561/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2211 - val_loss: 4.4243 Epoch 562/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2205 - val_loss: 4.4102 Epoch 563/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2198 - val_loss: 4.3965 Epoch 564/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2191 - val_loss: 4.3836 Epoch 565/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2184 - val_loss: 4.3712 Epoch 566/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2178 - val_loss: 4.3576 Epoch 567/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 4.3441 Epoch 568/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2164 - val_loss: 4.3317 Epoch 569/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2158 - val_loss: 4.3184 Epoch 570/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2151 - val_loss: 4.3051 Epoch 571/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2145 - val_loss: 4.2917 Epoch 572/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2138 - val_loss: 4.2786 Epoch 573/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2132 - val_loss: 4.2653 Epoch 574/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2125 - val_loss: 4.2523 Epoch 575/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2119 - val_loss: 4.2398 Epoch 576/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2112 - val_loss: 4.2273 Epoch 577/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2107 - val_loss: 4.2141 Epoch 578/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2100 - val_loss: 4.2012 Epoch 579/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2094 - val_loss: 4.1880 Epoch 580/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2087 - val_loss: 4.1755 Epoch 581/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2081 - val_loss: 4.1625 Epoch 582/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2075 - val_loss: 4.1492 Epoch 583/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2068 - val_loss: 4.1359 Epoch 584/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2062 - val_loss: 4.1222 Epoch 585/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2055 - val_loss: 4.1091 Epoch 586/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2050 - val_loss: 4.0958 Epoch 587/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2043 - val_loss: 4.0840 Epoch 588/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2037 - val_loss: 4.0713 Epoch 589/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2031 - val_loss: 4.0589 Epoch 590/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2025 - val_loss: 4.0462 Epoch 591/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2018 - val_loss: 4.0341 Epoch 592/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2013 - val_loss: 4.0212 Epoch 593/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2007 - val_loss: 4.0088 Epoch 594/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2001 - val_loss: 3.9966 Epoch 595/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1995 - val_loss: 3.9836 Epoch 596/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 3.9706 Epoch 597/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1982 - val_loss: 3.9582 Epoch 598/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1977 - val_loss: 3.9448 Epoch 599/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1971 - val_loss: 3.9314 Epoch 600/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1964 - val_loss: 3.9187 Epoch 601/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1959 - val_loss: 3.9060 Epoch 602/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1953 - val_loss: 3.8930 Epoch 603/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1947 - val_loss: 3.8804 Epoch 604/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1941 - val_loss: 3.8667 Epoch 605/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1935 - val_loss: 3.8544 Epoch 606/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1929 - val_loss: 3.8421 Epoch 607/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1924 - val_loss: 3.8289 Epoch 608/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1917 - val_loss: 3.8162 Epoch 609/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1912 - val_loss: 3.8026 Epoch 610/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1906 - val_loss: 3.7897 Epoch 611/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1900 - val_loss: 3.7766 Epoch 612/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1894 - val_loss: 3.7639 Epoch 613/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1888 - val_loss: 3.7507 Epoch 614/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1882 - val_loss: 3.7374 Epoch 615/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1877 - val_loss: 3.7244 Epoch 616/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1871 - val_loss: 3.7116 Epoch 617/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1865 - val_loss: 3.6990 Epoch 618/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1859 - val_loss: 3.6861 Epoch 619/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1854 - val_loss: 3.6728 Epoch 620/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1847 - val_loss: 3.6605 Epoch 621/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1842 - val_loss: 3.6471 Epoch 622/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1836 - val_loss: 3.6345 Epoch 623/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1830 - val_loss: 3.6220 Epoch 624/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1825 - val_loss: 3.6088 Epoch 625/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1819 - val_loss: 3.5953 Epoch 626/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1813 - val_loss: 3.5819 Epoch 627/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1807 - val_loss: 3.5693 Epoch 628/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1802 - val_loss: 3.5565 Epoch 629/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1796 - val_loss: 3.5439 Epoch 630/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1791 - val_loss: 3.5319 Epoch 631/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1785 - val_loss: 3.5202 Epoch 632/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1780 - val_loss: 3.5072 Epoch 633/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1774 - val_loss: 3.4947 Epoch 634/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1768 - val_loss: 3.4815 Epoch 635/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1763 - val_loss: 3.4685 Epoch 636/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1757 - val_loss: 3.4563 Epoch 637/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1751 - val_loss: 3.4440 Epoch 638/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1746 - val_loss: 3.4307 Epoch 639/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1741 - val_loss: 3.4180 Epoch 640/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1735 - val_loss: 3.4056 Epoch 641/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1729 - val_loss: 3.3936 Epoch 642/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1724 - val_loss: 3.3813 Epoch 643/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1719 - val_loss: 3.3682 Epoch 644/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1713 - val_loss: 3.3556 Epoch 645/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1708 - val_loss: 3.3429 Epoch 646/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1702 - val_loss: 3.3306 Epoch 647/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1697 - val_loss: 3.3180 Epoch 648/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1691 - val_loss: 3.3060 Epoch 649/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1686 - val_loss: 3.2933 Epoch 650/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1681 - val_loss: 3.2807 Epoch 651/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1675 - val_loss: 3.2675 Epoch 652/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1670 - val_loss: 3.2545 Epoch 653/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1664 - val_loss: 3.2423 Epoch 654/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1658 - val_loss: 3.2296 Epoch 655/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1653 - val_loss: 3.2170 Epoch 656/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1648 - val_loss: 3.2043 Epoch 657/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1642 - val_loss: 3.1921 Epoch 658/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1637 - val_loss: 3.1786 Epoch 659/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1631 - val_loss: 3.1659 Epoch 660/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1626 - val_loss: 3.1534 Epoch 661/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1621 - val_loss: 3.1400 Epoch 662/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1615 - val_loss: 3.1277 Epoch 663/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1610 - val_loss: 3.1148 Epoch 664/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1604 - val_loss: 3.1015 Epoch 665/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1599 - val_loss: 3.0887 Epoch 666/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 3.0764 Epoch 667/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1588 - val_loss: 3.0640 Epoch 668/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1583 - val_loss: 3.0516 Epoch 669/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1578 - val_loss: 3.0398 Epoch 670/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1572 - val_loss: 3.0274 Epoch 671/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1567 - val_loss: 3.0159 Epoch 672/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1562 - val_loss: 3.0039 Epoch 673/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1557 - val_loss: 2.9921 Epoch 674/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1551 - val_loss: 2.9799 Epoch 675/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1546 - val_loss: 2.9669 Epoch 676/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1540 - val_loss: 2.9550 Epoch 677/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1535 - val_loss: 2.9428 Epoch 678/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1530 - val_loss: 2.9305 Epoch 679/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1525 - val_loss: 2.9178 Epoch 680/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1519 - val_loss: 2.9059 Epoch 681/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1514 - val_loss: 2.8935 Epoch 682/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1509 - val_loss: 2.8816 Epoch 683/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1504 - val_loss: 2.8687 Epoch 684/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1499 - val_loss: 2.8565 Epoch 685/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1494 - val_loss: 2.8445 Epoch 686/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1489 - val_loss: 2.8318 Epoch 687/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1484 - val_loss: 2.8200 Epoch 688/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1479 - val_loss: 2.8081 Epoch 689/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1474 - val_loss: 2.7957 Epoch 690/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1468 - val_loss: 2.7837 Epoch 691/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1463 - val_loss: 2.7715 Epoch 692/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1458 - val_loss: 2.7596 Epoch 693/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1453 - val_loss: 2.7476 Epoch 694/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1448 - val_loss: 2.7354 Epoch 695/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1443 - val_loss: 2.7233 Epoch 696/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1438 - val_loss: 2.7113 Epoch 697/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1433 - val_loss: 2.6996 Epoch 698/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1428 - val_loss: 2.6874 Epoch 699/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1423 - val_loss: 2.6760 Epoch 700/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1418 - val_loss: 2.6651 Epoch 701/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1413 - val_loss: 2.6537 Epoch 702/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1408 - val_loss: 2.6435 Epoch 703/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1403 - val_loss: 2.6322 Epoch 704/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1398 - val_loss: 2.6204 Epoch 705/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1394 - val_loss: 2.6084 Epoch 706/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1389 - val_loss: 2.5964 Epoch 707/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1384 - val_loss: 2.5844 Epoch 708/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1378 - val_loss: 2.5730 Epoch 709/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1374 - val_loss: 2.5604 Epoch 710/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1369 - val_loss: 2.5488 Epoch 711/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1364 - val_loss: 2.5374 Epoch 712/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1359 - val_loss: 2.5266 Epoch 713/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1354 - val_loss: 2.5150 Epoch 714/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1350 - val_loss: 2.5034 Epoch 715/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1345 - val_loss: 2.4924 Epoch 716/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1340 - val_loss: 2.4817 Epoch 717/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1335 - val_loss: 2.4699 Epoch 718/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1331 - val_loss: 2.4581 Epoch 719/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1326 - val_loss: 2.4463 Epoch 720/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1321 - val_loss: 2.4342 Epoch 721/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1316 - val_loss: 2.4222 Epoch 722/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1311 - val_loss: 2.4105 Epoch 723/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1306 - val_loss: 2.3996 Epoch 724/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1302 - val_loss: 2.3888 Epoch 725/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1297 - val_loss: 2.3774 Epoch 726/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1292 - val_loss: 2.3659 Epoch 727/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1287 - val_loss: 2.3542 Epoch 728/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1283 - val_loss: 2.3425 Epoch 729/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1278 - val_loss: 2.3306 Epoch 730/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1273 - val_loss: 2.3193 Epoch 731/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1269 - val_loss: 2.3079 Epoch 732/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1264 - val_loss: 2.2972 Epoch 733/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1260 - val_loss: 2.2863 Epoch 734/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1255 - val_loss: 2.2755 Epoch 735/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1250 - val_loss: 2.2650 Epoch 736/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1246 - val_loss: 2.2541 Epoch 737/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1241 - val_loss: 2.2434 Epoch 738/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 2.2326 Epoch 739/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 2.2211 Epoch 740/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1227 - val_loss: 2.2096 Epoch 741/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1223 - val_loss: 2.1982 Epoch 742/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1218 - val_loss: 2.1873 Epoch 743/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1213 - val_loss: 2.1772 Epoch 744/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1209 - val_loss: 2.1657 Epoch 745/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1204 - val_loss: 2.1546 Epoch 746/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1200 - val_loss: 2.1438 Epoch 747/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1195 - val_loss: 2.1334 Epoch 748/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1191 - val_loss: 2.1226 Epoch 749/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1186 - val_loss: 2.1122 Epoch 750/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1181 - val_loss: 2.1021 Epoch 751/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1177 - val_loss: 2.0914 Epoch 752/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1173 - val_loss: 2.0800 Epoch 753/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1168 - val_loss: 2.0693 Epoch 754/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1164 - val_loss: 2.0587 Epoch 755/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1159 - val_loss: 2.0479 Epoch 756/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1154 - val_loss: 2.0373 Epoch 757/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1150 - val_loss: 2.0264 Epoch 758/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1146 - val_loss: 2.0153 Epoch 759/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1141 - val_loss: 2.0050 Epoch 760/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1137 - val_loss: 1.9949 Epoch 761/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 1.9837 Epoch 762/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1128 - val_loss: 1.9731 Epoch 763/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1124 - val_loss: 1.9618 Epoch 764/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1119 - val_loss: 1.9511 Epoch 765/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1115 - val_loss: 1.9405 Epoch 766/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 1.9295 Epoch 767/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1106 - val_loss: 1.9198 Epoch 768/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.1102 - val_loss: 1.9094 Epoch 769/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1098 - val_loss: 1.8989 Epoch 770/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1094 - val_loss: 1.8886 Epoch 771/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1089 - val_loss: 1.8785 Epoch 772/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1085 - val_loss: 1.8684 Epoch 773/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1081 - val_loss: 1.8580 Epoch 774/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1077 - val_loss: 1.8486 Epoch 775/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.1073 - val_loss: 1.8389 Epoch 776/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1069 - val_loss: 1.8292 Epoch 777/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 1.8192 Epoch 778/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1061 - val_loss: 1.8095 Epoch 779/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1057 - val_loss: 1.7997 Epoch 780/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1052 - val_loss: 1.7901 Epoch 781/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1049 - val_loss: 1.7800 Epoch 782/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1044 - val_loss: 1.7697 Epoch 783/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1040 - val_loss: 1.7598 Epoch 784/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1036 - val_loss: 1.7504 Epoch 785/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1032 - val_loss: 1.7409 Epoch 786/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 1.7314 Epoch 787/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1024 - val_loss: 1.7218 Epoch 788/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 1.7124 Epoch 789/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1016 - val_loss: 1.7032 Epoch 790/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 1.6925 Epoch 791/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1009 - val_loss: 1.6821 Epoch 792/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1005 - val_loss: 1.6722 Epoch 793/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1001 - val_loss: 1.6626 Epoch 794/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0997 - val_loss: 1.6531 Epoch 795/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0993 - val_loss: 1.6433 Epoch 796/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0989 - val_loss: 1.6334 Epoch 797/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0985 - val_loss: 1.6241 Epoch 798/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0981 - val_loss: 1.6152 Epoch 799/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0978 - val_loss: 1.6056 Epoch 800/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0974 - val_loss: 1.5965 Epoch 801/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 1.5872 Epoch 802/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 1.5778 Epoch 803/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0962 - val_loss: 1.5689 Epoch 804/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0959 - val_loss: 1.5599 Epoch 805/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0955 - val_loss: 1.5510 Epoch 806/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0951 - val_loss: 1.5423 Epoch 807/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0947 - val_loss: 1.5337 Epoch 808/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0944 - val_loss: 1.5249 Epoch 809/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0940 - val_loss: 1.5160 Epoch 810/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0936 - val_loss: 1.5071 Epoch 811/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0933 - val_loss: 1.4979 Epoch 812/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0929 - val_loss: 1.4890 Epoch 813/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0925 - val_loss: 1.4799 Epoch 814/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 1.4709 Epoch 815/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0918 - val_loss: 1.4622 Epoch 816/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0914 - val_loss: 1.4533 Epoch 817/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0910 - val_loss: 1.4441 Epoch 818/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0907 - val_loss: 1.4352 Epoch 819/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0903 - val_loss: 1.4268 Epoch 820/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0899 - val_loss: 1.4182 Epoch 821/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0896 - val_loss: 1.4094 Epoch 822/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0893 - val_loss: 1.4007 Epoch 823/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0889 - val_loss: 1.3927 Epoch 824/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0885 - val_loss: 1.3842 Epoch 825/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0882 - val_loss: 1.3756 Epoch 826/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0879 - val_loss: 1.3669 Epoch 827/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0875 - val_loss: 1.3588 Epoch 828/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0871 - val_loss: 1.3501 Epoch 829/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0868 - val_loss: 1.3413 Epoch 830/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0864 - val_loss: 1.3330 Epoch 831/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 1.3249 Epoch 832/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0858 - val_loss: 1.3168 Epoch 833/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 1.3080 Epoch 834/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0851 - val_loss: 1.2990 Epoch 835/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0847 - val_loss: 1.2905 Epoch 836/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0844 - val_loss: 1.2821 Epoch 837/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0840 - val_loss: 1.2737 Epoch 838/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0837 - val_loss: 1.2656 Epoch 839/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0834 - val_loss: 1.2577 Epoch 840/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0831 - val_loss: 1.2491 Epoch 841/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0827 - val_loss: 1.2417 Epoch 842/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0824 - val_loss: 1.2343 Epoch 843/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0820 - val_loss: 1.2269 Epoch 844/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0817 - val_loss: 1.2187 Epoch 845/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 1.2108 Epoch 846/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0811 - val_loss: 1.2027 Epoch 847/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 1.1946 Epoch 848/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0804 - val_loss: 1.1866 Epoch 849/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0801 - val_loss: 1.1785 Epoch 850/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0797 - val_loss: 1.1710 Epoch 851/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0794 - val_loss: 1.1633 Epoch 852/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0791 - val_loss: 1.1551 Epoch 853/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 1.1476 Epoch 854/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0785 - val_loss: 1.1401 Epoch 855/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 1.1325 Epoch 856/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 1.1250 Epoch 857/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0776 - val_loss: 1.1173 Epoch 858/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0773 - val_loss: 1.1094 Epoch 859/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0769 - val_loss: 1.1020 Epoch 860/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0767 - val_loss: 1.0947 Epoch 861/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 1.0881 Epoch 862/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0760 - val_loss: 1.0809 Epoch 863/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0757 - val_loss: 1.0733 Epoch 864/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0754 - val_loss: 1.0658 Epoch 865/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 1.0587 Epoch 866/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 1.0514 Epoch 867/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0745 - val_loss: 1.0443 Epoch 868/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 1.0366 Epoch 869/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0740 - val_loss: 1.0293 Epoch 870/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0736 - val_loss: 1.0222 Epoch 871/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 1.0156 Epoch 872/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0731 - val_loss: 1.0084 Epoch 873/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0728 - val_loss: 1.0014 Epoch 874/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0725 - val_loss: 0.9942 Epoch 875/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0722 - val_loss: 0.9875 Epoch 876/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.9811 Epoch 877/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.9742 Epoch 878/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.9676 Epoch 879/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.9613 Epoch 880/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0708 - val_loss: 0.9546 Epoch 881/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.9475 Epoch 882/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0702 - val_loss: 0.9403 Epoch 883/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.9331 Epoch 884/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.9261 Epoch 885/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0694 - val_loss: 0.9197 Epoch 886/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0691 - val_loss: 0.9135 Epoch 887/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.9066 Epoch 888/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0686 - val_loss: 0.8993 Epoch 889/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.8924 Epoch 890/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8855 Epoch 891/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0678 - val_loss: 0.8796 Epoch 892/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0675 - val_loss: 0.8733 Epoch 893/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0673 - val_loss: 0.8668 Epoch 894/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0670 - val_loss: 0.8608 Epoch 895/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.8547 Epoch 896/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0665 - val_loss: 0.8485 Epoch 897/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.8425 Epoch 898/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.8362 Epoch 899/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.8303 Epoch 900/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0655 - val_loss: 0.8244 Epoch 901/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.8186 Epoch 902/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.8127 Epoch 903/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0648 - val_loss: 0.8066 Epoch 904/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0646 - val_loss: 0.8011 Epoch 905/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0643 - val_loss: 0.7951 Epoch 906/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0641 - val_loss: 0.7896 Epoch 907/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7837 Epoch 908/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.7782 Epoch 909/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.7722 Epoch 910/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.7668 Epoch 911/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0629 - val_loss: 0.7612 Epoch 912/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.7556 Epoch 913/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.7500 Epoch 914/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0623 - val_loss: 0.7444 Epoch 915/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.0620 - val_loss: 0.7387 Epoch 916/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.7329 Epoch 917/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0616 - val_loss: 0.7272 Epoch 918/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0614 - val_loss: 0.7216 Epoch 919/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0611 - val_loss: 0.7163 Epoch 920/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.7108 Epoch 921/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.7053 Epoch 922/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.6999 Epoch 923/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.6943 Epoch 924/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.6890 Epoch 925/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0598 - val_loss: 0.6838 Epoch 926/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.6785 Epoch 927/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0594 - val_loss: 0.6733 Epoch 928/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.6678 Epoch 929/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6624 Epoch 930/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0587 - val_loss: 0.6573 Epoch 931/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0585 - val_loss: 0.6522 Epoch 932/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.6472 Epoch 933/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0581 - val_loss: 0.6422 Epoch 934/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0579 - val_loss: 0.6372 Epoch 935/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0577 - val_loss: 0.6322 Epoch 936/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0575 - val_loss: 0.6270 Epoch 937/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6220 Epoch 938/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0571 - val_loss: 0.6170 Epoch 939/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.6121 Epoch 940/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0567 - val_loss: 0.6070 Epoch 941/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.6022 Epoch 942/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0563 - val_loss: 0.5973 Epoch 943/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0561 - val_loss: 0.5923 Epoch 944/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0559 - val_loss: 0.5877 Epoch 945/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5830 Epoch 946/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.5779 Epoch 947/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.5730 Epoch 948/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0551 - val_loss: 0.5684 Epoch 949/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0549 - val_loss: 0.5634 Epoch 950/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.5588 Epoch 951/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.5542 Epoch 952/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0544 - val_loss: 0.5495 Epoch 953/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5449 Epoch 954/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5401 Epoch 955/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.5357 Epoch 956/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0536 - val_loss: 0.5312 Epoch 957/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0534 - val_loss: 0.5266 Epoch 958/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0533 - val_loss: 0.5221 Epoch 959/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.5178 Epoch 960/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5137 Epoch 961/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.5098 Epoch 962/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5058 Epoch 963/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5017 Epoch 964/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.4976 Epoch 965/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.4935 Epoch 966/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0519 - val_loss: 0.4896 Epoch 967/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4859 Epoch 968/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.4814 Epoch 969/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4773 Epoch 970/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4733 Epoch 971/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0511 - val_loss: 0.4691 Epoch 972/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4649 Epoch 973/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0508 - val_loss: 0.4611 Epoch 974/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.4570 Epoch 975/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0505 - val_loss: 0.4527 Epoch 976/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0503 - val_loss: 0.4487 Epoch 977/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4446 Epoch 978/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4406 Epoch 979/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.4368 Epoch 980/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0497 - val_loss: 0.4333 Epoch 981/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.4294 Epoch 982/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0494 - val_loss: 0.4259 Epoch 983/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4224 Epoch 984/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.4187 Epoch 985/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0489 - val_loss: 0.4149 Epoch 986/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4111 Epoch 987/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4071 Epoch 988/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.4037 Epoch 989/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0483 - val_loss: 0.4002 Epoch 990/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.3968 Epoch 991/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.3931 Epoch 992/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3897 Epoch 993/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0478 - val_loss: 0.3861 Epoch 994/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3829 Epoch 995/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3797 Epoch 996/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0474 - val_loss: 0.3764 Epoch 997/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3733 Epoch 998/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3703 Epoch 999/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3669 Epoch 1000/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0468 - val_loss: 0.3635 Epoch 1001/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.3602 Epoch 1002/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3569 Epoch 1003/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3540 Epoch 1004/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3506 Epoch 1005/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.3476 Epoch 1006/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3444 Epoch 1007/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3409 Epoch 1008/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0458 - val_loss: 0.3378 Epoch 1009/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3347 Epoch 1010/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.3315 Epoch 1011/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0454 - val_loss: 0.3286 Epoch 1012/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3256 Epoch 1013/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3225 Epoch 1014/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3197 Epoch 1015/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3168 Epoch 1016/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.3138 Epoch 1017/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.3111 Epoch 1018/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3082 Epoch 1019/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0445 - val_loss: 0.3054 Epoch 1020/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.3027 Epoch 1021/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.2998 Epoch 1022/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2970 Epoch 1023/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2940 Epoch 1024/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0439 - val_loss: 0.2913 Epoch 1025/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0438 - val_loss: 0.2886 Epoch 1026/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.2858 Epoch 1027/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.2831 Epoch 1028/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2806 Epoch 1029/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2782 Epoch 1030/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0433 - val_loss: 0.2755 Epoch 1031/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2731 Epoch 1032/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2702 Epoch 1033/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2675 Epoch 1034/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2650 Epoch 1035/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.2623 Epoch 1036/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0427 - val_loss: 0.2599 Epoch 1037/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2575 Epoch 1038/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0425 - val_loss: 0.2553 Epoch 1039/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0424 - val_loss: 0.2532 Epoch 1040/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.2508 Epoch 1041/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2486 Epoch 1042/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2461 Epoch 1043/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2439 Epoch 1044/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2415 Epoch 1045/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2393 Epoch 1046/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2369 Epoch 1047/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0416 - val_loss: 0.2347 Epoch 1048/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2325 Epoch 1049/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2303 Epoch 1050/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2282 Epoch 1051/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2259 Epoch 1052/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2238 Epoch 1053/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2215 Epoch 1054/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2191 Epoch 1055/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2170 Epoch 1056/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2149 Epoch 1057/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2129 Epoch 1058/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2111 Epoch 1059/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2093 Epoch 1060/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2075 Epoch 1061/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2056 Epoch 1062/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2037 Epoch 1063/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0403 - val_loss: 0.2018 Epoch 1064/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1999 Epoch 1065/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1978 Epoch 1066/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1958 Epoch 1067/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1939 Epoch 1068/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1920 Epoch 1069/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1904 Epoch 1070/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1886 Epoch 1071/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1868 Epoch 1072/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0397 - val_loss: 0.1851 Epoch 1073/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1834 Epoch 1074/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1816 Epoch 1075/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1799 Epoch 1076/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1784 Epoch 1077/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1765 Epoch 1078/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0393 - val_loss: 0.1747 Epoch 1079/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1731 Epoch 1080/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1715 Epoch 1081/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1699 Epoch 1082/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1682 Epoch 1083/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1667 Epoch 1084/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0389 - val_loss: 0.1650 Epoch 1085/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1636 Epoch 1086/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1622 Epoch 1087/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1606 Epoch 1088/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1592 Epoch 1089/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1578 Epoch 1090/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1563 Epoch 1091/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1547 Epoch 1092/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1532 Epoch 1093/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1517 Epoch 1094/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1503 Epoch 1095/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1489 Epoch 1096/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1475 Epoch 1097/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0381 - val_loss: 0.1460 Epoch 1098/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1446 Epoch 1099/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1434 Epoch 1100/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1419 Epoch 1101/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1407 Epoch 1102/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1393 Epoch 1103/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1381 Epoch 1104/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1367 Epoch 1105/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.1355 Epoch 1106/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1343 Epoch 1107/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1333 Epoch 1108/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1321 Epoch 1109/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1308 Epoch 1110/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1295 Epoch 1111/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1283 Epoch 1112/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1270 Epoch 1113/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1257 Epoch 1114/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1246 Epoch 1115/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1234 Epoch 1116/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1223 Epoch 1117/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1211 Epoch 1118/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1201 Epoch 1119/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1190 Epoch 1120/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1178 Epoch 1121/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1166 Epoch 1122/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1154 Epoch 1123/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1143 Epoch 1124/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1133 Epoch 1125/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1123 Epoch 1126/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1112 Epoch 1127/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1102 Epoch 1128/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1094 Epoch 1129/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1086 Epoch 1130/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1076 Epoch 1131/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0366 - val_loss: 0.1066 Epoch 1132/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1057 Epoch 1133/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1049 Epoch 1134/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1040 Epoch 1135/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1030 Epoch 1136/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1021 Epoch 1137/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1012 Epoch 1138/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1003 Epoch 1139/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0364 - val_loss: 0.0995 Epoch 1140/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0988 Epoch 1141/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0363 - val_loss: 0.0980 Epoch 1142/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0972 Epoch 1143/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0965 Epoch 1144/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0956 Epoch 1145/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0947 Epoch 1146/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0939 Epoch 1147/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0930 Epoch 1148/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0923 Epoch 1149/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0916 Epoch 1150/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0910 Epoch 1151/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0903 Epoch 1152/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0895 Epoch 1153/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0888 Epoch 1154/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0880 Epoch 1155/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0874 Epoch 1156/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0867 Epoch 1157/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0858 Epoch 1158/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0851 Epoch 1159/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0844 Epoch 1160/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0837 Epoch 1161/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0830 Epoch 1162/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0824 Epoch 1163/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0818 Epoch 1164/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0812 Epoch 1165/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0808 Epoch 1166/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0803 Epoch 1167/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0797 Epoch 1168/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0792 Epoch 1169/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0786 Epoch 1170/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0780 Epoch 1171/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0775 Epoch 1172/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0768 Epoch 1173/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0762 Epoch 1174/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0756 Epoch 1175/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0750 Epoch 1176/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0745 Epoch 1177/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0740 Epoch 1178/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0734 Epoch 1179/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0729 Epoch 1180/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0724 Epoch 1181/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0719 Epoch 1182/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0714 Epoch 1183/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0709 Epoch 1184/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0704 Epoch 1185/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0698 Epoch 1186/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0694 Epoch 1187/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0689 Epoch 1188/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0684 Epoch 1189/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0679 Epoch 1190/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0674 Epoch 1191/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0669 Epoch 1192/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0666 Epoch 1193/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0661 Epoch 1194/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0657 Epoch 1195/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0652 Epoch 1196/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0648 Epoch 1197/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0644 Epoch 1198/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0640 Epoch 1199/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0636 Epoch 1200/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0632 Epoch 1201/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0627 Epoch 1202/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0623 Epoch 1203/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0621 Epoch 1204/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0617 Epoch 1205/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0614 Epoch 1206/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0611 Epoch 1207/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0607 Epoch 1208/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0604 Epoch 1209/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0601 Epoch 1210/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0597 Epoch 1211/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0594 Epoch 1212/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0590 Epoch 1213/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0587 Epoch 1214/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0583 Epoch 1215/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0581 Epoch 1216/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0348 - val_loss: 0.0578 Epoch 1217/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0575 Epoch 1218/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0571 Epoch 1219/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0568 Epoch 1220/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0565 Epoch 1221/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0562 Epoch 1222/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0559 Epoch 1223/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0556 Epoch 1224/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0553 Epoch 1225/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0551 Epoch 1226/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0548 Epoch 1227/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0545 Epoch 1228/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0543 Epoch 1229/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0540 Epoch 1230/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0537 Epoch 1231/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0535 Epoch 1232/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0532 Epoch 1233/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0530 Epoch 1234/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0527 Epoch 1235/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0525 Epoch 1236/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0523 Epoch 1237/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0521 Epoch 1238/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0518 Epoch 1239/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0516 Epoch 1240/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0514 Epoch 1241/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0513 Epoch 1242/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0511 Epoch 1243/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0509 Epoch 1244/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0507 Epoch 1245/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0505 Epoch 1246/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0503 Epoch 1247/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0500 Epoch 1248/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0499 Epoch 1249/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0497 Epoch 1250/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0495 Epoch 1251/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0493 Epoch 1252/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0491 Epoch 1253/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0489 Epoch 1254/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0488 Epoch 1255/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0486 Epoch 1256/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0484 Epoch 1257/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0482 Epoch 1258/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0480 Epoch 1259/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0479 Epoch 1260/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0477 Epoch 1261/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0475 Epoch 1262/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473 Epoch 1263/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473 Epoch 1264/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0472 Epoch 1265/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0470 Epoch 1266/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0468 Epoch 1267/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0467 Epoch 1268/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0465 Epoch 1269/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463 Epoch 1270/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462 Epoch 1271/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0460 Epoch 1272/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458 Epoch 1273/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0457 Epoch 1274/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456 Epoch 1275/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454 Epoch 1276/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0453 Epoch 1277/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452 Epoch 1278/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0451 Epoch 1279/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0450 Epoch 1280/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0449 Epoch 1281/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448 Epoch 1282/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0447 Epoch 1283/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446 Epoch 1284/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0445 Epoch 1285/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444 Epoch 1286/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443 Epoch 1287/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442 Epoch 1288/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441 Epoch 1289/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440 Epoch 1290/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439 Epoch 1291/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438 Epoch 1292/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0437 Epoch 1293/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0437 Epoch 1294/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0436 Epoch 1295/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435 Epoch 1296/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434 Epoch 1297/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434 Epoch 1298/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433 Epoch 1299/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.0343 - val_loss: 0.0432 Epoch 1300/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432 Epoch 1301/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430 Epoch 1302/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430 Epoch 1303/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0429 Epoch 1304/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0428 Epoch 1305/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0427 Epoch 1306/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426 Epoch 1307/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426 Epoch 1308/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425 Epoch 1309/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424 Epoch 1310/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423 Epoch 1311/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423 Epoch 1312/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422 Epoch 1313/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421 Epoch 1314/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421 Epoch 1315/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420 Epoch 1316/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420 Epoch 1317/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419 Epoch 1318/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419 Epoch 1319/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418 Epoch 1320/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417 Epoch 1321/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417 Epoch 1322/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0416 Epoch 1323/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415 Epoch 1324/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415 Epoch 1325/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414 Epoch 1326/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414 Epoch 1327/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413 Epoch 1328/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413 Epoch 1329/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412 Epoch 1330/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412 Epoch 1331/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412 Epoch 1332/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411 Epoch 1333/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411 Epoch 1334/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411 Epoch 1335/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410 Epoch 1336/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410 Epoch 1337/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410 Epoch 1338/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409 Epoch 1339/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408 Epoch 1340/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0408 Epoch 1341/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407 Epoch 1342/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407 Epoch 1343/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406 Epoch 1344/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406 Epoch 1345/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405 Epoch 1346/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405 Epoch 1347/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405 Epoch 1348/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405 Epoch 1349/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404 Epoch 1350/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404 Epoch 1351/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404 Epoch 1352/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1353/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1354/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1355/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1356/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1357/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1358/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1359/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1360/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400 Epoch 1361/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400 Epoch 1362/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400 Epoch 1363/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0400 Epoch 1364/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1365/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1366/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1367/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1368/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1369/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1370/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1371/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1372/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1373/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1374/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1375/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1376/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1377/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1378/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1379/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1380/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1381/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1382/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1383/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1384/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1385/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1386/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1387/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1388/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1389/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1390/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1391/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1392/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1393/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1394/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1395/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1396/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1397/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1398/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1399/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1400/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1401/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1402/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1403/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1404/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1405/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1406/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1407/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1408/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1409/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1410/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1411/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1412/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1413/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1414/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1415/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1416/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1417/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1418/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1419/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1420/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1421/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1422/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1423/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1424/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1425/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1426/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1427/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1428/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1429/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1430/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1431/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1432/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1433/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1434/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1435/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1436/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1437/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1438/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1439/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1440/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1441/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1442/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1443/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1444/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1445/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1446/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1447/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1448/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1449/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1450/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1451/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1452/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1453/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1454/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1455/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1456/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1457/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1458/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1459/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1460/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1461/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1462/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1463/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1464/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1465/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1466/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1467/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1468/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1469/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1470/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1471/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1472/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1473/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1474/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1475/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1476/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1477/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1478/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1479/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1480/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1481/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1482/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1483/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1484/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1485/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1486/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1487/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1488/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1489/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1490/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1491/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1492/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1493/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1494/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1495/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1496/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1497/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1498/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1499/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1500/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1501/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1502/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1503/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1504/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1505/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1506/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1507/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1508/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1509/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1510/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1511/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1512/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1513/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1514/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1515/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1516/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1517/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1518/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1519/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1520/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1521/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1522/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1523/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1524/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1525/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1526/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1527/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1528/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1529/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1530/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1531/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1532/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1533/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1534/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1535/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1536/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1537/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1538/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1539/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1540/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1541/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1542/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1543/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1544/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1545/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1546/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1547/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1548/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1549/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1550/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1551/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1552/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1553/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1554/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1555/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1556/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1557/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1558/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1559/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1560/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1561/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1562/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1563/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1564/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1565/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1566/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1567/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1568/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1569/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1570/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1571/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1572/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1573/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1574/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1575/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1576/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1577/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1578/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1579/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1580/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1581/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1582/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1583/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1584/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1585/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1586/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1587/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1588/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1589/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1590/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1591/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1592/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1593/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1594/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1595/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1596/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1597/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1598/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1599/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1600/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1601/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1602/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1603/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1604/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1605/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1606/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1607/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1608/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1609/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1610/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1611/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1612/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1613/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1614/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1615/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1616/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1617/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1618/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1619/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1620/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1621/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1622/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1623/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1624/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1625/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1626/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1627/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1628/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1629/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1630/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1631/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1632/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1633/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1634/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1635/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1636/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1637/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1638/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1639/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1640/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1641/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1642/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1643/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1644/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1645/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1646/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1647/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1648/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1649/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1650/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1651/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1652/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1653/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1654/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1655/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1656/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1657/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1658/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1659/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1660/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1661/2000 6/6 [==============================] - ETA: 0s - loss: 0.034 - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1662/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1663/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1664/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1665/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1666/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1667/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1668/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1669/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1670/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1671/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1672/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1673/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1674/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1675/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1676/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1677/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1678/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1679/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1680/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1681/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1682/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1683/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1684/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1685/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1686/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1687/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1688/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1689/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1690/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1691/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1692/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1693/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1694/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1695/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1696/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1697/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1698/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1699/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1700/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1701/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1702/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1703/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1704/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1705/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1706/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1707/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1708/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1709/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1710/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1711/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1712/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1713/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1714/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1715/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1716/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1717/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1718/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1719/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1720/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1721/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1722/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1723/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1724/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1725/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1726/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1727/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1728/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1729/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1730/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1731/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1732/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1733/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1734/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1735/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1736/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1737/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1738/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1739/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1740/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1741/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1742/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1743/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1744/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1745/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1746/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1747/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1748/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1749/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1750/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1751/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1752/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1753/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1754/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1755/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1756/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1757/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1758/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1759/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1760/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1761/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1762/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1763/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1764/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1765/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1766/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1767/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1768/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1769/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1770/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1771/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1772/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1773/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1774/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1775/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1776/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1777/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1778/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1779/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1780/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1781/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1782/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1783/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1784/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1785/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1786/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1787/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1788/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1789/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1790/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1791/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1792/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1793/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1794/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1795/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1796/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1797/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1798/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1799/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1800/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1801/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1802/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1803/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1804/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1805/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1806/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1807/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1808/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1809/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1810/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1811/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1812/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1813/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1814/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1815/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1816/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1817/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1818/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1819/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1820/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1821/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1822/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1823/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1824/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1825/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1826/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1827/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1828/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1829/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1830/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1831/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1832/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1833/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1834/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1835/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1836/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1837/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1838/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1839/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1840/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1841/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1842/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1843/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1844/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1845/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1846/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1847/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1848/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1849/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1850/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1851/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1852/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1853/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1854/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1855/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1856/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1857/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1858/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1859/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1860/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1861/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1862/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1863/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1864/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1865/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1866/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1867/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1868/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1869/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1870/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1871/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1872/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1873/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1874/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1875/2000 6/6 [==============================] - 0s 5ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1876/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1877/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1878/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1879/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1880/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1881/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1882/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1883/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1884/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1885/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1886/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1887/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1888/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1889/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1890/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1891/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1892/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1893/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1894/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1895/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1896/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1897/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1898/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1899/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1900/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1901/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1902/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1903/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1904/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1905/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1906/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1907/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1908/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1909/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1910/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1911/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1912/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1913/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1914/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1915/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1916/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1917/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1918/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1919/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1920/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1921/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1922/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1923/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1924/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1925/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1926/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1927/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1928/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1929/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1930/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1931/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1932/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1933/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1934/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1935/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1936/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1937/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1938/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1939/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1940/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1941/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1942/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1943/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1944/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1945/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1946/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1947/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1948/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1949/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1950/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1951/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1952/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1953/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1954/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1955/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1956/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1957/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1958/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1959/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1960/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1961/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1962/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1963/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1964/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1965/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1966/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1967/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1968/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1969/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1970/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1971/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1972/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1973/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1974/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1975/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1976/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1977/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1978/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1979/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1980/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1981/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1982/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1983/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1984/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1985/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1986/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1987/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1988/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1989/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1990/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1991/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1992/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1993/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1994/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1995/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1996/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1997/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1998/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1999/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 2000/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 . &lt;keras.callbacks.History at 0x7f708bd97e50&gt; . . plt.plot(y,&#39;.&#39;,alpha=0.1) plt.plot(net(X),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f708bfe5030&gt;] . #%tensorboard --logdir logs --host 0.0.0.0 . 이런것은 오버핏이 아님! | . - 결론적으로 말해서 위와 같은 net는 설계하였을 경우 val을 빼는 것은 어리석음. (데이터만 버리는 꼴임) . - 더 많은 데이터를 남겨주면 더 빨리 학습한다. . !rm -rf logs net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) net.fit(X,y,epochs=500,batch_size=100, validation_split=0.1, callbacks=tf.keras.callbacks.TensorBoard()) . Epoch 1/500 9/9 [==============================] - 0s 5ms/step - loss: 11.1529 - val_loss: 17.6322 Epoch 2/500 9/9 [==============================] - 0s 2ms/step - loss: 11.0510 - val_loss: 17.4478 Epoch 3/500 9/9 [==============================] - 0s 2ms/step - loss: 10.9482 - val_loss: 17.2670 Epoch 4/500 9/9 [==============================] - 0s 2ms/step - loss: 10.8465 - val_loss: 17.0850 Epoch 5/500 9/9 [==============================] - 0s 2ms/step - loss: 10.7443 - val_loss: 16.9074 Epoch 6/500 9/9 [==============================] - 0s 2ms/step - loss: 10.6457 - val_loss: 16.7250 Epoch 7/500 9/9 [==============================] - 0s 1ms/step - loss: 10.5456 - val_loss: 16.5480 Epoch 8/500 9/9 [==============================] - 0s 2ms/step - loss: 10.4474 - val_loss: 16.3721 Epoch 9/500 9/9 [==============================] - 0s 2ms/step - loss: 10.3499 - val_loss: 16.1945 Epoch 10/500 9/9 [==============================] - 0s 2ms/step - loss: 10.2516 - val_loss: 16.0212 Epoch 11/500 9/9 [==============================] - 0s 2ms/step - loss: 10.1587 - val_loss: 15.8501 Epoch 12/500 9/9 [==============================] - 0s 2ms/step - loss: 10.0600 - val_loss: 15.6844 Epoch 13/500 9/9 [==============================] - 0s 2ms/step - loss: 9.9677 - val_loss: 15.5179 Epoch 14/500 9/9 [==============================] - 0s 2ms/step - loss: 9.8747 - val_loss: 15.3479 Epoch 15/500 9/9 [==============================] - 0s 2ms/step - loss: 9.7806 - val_loss: 15.1842 Epoch 16/500 9/9 [==============================] - 0s 2ms/step - loss: 9.6911 - val_loss: 15.0164 Epoch 17/500 9/9 [==============================] - 0s 2ms/step - loss: 9.5969 - val_loss: 14.8620 Epoch 18/500 9/9 [==============================] - 0s 2ms/step - loss: 9.5088 - val_loss: 14.6981 Epoch 19/500 9/9 [==============================] - 0s 2ms/step - loss: 9.4182 - val_loss: 14.5400 Epoch 20/500 9/9 [==============================] - 0s 1ms/step - loss: 9.3294 - val_loss: 14.3857 Epoch 21/500 9/9 [==============================] - 0s 2ms/step - loss: 9.2421 - val_loss: 14.2279 Epoch 22/500 9/9 [==============================] - 0s 1ms/step - loss: 9.1551 - val_loss: 14.0740 Epoch 23/500 9/9 [==============================] - 0s 2ms/step - loss: 9.0690 - val_loss: 13.9182 Epoch 24/500 9/9 [==============================] - 0s 1ms/step - loss: 8.9823 - val_loss: 13.7646 Epoch 25/500 9/9 [==============================] - 0s 2ms/step - loss: 8.8961 - val_loss: 13.6238 Epoch 26/500 9/9 [==============================] - 0s 1ms/step - loss: 8.8128 - val_loss: 13.4795 Epoch 27/500 9/9 [==============================] - 0s 2ms/step - loss: 8.7307 - val_loss: 13.3281 Epoch 28/500 9/9 [==============================] - 0s 2ms/step - loss: 8.6474 - val_loss: 13.1848 Epoch 29/500 9/9 [==============================] - 0s 2ms/step - loss: 8.5655 - val_loss: 13.0439 Epoch 30/500 9/9 [==============================] - 0s 2ms/step - loss: 8.4836 - val_loss: 12.9032 Epoch 31/500 9/9 [==============================] - 0s 1ms/step - loss: 8.4041 - val_loss: 12.7584 Epoch 32/500 9/9 [==============================] - 0s 2ms/step - loss: 8.3237 - val_loss: 12.6194 Epoch 33/500 9/9 [==============================] - 0s 2ms/step - loss: 8.2445 - val_loss: 12.4818 Epoch 34/500 9/9 [==============================] - 0s 2ms/step - loss: 8.1652 - val_loss: 12.3472 Epoch 35/500 9/9 [==============================] - 0s 2ms/step - loss: 8.0891 - val_loss: 12.2060 Epoch 36/500 9/9 [==============================] - 0s 2ms/step - loss: 8.0112 - val_loss: 12.0756 Epoch 37/500 9/9 [==============================] - 0s 2ms/step - loss: 7.9331 - val_loss: 11.9493 Epoch 38/500 9/9 [==============================] - 0s 1ms/step - loss: 7.8597 - val_loss: 11.8165 Epoch 39/500 9/9 [==============================] - 0s 2ms/step - loss: 7.7832 - val_loss: 11.6866 Epoch 40/500 9/9 [==============================] - 0s 2ms/step - loss: 7.7085 - val_loss: 11.5609 Epoch 41/500 9/9 [==============================] - 0s 1ms/step - loss: 7.6347 - val_loss: 11.4300 Epoch 42/500 9/9 [==============================] - 0s 2ms/step - loss: 7.5625 - val_loss: 11.3039 Epoch 43/500 9/9 [==============================] - 0s 2ms/step - loss: 7.4886 - val_loss: 11.1786 Epoch 44/500 9/9 [==============================] - 0s 2ms/step - loss: 7.4170 - val_loss: 11.0613 Epoch 45/500 9/9 [==============================] - 0s 2ms/step - loss: 7.3459 - val_loss: 10.9341 Epoch 46/500 9/9 [==============================] - 0s 2ms/step - loss: 7.2742 - val_loss: 10.8175 Epoch 47/500 9/9 [==============================] - 0s 2ms/step - loss: 7.2045 - val_loss: 10.6986 Epoch 48/500 9/9 [==============================] - 0s 2ms/step - loss: 7.1351 - val_loss: 10.5782 Epoch 49/500 9/9 [==============================] - 0s 2ms/step - loss: 7.0656 - val_loss: 10.4625 Epoch 50/500 9/9 [==============================] - 0s 2ms/step - loss: 6.9977 - val_loss: 10.3436 Epoch 51/500 9/9 [==============================] - 0s 2ms/step - loss: 6.9311 - val_loss: 10.2223 Epoch 52/500 9/9 [==============================] - 0s 2ms/step - loss: 6.8624 - val_loss: 10.1109 Epoch 53/500 9/9 [==============================] - 0s 2ms/step - loss: 6.7964 - val_loss: 9.9984 Epoch 54/500 9/9 [==============================] - 0s 2ms/step - loss: 6.7296 - val_loss: 9.8888 Epoch 55/500 9/9 [==============================] - 0s 2ms/step - loss: 6.6645 - val_loss: 9.7811 Epoch 56/500 9/9 [==============================] - 0s 2ms/step - loss: 6.5996 - val_loss: 9.6738 Epoch 57/500 9/9 [==============================] - 0s 1ms/step - loss: 6.5362 - val_loss: 9.5644 Epoch 58/500 9/9 [==============================] - 0s 2ms/step - loss: 6.4717 - val_loss: 9.4555 Epoch 59/500 9/9 [==============================] - 0s 2ms/step - loss: 6.4094 - val_loss: 9.3465 Epoch 60/500 9/9 [==============================] - 0s 2ms/step - loss: 6.3461 - val_loss: 9.2445 Epoch 61/500 9/9 [==============================] - 0s 2ms/step - loss: 6.2842 - val_loss: 9.1374 Epoch 62/500 9/9 [==============================] - 0s 2ms/step - loss: 6.2229 - val_loss: 9.0369 Epoch 63/500 9/9 [==============================] - 0s 2ms/step - loss: 6.1613 - val_loss: 8.9344 Epoch 64/500 9/9 [==============================] - 0s 2ms/step - loss: 6.1003 - val_loss: 8.8330 Epoch 65/500 9/9 [==============================] - 0s 2ms/step - loss: 6.0411 - val_loss: 8.7345 Epoch 66/500 9/9 [==============================] - 0s 2ms/step - loss: 5.9818 - val_loss: 8.6328 Epoch 67/500 9/9 [==============================] - 0s 2ms/step - loss: 5.9227 - val_loss: 8.5349 Epoch 68/500 9/9 [==============================] - 0s 2ms/step - loss: 5.8640 - val_loss: 8.4380 Epoch 69/500 9/9 [==============================] - 0s 2ms/step - loss: 5.8069 - val_loss: 8.3396 Epoch 70/500 9/9 [==============================] - 0s 2ms/step - loss: 5.7490 - val_loss: 8.2460 Epoch 71/500 9/9 [==============================] - 0s 1ms/step - loss: 5.6922 - val_loss: 8.1507 Epoch 72/500 9/9 [==============================] - 0s 1ms/step - loss: 5.6366 - val_loss: 8.0578 Epoch 73/500 9/9 [==============================] - 0s 2ms/step - loss: 5.5798 - val_loss: 7.9645 Epoch 74/500 9/9 [==============================] - 0s 1ms/step - loss: 5.5245 - val_loss: 7.8750 Epoch 75/500 9/9 [==============================] - 0s 2ms/step - loss: 5.4699 - val_loss: 7.7816 Epoch 76/500 9/9 [==============================] - 0s 1ms/step - loss: 5.4155 - val_loss: 7.6949 Epoch 77/500 9/9 [==============================] - 0s 2ms/step - loss: 5.3615 - val_loss: 7.6049 Epoch 78/500 9/9 [==============================] - 0s 2ms/step - loss: 5.3088 - val_loss: 7.5146 Epoch 79/500 9/9 [==============================] - 0s 2ms/step - loss: 5.2546 - val_loss: 7.4298 Epoch 80/500 9/9 [==============================] - 0s 2ms/step - loss: 5.2024 - val_loss: 7.3431 Epoch 81/500 9/9 [==============================] - 0s 2ms/step - loss: 5.1507 - val_loss: 7.2562 Epoch 82/500 9/9 [==============================] - 0s 2ms/step - loss: 5.0991 - val_loss: 7.1726 Epoch 83/500 9/9 [==============================] - 0s 2ms/step - loss: 5.0482 - val_loss: 7.0886 Epoch 84/500 9/9 [==============================] - 0s 2ms/step - loss: 4.9975 - val_loss: 7.0058 Epoch 85/500 9/9 [==============================] - 0s 1ms/step - loss: 4.9472 - val_loss: 6.9263 Epoch 86/500 9/9 [==============================] - 0s 2ms/step - loss: 4.8969 - val_loss: 6.8459 Epoch 87/500 9/9 [==============================] - 0s 1ms/step - loss: 4.8479 - val_loss: 6.7668 Epoch 88/500 9/9 [==============================] - 0s 2ms/step - loss: 4.7994 - val_loss: 6.6891 Epoch 89/500 9/9 [==============================] - 0s 2ms/step - loss: 4.7512 - val_loss: 6.6063 Epoch 90/500 9/9 [==============================] - 0s 1ms/step - loss: 4.7022 - val_loss: 6.5310 Epoch 91/500 9/9 [==============================] - 0s 2ms/step - loss: 4.6547 - val_loss: 6.4561 Epoch 92/500 9/9 [==============================] - 0s 2ms/step - loss: 4.6079 - val_loss: 6.3786 Epoch 93/500 9/9 [==============================] - 0s 2ms/step - loss: 4.5607 - val_loss: 6.3046 Epoch 94/500 9/9 [==============================] - 0s 2ms/step - loss: 4.5144 - val_loss: 6.2286 Epoch 95/500 9/9 [==============================] - 0s 2ms/step - loss: 4.4690 - val_loss: 6.1530 Epoch 96/500 9/9 [==============================] - 0s 2ms/step - loss: 4.4237 - val_loss: 6.0801 Epoch 97/500 9/9 [==============================] - 0s 1ms/step - loss: 4.3780 - val_loss: 6.0086 Epoch 98/500 9/9 [==============================] - 0s 1ms/step - loss: 4.3333 - val_loss: 5.9393 Epoch 99/500 9/9 [==============================] - 0s 2ms/step - loss: 4.2886 - val_loss: 5.8690 Epoch 100/500 9/9 [==============================] - 0s 2ms/step - loss: 4.2452 - val_loss: 5.7963 Epoch 101/500 9/9 [==============================] - 0s 1ms/step - loss: 4.2013 - val_loss: 5.7301 Epoch 102/500 9/9 [==============================] - 0s 1ms/step - loss: 4.1583 - val_loss: 5.6612 Epoch 103/500 9/9 [==============================] - 0s 2ms/step - loss: 4.1153 - val_loss: 5.5936 Epoch 104/500 9/9 [==============================] - 0s 2ms/step - loss: 4.0736 - val_loss: 5.5238 Epoch 105/500 9/9 [==============================] - 0s 2ms/step - loss: 4.0305 - val_loss: 5.4606 Epoch 106/500 9/9 [==============================] - 0s 1ms/step - loss: 3.9894 - val_loss: 5.3935 Epoch 107/500 9/9 [==============================] - 0s 1ms/step - loss: 3.9481 - val_loss: 5.3268 Epoch 108/500 9/9 [==============================] - 0s 1ms/step - loss: 3.9076 - val_loss: 5.2610 Epoch 109/500 9/9 [==============================] - 0s 2ms/step - loss: 3.8664 - val_loss: 5.1996 Epoch 110/500 9/9 [==============================] - 0s 1ms/step - loss: 3.8257 - val_loss: 5.1385 Epoch 111/500 9/9 [==============================] - 0s 2ms/step - loss: 3.7870 - val_loss: 5.0747 Epoch 112/500 9/9 [==============================] - 0s 2ms/step - loss: 3.7474 - val_loss: 5.0111 Epoch 113/500 9/9 [==============================] - 0s 1ms/step - loss: 3.7078 - val_loss: 4.9471 Epoch 114/500 9/9 [==============================] - 0s 2ms/step - loss: 3.6688 - val_loss: 4.8894 Epoch 115/500 9/9 [==============================] - 0s 2ms/step - loss: 3.6306 - val_loss: 4.8294 Epoch 116/500 9/9 [==============================] - 0s 2ms/step - loss: 3.5922 - val_loss: 4.7704 Epoch 117/500 9/9 [==============================] - 0s 1ms/step - loss: 3.5549 - val_loss: 4.7096 Epoch 118/500 9/9 [==============================] - 0s 2ms/step - loss: 3.5174 - val_loss: 4.6514 Epoch 119/500 9/9 [==============================] - 0s 1ms/step - loss: 3.4798 - val_loss: 4.5948 Epoch 120/500 9/9 [==============================] - 0s 2ms/step - loss: 3.4434 - val_loss: 4.5386 Epoch 121/500 9/9 [==============================] - 0s 1ms/step - loss: 3.4072 - val_loss: 4.4816 Epoch 122/500 9/9 [==============================] - 0s 2ms/step - loss: 3.3708 - val_loss: 4.4268 Epoch 123/500 9/9 [==============================] - 0s 2ms/step - loss: 3.3349 - val_loss: 4.3720 Epoch 124/500 9/9 [==============================] - 0s 2ms/step - loss: 3.2994 - val_loss: 4.3174 Epoch 125/500 9/9 [==============================] - 0s 2ms/step - loss: 3.2643 - val_loss: 4.2653 Epoch 126/500 9/9 [==============================] - 0s 1ms/step - loss: 3.2300 - val_loss: 4.2094 Epoch 127/500 9/9 [==============================] - 0s 2ms/step - loss: 3.1955 - val_loss: 4.1579 Epoch 128/500 9/9 [==============================] - 0s 2ms/step - loss: 3.1605 - val_loss: 4.1050 Epoch 129/500 9/9 [==============================] - 0s 1ms/step - loss: 3.1269 - val_loss: 4.0541 Epoch 130/500 9/9 [==============================] - 0s 2ms/step - loss: 3.0935 - val_loss: 4.0013 Epoch 131/500 9/9 [==============================] - 0s 2ms/step - loss: 3.0599 - val_loss: 3.9509 Epoch 132/500 9/9 [==============================] - 0s 2ms/step - loss: 3.0270 - val_loss: 3.9000 Epoch 133/500 9/9 [==============================] - 0s 2ms/step - loss: 2.9942 - val_loss: 3.8525 Epoch 134/500 9/9 [==============================] - 0s 2ms/step - loss: 2.9619 - val_loss: 3.8035 Epoch 135/500 9/9 [==============================] - 0s 1ms/step - loss: 2.9301 - val_loss: 3.7534 Epoch 136/500 9/9 [==============================] - 0s 1ms/step - loss: 2.8981 - val_loss: 3.7043 Epoch 137/500 9/9 [==============================] - 0s 2ms/step - loss: 2.8662 - val_loss: 3.6581 Epoch 138/500 9/9 [==============================] - 0s 1ms/step - loss: 2.8350 - val_loss: 3.6088 Epoch 139/500 9/9 [==============================] - 0s 2ms/step - loss: 2.8040 - val_loss: 3.5631 Epoch 140/500 9/9 [==============================] - 0s 2ms/step - loss: 2.7735 - val_loss: 3.5183 Epoch 141/500 9/9 [==============================] - 0s 2ms/step - loss: 2.7429 - val_loss: 3.4733 Epoch 142/500 9/9 [==============================] - 0s 1ms/step - loss: 2.7129 - val_loss: 3.4262 Epoch 143/500 9/9 [==============================] - 0s 1ms/step - loss: 2.6828 - val_loss: 3.3835 Epoch 144/500 9/9 [==============================] - 0s 1ms/step - loss: 2.6533 - val_loss: 3.3392 Epoch 145/500 9/9 [==============================] - 0s 2ms/step - loss: 2.6238 - val_loss: 3.2961 Epoch 146/500 9/9 [==============================] - 0s 2ms/step - loss: 2.5950 - val_loss: 3.2534 Epoch 147/500 9/9 [==============================] - 0s 1ms/step - loss: 2.5661 - val_loss: 3.2100 Epoch 148/500 9/9 [==============================] - 0s 1ms/step - loss: 2.5375 - val_loss: 3.1676 Epoch 149/500 9/9 [==============================] - 0s 2ms/step - loss: 2.5092 - val_loss: 3.1254 Epoch 150/500 9/9 [==============================] - 0s 1ms/step - loss: 2.4813 - val_loss: 3.0836 Epoch 151/500 9/9 [==============================] - 0s 2ms/step - loss: 2.4537 - val_loss: 3.0419 Epoch 152/500 9/9 [==============================] - 0s 2ms/step - loss: 2.4258 - val_loss: 3.0024 Epoch 153/500 9/9 [==============================] - 0s 2ms/step - loss: 2.3987 - val_loss: 2.9623 Epoch 154/500 9/9 [==============================] - 0s 2ms/step - loss: 2.3716 - val_loss: 2.9227 Epoch 155/500 9/9 [==============================] - 0s 2ms/step - loss: 2.3446 - val_loss: 2.8838 Epoch 156/500 9/9 [==============================] - 0s 1ms/step - loss: 2.3184 - val_loss: 2.8463 Epoch 157/500 9/9 [==============================] - 0s 2ms/step - loss: 2.2919 - val_loss: 2.8087 Epoch 158/500 9/9 [==============================] - 0s 2ms/step - loss: 2.2661 - val_loss: 2.7687 Epoch 159/500 9/9 [==============================] - 0s 2ms/step - loss: 2.2403 - val_loss: 2.7325 Epoch 160/500 9/9 [==============================] - 0s 1ms/step - loss: 2.2147 - val_loss: 2.6946 Epoch 161/500 9/9 [==============================] - 0s 2ms/step - loss: 2.1892 - val_loss: 2.6583 Epoch 162/500 9/9 [==============================] - 0s 2ms/step - loss: 2.1645 - val_loss: 2.6212 Epoch 163/500 9/9 [==============================] - 0s 2ms/step - loss: 2.1394 - val_loss: 2.5858 Epoch 164/500 9/9 [==============================] - 0s 2ms/step - loss: 2.1146 - val_loss: 2.5519 Epoch 165/500 9/9 [==============================] - 0s 1ms/step - loss: 2.0904 - val_loss: 2.5177 Epoch 166/500 9/9 [==============================] - 0s 1ms/step - loss: 2.0662 - val_loss: 2.4819 Epoch 167/500 9/9 [==============================] - 0s 2ms/step - loss: 2.0422 - val_loss: 2.4472 Epoch 168/500 9/9 [==============================] - 0s 2ms/step - loss: 2.0186 - val_loss: 2.4146 Epoch 169/500 9/9 [==============================] - 0s 2ms/step - loss: 1.9950 - val_loss: 2.3812 Epoch 170/500 9/9 [==============================] - 0s 2ms/step - loss: 1.9717 - val_loss: 2.3481 Epoch 171/500 9/9 [==============================] - 0s 2ms/step - loss: 1.9486 - val_loss: 2.3145 Epoch 172/500 9/9 [==============================] - 0s 2ms/step - loss: 1.9259 - val_loss: 2.2815 Epoch 173/500 9/9 [==============================] - 0s 1ms/step - loss: 1.9030 - val_loss: 2.2488 Epoch 174/500 9/9 [==============================] - 0s 2ms/step - loss: 1.8807 - val_loss: 2.2179 Epoch 175/500 9/9 [==============================] - 0s 1ms/step - loss: 1.8587 - val_loss: 2.1859 Epoch 176/500 9/9 [==============================] - 0s 2ms/step - loss: 1.8365 - val_loss: 2.1555 Epoch 177/500 9/9 [==============================] - 0s 1ms/step - loss: 1.8148 - val_loss: 2.1247 Epoch 178/500 9/9 [==============================] - 0s 2ms/step - loss: 1.7933 - val_loss: 2.0933 Epoch 179/500 9/9 [==============================] - 0s 2ms/step - loss: 1.7719 - val_loss: 2.0639 Epoch 180/500 9/9 [==============================] - 0s 2ms/step - loss: 1.7508 - val_loss: 2.0344 Epoch 181/500 9/9 [==============================] - 0s 2ms/step - loss: 1.7297 - val_loss: 2.0057 Epoch 182/500 9/9 [==============================] - 0s 2ms/step - loss: 1.7092 - val_loss: 1.9759 Epoch 183/500 9/9 [==============================] - 0s 2ms/step - loss: 1.6885 - val_loss: 1.9478 Epoch 184/500 9/9 [==============================] - 0s 2ms/step - loss: 1.6680 - val_loss: 1.9216 Epoch 185/500 9/9 [==============================] - 0s 2ms/step - loss: 1.6480 - val_loss: 1.8931 Epoch 186/500 9/9 [==============================] - 0s 1ms/step - loss: 1.6280 - val_loss: 1.8653 Epoch 187/500 9/9 [==============================] - 0s 1ms/step - loss: 1.6082 - val_loss: 1.8372 Epoch 188/500 9/9 [==============================] - 0s 2ms/step - loss: 1.5887 - val_loss: 1.8108 Epoch 189/500 9/9 [==============================] - 0s 1ms/step - loss: 1.5693 - val_loss: 1.7840 Epoch 190/500 9/9 [==============================] - 0s 2ms/step - loss: 1.5501 - val_loss: 1.7577 Epoch 191/500 9/9 [==============================] - 0s 1ms/step - loss: 1.5310 - val_loss: 1.7306 Epoch 192/500 9/9 [==============================] - 0s 2ms/step - loss: 1.5123 - val_loss: 1.7052 Epoch 193/500 9/9 [==============================] - 0s 2ms/step - loss: 1.4938 - val_loss: 1.6806 Epoch 194/500 9/9 [==============================] - 0s 2ms/step - loss: 1.4752 - val_loss: 1.6546 Epoch 195/500 9/9 [==============================] - 0s 1ms/step - loss: 1.4568 - val_loss: 1.6293 Epoch 196/500 9/9 [==============================] - 0s 2ms/step - loss: 1.4390 - val_loss: 1.6049 Epoch 197/500 9/9 [==============================] - 0s 2ms/step - loss: 1.4210 - val_loss: 1.5808 Epoch 198/500 9/9 [==============================] - 0s 2ms/step - loss: 1.4031 - val_loss: 1.5575 Epoch 199/500 9/9 [==============================] - 0s 1ms/step - loss: 1.3857 - val_loss: 1.5334 Epoch 200/500 9/9 [==============================] - 0s 1ms/step - loss: 1.3684 - val_loss: 1.5114 Epoch 201/500 9/9 [==============================] - 0s 2ms/step - loss: 1.3513 - val_loss: 1.4878 Epoch 202/500 9/9 [==============================] - 0s 2ms/step - loss: 1.3341 - val_loss: 1.4668 Epoch 203/500 9/9 [==============================] - 0s 1ms/step - loss: 1.3173 - val_loss: 1.4431 Epoch 204/500 9/9 [==============================] - 0s 1ms/step - loss: 1.3006 - val_loss: 1.4205 Epoch 205/500 9/9 [==============================] - 0s 2ms/step - loss: 1.2841 - val_loss: 1.3977 Epoch 206/500 9/9 [==============================] - 0s 2ms/step - loss: 1.2679 - val_loss: 1.3771 Epoch 207/500 9/9 [==============================] - 0s 2ms/step - loss: 1.2517 - val_loss: 1.3541 Epoch 208/500 9/9 [==============================] - 0s 1ms/step - loss: 1.2358 - val_loss: 1.3334 Epoch 209/500 9/9 [==============================] - 0s 1ms/step - loss: 1.2198 - val_loss: 1.3130 Epoch 210/500 9/9 [==============================] - 0s 1ms/step - loss: 1.2042 - val_loss: 1.2929 Epoch 211/500 9/9 [==============================] - 0s 1ms/step - loss: 1.1888 - val_loss: 1.2718 Epoch 212/500 9/9 [==============================] - 0s 2ms/step - loss: 1.1734 - val_loss: 1.2505 Epoch 213/500 9/9 [==============================] - 0s 2ms/step - loss: 1.1582 - val_loss: 1.2305 Epoch 214/500 9/9 [==============================] - 0s 2ms/step - loss: 1.1431 - val_loss: 1.2123 Epoch 215/500 9/9 [==============================] - 0s 2ms/step - loss: 1.1284 - val_loss: 1.1923 Epoch 216/500 9/9 [==============================] - 0s 2ms/step - loss: 1.1135 - val_loss: 1.1739 Epoch 217/500 9/9 [==============================] - 0s 2ms/step - loss: 1.0991 - val_loss: 1.1556 Epoch 218/500 9/9 [==============================] - 0s 2ms/step - loss: 1.0848 - val_loss: 1.1372 Epoch 219/500 9/9 [==============================] - 0s 2ms/step - loss: 1.0705 - val_loss: 1.1175 Epoch 220/500 9/9 [==============================] - 0s 2ms/step - loss: 1.0565 - val_loss: 1.0985 Epoch 221/500 9/9 [==============================] - 0s 2ms/step - loss: 1.0425 - val_loss: 1.0817 Epoch 222/500 9/9 [==============================] - 0s 1ms/step - loss: 1.0286 - val_loss: 1.0638 Epoch 223/500 9/9 [==============================] - 0s 2ms/step - loss: 1.0150 - val_loss: 1.0465 Epoch 224/500 9/9 [==============================] - 0s 2ms/step - loss: 1.0016 - val_loss: 1.0295 Epoch 225/500 9/9 [==============================] - 0s 2ms/step - loss: 0.9882 - val_loss: 1.0121 Epoch 226/500 9/9 [==============================] - 0s 2ms/step - loss: 0.9751 - val_loss: 0.9955 Epoch 227/500 9/9 [==============================] - 0s 1ms/step - loss: 0.9620 - val_loss: 0.9794 Epoch 228/500 9/9 [==============================] - 0s 1ms/step - loss: 0.9490 - val_loss: 0.9625 Epoch 229/500 9/9 [==============================] - 0s 2ms/step - loss: 0.9363 - val_loss: 0.9461 Epoch 230/500 9/9 [==============================] - 0s 1ms/step - loss: 0.9237 - val_loss: 0.9302 Epoch 231/500 9/9 [==============================] - 0s 1ms/step - loss: 0.9113 - val_loss: 0.9141 Epoch 232/500 9/9 [==============================] - 0s 2ms/step - loss: 0.8988 - val_loss: 0.8987 Epoch 233/500 9/9 [==============================] - 0s 2ms/step - loss: 0.8868 - val_loss: 0.8831 Epoch 234/500 9/9 [==============================] - 0s 1ms/step - loss: 0.8747 - val_loss: 0.8684 Epoch 235/500 9/9 [==============================] - 0s 2ms/step - loss: 0.8628 - val_loss: 0.8537 Epoch 236/500 9/9 [==============================] - 0s 2ms/step - loss: 0.8509 - val_loss: 0.8394 Epoch 237/500 9/9 [==============================] - 0s 1ms/step - loss: 0.8393 - val_loss: 0.8247 Epoch 238/500 9/9 [==============================] - 0s 2ms/step - loss: 0.8278 - val_loss: 0.8101 Epoch 239/500 9/9 [==============================] - 0s 2ms/step - loss: 0.8164 - val_loss: 0.7968 Epoch 240/500 9/9 [==============================] - 0s 1ms/step - loss: 0.8051 - val_loss: 0.7825 Epoch 241/500 9/9 [==============================] - 0s 1ms/step - loss: 0.7940 - val_loss: 0.7689 Epoch 242/500 9/9 [==============================] - 0s 1ms/step - loss: 0.7831 - val_loss: 0.7554 Epoch 243/500 9/9 [==============================] - 0s 1ms/step - loss: 0.7721 - val_loss: 0.7424 Epoch 244/500 9/9 [==============================] - 0s 2ms/step - loss: 0.7614 - val_loss: 0.7292 Epoch 245/500 9/9 [==============================] - 0s 1ms/step - loss: 0.7507 - val_loss: 0.7159 Epoch 246/500 9/9 [==============================] - 0s 1ms/step - loss: 0.7403 - val_loss: 0.7026 Epoch 247/500 9/9 [==============================] - 0s 2ms/step - loss: 0.7300 - val_loss: 0.6907 Epoch 248/500 9/9 [==============================] - 0s 2ms/step - loss: 0.7197 - val_loss: 0.6785 Epoch 249/500 9/9 [==============================] - 0s 1ms/step - loss: 0.7096 - val_loss: 0.6669 Epoch 250/500 9/9 [==============================] - 0s 1ms/step - loss: 0.6996 - val_loss: 0.6546 Epoch 251/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6896 - val_loss: 0.6430 Epoch 252/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6799 - val_loss: 0.6311 Epoch 253/500 9/9 [==============================] - 0s 1ms/step - loss: 0.6702 - val_loss: 0.6192 Epoch 254/500 9/9 [==============================] - 0s 1ms/step - loss: 0.6608 - val_loss: 0.6076 Epoch 255/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6513 - val_loss: 0.5967 Epoch 256/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6420 - val_loss: 0.5856 Epoch 257/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6329 - val_loss: 0.5744 Epoch 258/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6238 - val_loss: 0.5640 Epoch 259/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6148 - val_loss: 0.5537 Epoch 260/500 9/9 [==============================] - 0s 1ms/step - loss: 0.6060 - val_loss: 0.5435 Epoch 261/500 9/9 [==============================] - 0s 2ms/step - loss: 0.5972 - val_loss: 0.5333 Epoch 262/500 9/9 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 0.5234 Epoch 263/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5801 - val_loss: 0.5131 Epoch 264/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5717 - val_loss: 0.5038 Epoch 265/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5634 - val_loss: 0.4936 Epoch 266/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5552 - val_loss: 0.4847 Epoch 267/500 9/9 [==============================] - 0s 2ms/step - loss: 0.5470 - val_loss: 0.4754 Epoch 268/500 9/9 [==============================] - 0s 2ms/step - loss: 0.5391 - val_loss: 0.4663 Epoch 269/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5312 - val_loss: 0.4569 Epoch 270/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5234 - val_loss: 0.4483 Epoch 271/500 9/9 [==============================] - 0s 2ms/step - loss: 0.5157 - val_loss: 0.4397 Epoch 272/500 9/9 [==============================] - 0s 2ms/step - loss: 0.5081 - val_loss: 0.4307 Epoch 273/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5007 - val_loss: 0.4224 Epoch 274/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4933 - val_loss: 0.4148 Epoch 275/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4860 - val_loss: 0.4066 Epoch 276/500 9/9 [==============================] - 0s 1ms/step - loss: 0.4788 - val_loss: 0.3987 Epoch 277/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4717 - val_loss: 0.3905 Epoch 278/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4647 - val_loss: 0.3831 Epoch 279/500 9/9 [==============================] - 0s 1ms/step - loss: 0.4577 - val_loss: 0.3754 Epoch 280/500 9/9 [==============================] - 0s 1ms/step - loss: 0.4509 - val_loss: 0.3683 Epoch 281/500 9/9 [==============================] - 0s 1ms/step - loss: 0.4442 - val_loss: 0.3608 Epoch 282/500 9/9 [==============================] - 0s 1ms/step - loss: 0.4376 - val_loss: 0.3536 Epoch 283/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4310 - val_loss: 0.3468 Epoch 284/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4245 - val_loss: 0.3395 Epoch 285/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4182 - val_loss: 0.3326 Epoch 286/500 9/9 [==============================] - 0s 1ms/step - loss: 0.4120 - val_loss: 0.3255 Epoch 287/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4057 - val_loss: 0.3190 Epoch 288/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3996 - val_loss: 0.3124 Epoch 289/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3936 - val_loss: 0.3065 Epoch 290/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3876 - val_loss: 0.3002 Epoch 291/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3818 - val_loss: 0.2941 Epoch 292/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3760 - val_loss: 0.2880 Epoch 293/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3703 - val_loss: 0.2820 Epoch 294/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.2763 Epoch 295/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3591 - val_loss: 0.2704 Epoch 296/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3537 - val_loss: 0.2649 Epoch 297/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3483 - val_loss: 0.2594 Epoch 298/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3430 - val_loss: 0.2540 Epoch 299/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3378 - val_loss: 0.2489 Epoch 300/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3326 - val_loss: 0.2438 Epoch 301/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3275 - val_loss: 0.2386 Epoch 302/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3225 - val_loss: 0.2335 Epoch 303/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3176 - val_loss: 0.2288 Epoch 304/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3128 - val_loss: 0.2238 Epoch 305/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3080 - val_loss: 0.2190 Epoch 306/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3032 - val_loss: 0.2144 Epoch 307/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2986 - val_loss: 0.2099 Epoch 308/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2941 - val_loss: 0.2054 Epoch 309/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2895 - val_loss: 0.2010 Epoch 310/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2851 - val_loss: 0.1969 Epoch 311/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2807 - val_loss: 0.1927 Epoch 312/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 0.1885 Epoch 313/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2722 - val_loss: 0.1847 Epoch 314/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2680 - val_loss: 0.1807 Epoch 315/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2639 - val_loss: 0.1769 Epoch 316/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2598 - val_loss: 0.1731 Epoch 317/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2559 - val_loss: 0.1694 Epoch 318/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2519 - val_loss: 0.1658 Epoch 319/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2480 - val_loss: 0.1622 Epoch 320/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2443 - val_loss: 0.1590 Epoch 321/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2405 - val_loss: 0.1555 Epoch 322/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2368 - val_loss: 0.1522 Epoch 323/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2332 - val_loss: 0.1488 Epoch 324/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2296 - val_loss: 0.1457 Epoch 325/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2261 - val_loss: 0.1428 Epoch 326/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2227 - val_loss: 0.1396 Epoch 327/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2192 - val_loss: 0.1366 Epoch 328/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2159 - val_loss: 0.1337 Epoch 329/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2126 - val_loss: 0.1312 Epoch 330/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2094 - val_loss: 0.1284 Epoch 331/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2062 - val_loss: 0.1255 Epoch 332/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2030 - val_loss: 0.1228 Epoch 333/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2000 - val_loss: 0.1203 Epoch 334/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1969 - val_loss: 0.1179 Epoch 335/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1940 - val_loss: 0.1153 Epoch 336/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1910 - val_loss: 0.1129 Epoch 337/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1881 - val_loss: 0.1107 Epoch 338/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1853 - val_loss: 0.1082 Epoch 339/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1825 - val_loss: 0.1060 Epoch 340/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1797 - val_loss: 0.1039 Epoch 341/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1770 - val_loss: 0.1018 Epoch 342/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1744 - val_loss: 0.0996 Epoch 343/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1718 - val_loss: 0.0976 Epoch 344/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1692 - val_loss: 0.0956 Epoch 345/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1667 - val_loss: 0.0939 Epoch 346/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1642 - val_loss: 0.0918 Epoch 347/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1618 - val_loss: 0.0900 Epoch 348/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1594 - val_loss: 0.0882 Epoch 349/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1570 - val_loss: 0.0865 Epoch 350/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1547 - val_loss: 0.0849 Epoch 351/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1524 - val_loss: 0.0833 Epoch 352/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1502 - val_loss: 0.0815 Epoch 353/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1480 - val_loss: 0.0800 Epoch 354/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1458 - val_loss: 0.0785 Epoch 355/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1437 - val_loss: 0.0769 Epoch 356/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1416 - val_loss: 0.0755 Epoch 357/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1396 - val_loss: 0.0741 Epoch 358/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1376 - val_loss: 0.0728 Epoch 359/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1356 - val_loss: 0.0715 Epoch 360/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1337 - val_loss: 0.0702 Epoch 361/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1317 - val_loss: 0.0691 Epoch 362/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1298 - val_loss: 0.0678 Epoch 363/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1280 - val_loss: 0.0667 Epoch 364/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1262 - val_loss: 0.0654 Epoch 365/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1244 - val_loss: 0.0643 Epoch 366/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1227 - val_loss: 0.0632 Epoch 367/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1210 - val_loss: 0.0622 Epoch 368/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1193 - val_loss: 0.0612 Epoch 369/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1177 - val_loss: 0.0601 Epoch 370/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1160 - val_loss: 0.0592 Epoch 371/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1144 - val_loss: 0.0582 Epoch 372/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.0574 Epoch 373/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1114 - val_loss: 0.0565 Epoch 374/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1099 - val_loss: 0.0556 Epoch 375/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1084 - val_loss: 0.0548 Epoch 376/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1069 - val_loss: 0.0541 Epoch 377/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 0.0533 Epoch 378/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1041 - val_loss: 0.0525 Epoch 379/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1028 - val_loss: 0.0518 Epoch 380/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1014 - val_loss: 0.0511 Epoch 381/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1001 - val_loss: 0.0504 Epoch 382/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0988 - val_loss: 0.0498 Epoch 383/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0975 - val_loss: 0.0492 Epoch 384/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0963 - val_loss: 0.0486 Epoch 385/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0951 - val_loss: 0.0480 Epoch 386/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0939 - val_loss: 0.0474 Epoch 387/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0927 - val_loss: 0.0469 Epoch 388/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0915 - val_loss: 0.0463 Epoch 389/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0904 - val_loss: 0.0458 Epoch 390/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0893 - val_loss: 0.0453 Epoch 391/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0882 - val_loss: 0.0448 Epoch 392/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0871 - val_loss: 0.0444 Epoch 393/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0861 - val_loss: 0.0439 Epoch 394/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0851 - val_loss: 0.0435 Epoch 395/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0841 - val_loss: 0.0431 Epoch 396/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0831 - val_loss: 0.0427 Epoch 397/500 9/9 [==============================] - ETA: 0s - loss: 0.088 - 0s 2ms/step - loss: 0.0821 - val_loss: 0.0423 Epoch 398/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0812 - val_loss: 0.0420 Epoch 399/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0802 - val_loss: 0.0416 Epoch 400/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0793 - val_loss: 0.0413 Epoch 401/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0784 - val_loss: 0.0410 Epoch 402/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0775 - val_loss: 0.0407 Epoch 403/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0767 - val_loss: 0.0403 Epoch 404/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0758 - val_loss: 0.0401 Epoch 405/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0750 - val_loss: 0.0398 Epoch 406/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0742 - val_loss: 0.0395 Epoch 407/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0734 - val_loss: 0.0392 Epoch 408/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0726 - val_loss: 0.0390 Epoch 409/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0718 - val_loss: 0.0388 Epoch 410/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0711 - val_loss: 0.0386 Epoch 411/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0704 - val_loss: 0.0384 Epoch 412/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0696 - val_loss: 0.0382 Epoch 413/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0689 - val_loss: 0.0380 Epoch 414/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0682 - val_loss: 0.0378 Epoch 415/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0676 - val_loss: 0.0376 Epoch 416/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0669 - val_loss: 0.0374 Epoch 417/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0663 - val_loss: 0.0373 Epoch 418/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0656 - val_loss: 0.0371 Epoch 419/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0650 - val_loss: 0.0370 Epoch 420/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0644 - val_loss: 0.0369 Epoch 421/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0367 Epoch 422/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0632 - val_loss: 0.0366 Epoch 423/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0626 - val_loss: 0.0365 Epoch 424/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0620 - val_loss: 0.0364 Epoch 425/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0615 - val_loss: 0.0363 Epoch 426/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0609 - val_loss: 0.0362 Epoch 427/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0604 - val_loss: 0.0361 Epoch 428/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0599 - val_loss: 0.0360 Epoch 429/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0594 - val_loss: 0.0359 Epoch 430/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0589 - val_loss: 0.0358 Epoch 431/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0584 - val_loss: 0.0358 Epoch 432/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0579 - val_loss: 0.0357 Epoch 433/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0574 - val_loss: 0.0356 Epoch 434/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0570 - val_loss: 0.0356 Epoch 435/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0565 - val_loss: 0.0355 Epoch 436/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0561 - val_loss: 0.0355 Epoch 437/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0556 - val_loss: 0.0354 Epoch 438/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0552 - val_loss: 0.0354 Epoch 439/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0548 - val_loss: 0.0353 Epoch 440/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0544 - val_loss: 0.0353 Epoch 441/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0540 - val_loss: 0.0353 Epoch 442/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0536 - val_loss: 0.0352 Epoch 443/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0532 - val_loss: 0.0352 Epoch 444/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0528 - val_loss: 0.0352 Epoch 445/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0525 - val_loss: 0.0352 Epoch 446/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0521 - val_loss: 0.0351 Epoch 447/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0518 - val_loss: 0.0351 Epoch 448/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0514 - val_loss: 0.0351 Epoch 449/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0511 - val_loss: 0.0351 Epoch 450/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0507 - val_loss: 0.0351 Epoch 451/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0504 - val_loss: 0.0350 Epoch 452/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0501 - val_loss: 0.0350 Epoch 453/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0498 - val_loss: 0.0350 Epoch 454/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0495 - val_loss: 0.0350 Epoch 455/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0492 - val_loss: 0.0350 Epoch 456/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0489 - val_loss: 0.0350 Epoch 457/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0486 - val_loss: 0.0350 Epoch 458/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0483 - val_loss: 0.0350 Epoch 459/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0480 - val_loss: 0.0350 Epoch 460/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0478 - val_loss: 0.0350 Epoch 461/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0475 - val_loss: 0.0350 Epoch 462/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0473 - val_loss: 0.0350 Epoch 463/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0470 - val_loss: 0.0350 Epoch 464/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0468 - val_loss: 0.0350 Epoch 465/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0465 - val_loss: 0.0350 Epoch 466/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0463 - val_loss: 0.0350 Epoch 467/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0460 - val_loss: 0.0350 Epoch 468/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0458 - val_loss: 0.0350 Epoch 469/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0456 - val_loss: 0.0350 Epoch 470/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0454 - val_loss: 0.0350 Epoch 471/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0452 - val_loss: 0.0351 Epoch 472/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.0351 Epoch 473/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.0351 Epoch 474/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0445 - val_loss: 0.0351 Epoch 475/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0443 - val_loss: 0.0351 Epoch 476/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0442 - val_loss: 0.0351 Epoch 477/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0440 - val_loss: 0.0351 Epoch 478/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0438 - val_loss: 0.0351 Epoch 479/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0436 - val_loss: 0.0351 Epoch 480/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.0351 Epoch 481/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0432 - val_loss: 0.0351 Epoch 482/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0431 - val_loss: 0.0351 Epoch 483/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0429 - val_loss: 0.0351 Epoch 484/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0428 - val_loss: 0.0352 Epoch 485/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0426 - val_loss: 0.0352 Epoch 486/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0424 - val_loss: 0.0352 Epoch 487/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0423 - val_loss: 0.0352 Epoch 488/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0421 - val_loss: 0.0352 Epoch 489/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0420 - val_loss: 0.0352 Epoch 490/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0418 - val_loss: 0.0352 Epoch 491/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0417 - val_loss: 0.0352 Epoch 492/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0416 - val_loss: 0.0352 Epoch 493/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0414 - val_loss: 0.0352 Epoch 494/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0413 - val_loss: 0.0352 Epoch 495/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0412 - val_loss: 0.0352 Epoch 496/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0411 - val_loss: 0.0353 Epoch 497/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0409 - val_loss: 0.0353 Epoch 498/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0408 - val_loss: 0.0353 Epoch 499/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0407 - val_loss: 0.0353 Epoch 500/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0406 - val_loss: 0.0353 . &lt;keras.callbacks.History at 0x7f708be9a470&gt; . . plt.plot(y,&#39;.&#39;,alpha=0.1) plt.plot(net(X),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f708bc30580&gt;] . #%tensorboard --logdir logs --host 0.0.0.0 . &#53584;&#49436;&#48372;&#46300;: &#51201;&#54633;&#44208;&#44284; &#49884;&#44033;&#54868; . - 시각화결과는 모두 텐서보드에서 보고 싶다! 적합결과를 보여주는 fig 오브젝트를 텐서보드에 끼워넣어서 출력하는 방법을 알아보자. . !rm -rf logs net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) net.fit(X,y,epochs=500,batch_size=100, validation_split=0.1, callbacks=tf.keras.callbacks.TensorBoard()) . Epoch 1/500 9/9 [==============================] - 0s 5ms/step - loss: 11.9832 - val_loss: 10.8663 Epoch 2/500 9/9 [==============================] - 0s 2ms/step - loss: 11.8783 - val_loss: 10.7961 Epoch 3/500 9/9 [==============================] - 0s 2ms/step - loss: 11.7750 - val_loss: 10.7292 Epoch 4/500 9/9 [==============================] - 0s 2ms/step - loss: 11.6712 - val_loss: 10.6628 Epoch 5/500 9/9 [==============================] - 0s 2ms/step - loss: 11.5688 - val_loss: 10.5916 Epoch 6/500 9/9 [==============================] - 0s 1ms/step - loss: 11.4671 - val_loss: 10.5235 Epoch 7/500 9/9 [==============================] - 0s 2ms/step - loss: 11.3650 - val_loss: 10.4527 Epoch 8/500 9/9 [==============================] - 0s 2ms/step - loss: 11.2672 - val_loss: 10.3818 Epoch 9/500 9/9 [==============================] - 0s 2ms/step - loss: 11.1664 - val_loss: 10.3141 Epoch 10/500 9/9 [==============================] - 0s 2ms/step - loss: 11.0684 - val_loss: 10.2464 Epoch 11/500 9/9 [==============================] - 0s 2ms/step - loss: 10.9698 - val_loss: 10.1806 Epoch 12/500 9/9 [==============================] - 0s 1ms/step - loss: 10.8727 - val_loss: 10.1120 Epoch 13/500 9/9 [==============================] - 0s 2ms/step - loss: 10.7777 - val_loss: 10.0403 Epoch 14/500 9/9 [==============================] - 0s 1ms/step - loss: 10.6817 - val_loss: 9.9739 Epoch 15/500 9/9 [==============================] - 0s 2ms/step - loss: 10.5871 - val_loss: 9.9000 Epoch 16/500 9/9 [==============================] - 0s 2ms/step - loss: 10.4931 - val_loss: 9.8320 Epoch 17/500 9/9 [==============================] - 0s 1ms/step - loss: 10.3994 - val_loss: 9.7659 Epoch 18/500 9/9 [==============================] - 0s 1ms/step - loss: 10.3062 - val_loss: 9.7024 Epoch 19/500 9/9 [==============================] - 0s 2ms/step - loss: 10.2162 - val_loss: 9.6333 Epoch 20/500 9/9 [==============================] - 0s 2ms/step - loss: 10.1243 - val_loss: 9.5670 Epoch 21/500 9/9 [==============================] - 0s 2ms/step - loss: 10.0331 - val_loss: 9.5003 Epoch 22/500 9/9 [==============================] - 0s 1ms/step - loss: 9.9449 - val_loss: 9.4318 Epoch 23/500 9/9 [==============================] - 0s 1ms/step - loss: 9.8568 - val_loss: 9.3642 Epoch 24/500 9/9 [==============================] - 0s 2ms/step - loss: 9.7675 - val_loss: 9.2992 Epoch 25/500 9/9 [==============================] - 0s 2ms/step - loss: 9.6794 - val_loss: 9.2379 Epoch 26/500 9/9 [==============================] - 0s 2ms/step - loss: 9.5940 - val_loss: 9.1682 Epoch 27/500 9/9 [==============================] - 0s 2ms/step - loss: 9.5071 - val_loss: 9.1010 Epoch 28/500 9/9 [==============================] - 0s 2ms/step - loss: 9.4226 - val_loss: 9.0350 Epoch 29/500 9/9 [==============================] - 0s 2ms/step - loss: 9.3376 - val_loss: 8.9707 Epoch 30/500 9/9 [==============================] - 0s 2ms/step - loss: 9.2533 - val_loss: 8.9077 Epoch 31/500 9/9 [==============================] - 0s 2ms/step - loss: 9.1692 - val_loss: 8.8398 Epoch 32/500 9/9 [==============================] - 0s 2ms/step - loss: 9.0872 - val_loss: 8.7771 Epoch 33/500 9/9 [==============================] - 0s 1ms/step - loss: 9.0056 - val_loss: 8.7131 Epoch 34/500 9/9 [==============================] - 0s 2ms/step - loss: 8.9235 - val_loss: 8.6456 Epoch 35/500 9/9 [==============================] - 0s 2ms/step - loss: 8.8424 - val_loss: 8.5801 Epoch 36/500 9/9 [==============================] - 0s 1ms/step - loss: 8.7621 - val_loss: 8.5169 Epoch 37/500 9/9 [==============================] - 0s 2ms/step - loss: 8.6824 - val_loss: 8.4507 Epoch 38/500 9/9 [==============================] - 0s 2ms/step - loss: 8.6039 - val_loss: 8.3903 Epoch 39/500 9/9 [==============================] - 0s 1ms/step - loss: 8.5253 - val_loss: 8.3273 Epoch 40/500 9/9 [==============================] - 0s 2ms/step - loss: 8.4475 - val_loss: 8.2648 Epoch 41/500 9/9 [==============================] - 0s 2ms/step - loss: 8.3703 - val_loss: 8.2014 Epoch 42/500 9/9 [==============================] - 0s 2ms/step - loss: 8.2945 - val_loss: 8.1372 Epoch 43/500 9/9 [==============================] - 0s 2ms/step - loss: 8.2177 - val_loss: 8.0751 Epoch 44/500 9/9 [==============================] - 0s 1ms/step - loss: 8.1433 - val_loss: 8.0141 Epoch 45/500 9/9 [==============================] - 0s 2ms/step - loss: 8.0678 - val_loss: 7.9512 Epoch 46/500 9/9 [==============================] - 0s 1ms/step - loss: 7.9936 - val_loss: 7.8923 Epoch 47/500 9/9 [==============================] - 0s 2ms/step - loss: 7.9200 - val_loss: 7.8267 Epoch 48/500 9/9 [==============================] - 0s 1ms/step - loss: 7.8475 - val_loss: 7.7637 Epoch 49/500 9/9 [==============================] - 0s 2ms/step - loss: 7.7742 - val_loss: 7.7034 Epoch 50/500 9/9 [==============================] - 0s 2ms/step - loss: 7.7027 - val_loss: 7.6417 Epoch 51/500 9/9 [==============================] - 0s 2ms/step - loss: 7.6314 - val_loss: 7.5816 Epoch 52/500 9/9 [==============================] - 0s 1ms/step - loss: 7.5596 - val_loss: 7.5203 Epoch 53/500 9/9 [==============================] - 0s 2ms/step - loss: 7.4893 - val_loss: 7.4622 Epoch 54/500 9/9 [==============================] - 0s 1ms/step - loss: 7.4199 - val_loss: 7.4046 Epoch 55/500 9/9 [==============================] - 0s 2ms/step - loss: 7.3517 - val_loss: 7.3413 Epoch 56/500 9/9 [==============================] - 0s 1ms/step - loss: 7.2821 - val_loss: 7.2826 Epoch 57/500 9/9 [==============================] - 0s 2ms/step - loss: 7.2138 - val_loss: 7.2205 Epoch 58/500 9/9 [==============================] - 0s 1ms/step - loss: 7.1461 - val_loss: 7.1610 Epoch 59/500 9/9 [==============================] - 0s 2ms/step - loss: 7.0787 - val_loss: 7.1049 Epoch 60/500 9/9 [==============================] - 0s 2ms/step - loss: 7.0124 - val_loss: 7.0466 Epoch 61/500 9/9 [==============================] - 0s 2ms/step - loss: 6.9463 - val_loss: 6.9873 Epoch 62/500 9/9 [==============================] - 0s 1ms/step - loss: 6.8811 - val_loss: 6.9300 Epoch 63/500 9/9 [==============================] - 0s 1ms/step - loss: 6.8151 - val_loss: 6.8720 Epoch 64/500 9/9 [==============================] - 0s 1ms/step - loss: 6.7508 - val_loss: 6.8148 Epoch 65/500 9/9 [==============================] - 0s 2ms/step - loss: 6.6870 - val_loss: 6.7572 Epoch 66/500 9/9 [==============================] - 0s 2ms/step - loss: 6.6227 - val_loss: 6.7017 Epoch 67/500 9/9 [==============================] - 0s 1ms/step - loss: 6.5596 - val_loss: 6.6439 Epoch 68/500 9/9 [==============================] - 0s 2ms/step - loss: 6.4973 - val_loss: 6.5899 Epoch 69/500 9/9 [==============================] - 0s 1ms/step - loss: 6.4353 - val_loss: 6.5328 Epoch 70/500 9/9 [==============================] - 0s 2ms/step - loss: 6.3738 - val_loss: 6.4786 Epoch 71/500 9/9 [==============================] - 0s 2ms/step - loss: 6.3121 - val_loss: 6.4191 Epoch 72/500 9/9 [==============================] - 0s 2ms/step - loss: 6.2514 - val_loss: 6.3663 Epoch 73/500 9/9 [==============================] - 0s 2ms/step - loss: 6.1910 - val_loss: 6.3117 Epoch 74/500 9/9 [==============================] - 0s 2ms/step - loss: 6.1314 - val_loss: 6.2588 Epoch 75/500 9/9 [==============================] - 0s 2ms/step - loss: 6.0724 - val_loss: 6.2038 Epoch 76/500 9/9 [==============================] - 0s 2ms/step - loss: 6.0138 - val_loss: 6.1501 Epoch 77/500 9/9 [==============================] - 0s 2ms/step - loss: 5.9543 - val_loss: 6.0937 Epoch 78/500 9/9 [==============================] - 0s 2ms/step - loss: 5.8974 - val_loss: 6.0370 Epoch 79/500 9/9 [==============================] - 0s 2ms/step - loss: 5.8396 - val_loss: 5.9847 Epoch 80/500 9/9 [==============================] - 0s 2ms/step - loss: 5.7821 - val_loss: 5.9322 Epoch 81/500 9/9 [==============================] - 0s 2ms/step - loss: 5.7258 - val_loss: 5.8811 Epoch 82/500 9/9 [==============================] - 0s 2ms/step - loss: 5.6695 - val_loss: 5.8268 Epoch 83/500 9/9 [==============================] - 0s 2ms/step - loss: 5.6137 - val_loss: 5.7750 Epoch 84/500 9/9 [==============================] - 0s 2ms/step - loss: 5.5585 - val_loss: 5.7240 Epoch 85/500 9/9 [==============================] - 0s 2ms/step - loss: 5.5036 - val_loss: 5.6720 Epoch 86/500 9/9 [==============================] - 0s 2ms/step - loss: 5.4493 - val_loss: 5.6208 Epoch 87/500 9/9 [==============================] - 0s 2ms/step - loss: 5.3949 - val_loss: 5.5691 Epoch 88/500 9/9 [==============================] - 0s 2ms/step - loss: 5.3418 - val_loss: 5.5173 Epoch 89/500 9/9 [==============================] - 0s 2ms/step - loss: 5.2885 - val_loss: 5.4681 Epoch 90/500 9/9 [==============================] - 0s 2ms/step - loss: 5.2360 - val_loss: 5.4166 Epoch 91/500 9/9 [==============================] - 0s 2ms/step - loss: 5.1837 - val_loss: 5.3662 Epoch 92/500 9/9 [==============================] - 0s 2ms/step - loss: 5.1319 - val_loss: 5.3159 Epoch 93/500 9/9 [==============================] - 0s 2ms/step - loss: 5.0797 - val_loss: 5.2649 Epoch 94/500 9/9 [==============================] - 0s 1ms/step - loss: 5.0286 - val_loss: 5.2175 Epoch 95/500 9/9 [==============================] - 0s 2ms/step - loss: 4.9783 - val_loss: 5.1691 Epoch 96/500 9/9 [==============================] - 0s 2ms/step - loss: 4.9281 - val_loss: 5.1196 Epoch 97/500 9/9 [==============================] - 0s 2ms/step - loss: 4.8785 - val_loss: 5.0692 Epoch 98/500 9/9 [==============================] - 0s 2ms/step - loss: 4.8286 - val_loss: 5.0219 Epoch 99/500 9/9 [==============================] - 0s 1ms/step - loss: 4.7799 - val_loss: 4.9733 Epoch 100/500 9/9 [==============================] - 0s 2ms/step - loss: 4.7310 - val_loss: 4.9264 Epoch 101/500 9/9 [==============================] - 0s 2ms/step - loss: 4.6824 - val_loss: 4.8787 Epoch 102/500 9/9 [==============================] - 0s 2ms/step - loss: 4.6351 - val_loss: 4.8324 Epoch 103/500 9/9 [==============================] - 0s 2ms/step - loss: 4.5871 - val_loss: 4.7854 Epoch 104/500 9/9 [==============================] - 0s 2ms/step - loss: 4.5403 - val_loss: 4.7400 Epoch 105/500 9/9 [==============================] - 0s 2ms/step - loss: 4.4937 - val_loss: 4.6915 Epoch 106/500 9/9 [==============================] - 0s 2ms/step - loss: 4.4473 - val_loss: 4.6465 Epoch 107/500 9/9 [==============================] - 0s 2ms/step - loss: 4.4013 - val_loss: 4.5990 Epoch 108/500 9/9 [==============================] - 0s 2ms/step - loss: 4.3553 - val_loss: 4.5538 Epoch 109/500 9/9 [==============================] - 0s 2ms/step - loss: 4.3105 - val_loss: 4.5088 Epoch 110/500 9/9 [==============================] - 0s 2ms/step - loss: 4.2656 - val_loss: 4.4612 Epoch 111/500 9/9 [==============================] - 0s 2ms/step - loss: 4.2210 - val_loss: 4.4165 Epoch 112/500 9/9 [==============================] - 0s 2ms/step - loss: 4.1765 - val_loss: 4.3754 Epoch 113/500 9/9 [==============================] - 0s 2ms/step - loss: 4.1330 - val_loss: 4.3317 Epoch 114/500 9/9 [==============================] - 0s 2ms/step - loss: 4.0899 - val_loss: 4.2881 Epoch 115/500 9/9 [==============================] - 0s 2ms/step - loss: 4.0470 - val_loss: 4.2423 Epoch 116/500 9/9 [==============================] - 0s 2ms/step - loss: 4.0040 - val_loss: 4.1998 Epoch 117/500 9/9 [==============================] - 0s 2ms/step - loss: 3.9619 - val_loss: 4.1579 Epoch 118/500 9/9 [==============================] - 0s 2ms/step - loss: 3.9198 - val_loss: 4.1154 Epoch 119/500 9/9 [==============================] - 0s 1ms/step - loss: 3.8786 - val_loss: 4.0717 Epoch 120/500 9/9 [==============================] - 0s 2ms/step - loss: 3.8369 - val_loss: 4.0288 Epoch 121/500 9/9 [==============================] - 0s 2ms/step - loss: 3.7964 - val_loss: 3.9861 Epoch 122/500 9/9 [==============================] - 0s 2ms/step - loss: 3.7558 - val_loss: 3.9436 Epoch 123/500 9/9 [==============================] - 0s 2ms/step - loss: 3.7153 - val_loss: 3.9022 Epoch 124/500 9/9 [==============================] - 0s 2ms/step - loss: 3.6752 - val_loss: 3.8638 Epoch 125/500 9/9 [==============================] - 0s 2ms/step - loss: 3.6362 - val_loss: 3.8228 Epoch 126/500 9/9 [==============================] - 0s 2ms/step - loss: 3.5969 - val_loss: 3.7823 Epoch 127/500 9/9 [==============================] - 0s 2ms/step - loss: 3.5579 - val_loss: 3.7407 Epoch 128/500 9/9 [==============================] - 0s 1ms/step - loss: 3.5196 - val_loss: 3.7022 Epoch 129/500 9/9 [==============================] - 0s 1ms/step - loss: 3.4812 - val_loss: 3.6638 Epoch 130/500 9/9 [==============================] - 0s 2ms/step - loss: 3.4437 - val_loss: 3.6228 Epoch 131/500 9/9 [==============================] - 0s 1ms/step - loss: 3.4061 - val_loss: 3.5835 Epoch 132/500 9/9 [==============================] - 0s 2ms/step - loss: 3.3685 - val_loss: 3.5470 Epoch 133/500 9/9 [==============================] - 0s 2ms/step - loss: 3.3320 - val_loss: 3.5076 Epoch 134/500 9/9 [==============================] - 0s 2ms/step - loss: 3.2952 - val_loss: 3.4692 Epoch 135/500 9/9 [==============================] - 0s 1ms/step - loss: 3.2594 - val_loss: 3.4310 Epoch 136/500 9/9 [==============================] - 0s 2ms/step - loss: 3.2230 - val_loss: 3.3932 Epoch 137/500 9/9 [==============================] - 0s 2ms/step - loss: 3.1874 - val_loss: 3.3545 Epoch 138/500 9/9 [==============================] - 0s 2ms/step - loss: 3.1525 - val_loss: 3.3177 Epoch 139/500 9/9 [==============================] - 0s 2ms/step - loss: 3.1174 - val_loss: 3.2801 Epoch 140/500 9/9 [==============================] - 0s 2ms/step - loss: 3.0823 - val_loss: 3.2447 Epoch 141/500 9/9 [==============================] - 0s 1ms/step - loss: 3.0488 - val_loss: 3.2077 Epoch 142/500 9/9 [==============================] - 0s 2ms/step - loss: 3.0140 - val_loss: 3.1742 Epoch 143/500 9/9 [==============================] - 0s 2ms/step - loss: 2.9804 - val_loss: 3.1380 Epoch 144/500 9/9 [==============================] - 0s 2ms/step - loss: 2.9471 - val_loss: 3.1027 Epoch 145/500 9/9 [==============================] - 0s 2ms/step - loss: 2.9140 - val_loss: 3.0676 Epoch 146/500 9/9 [==============================] - 0s 2ms/step - loss: 2.8808 - val_loss: 3.0307 Epoch 147/500 9/9 [==============================] - 0s 1ms/step - loss: 2.8484 - val_loss: 2.9959 Epoch 148/500 9/9 [==============================] - 0s 1ms/step - loss: 2.8164 - val_loss: 2.9628 Epoch 149/500 9/9 [==============================] - 0s 2ms/step - loss: 2.7844 - val_loss: 2.9261 Epoch 150/500 9/9 [==============================] - 0s 2ms/step - loss: 2.7524 - val_loss: 2.8912 Epoch 151/500 9/9 [==============================] - 0s 1ms/step - loss: 2.7214 - val_loss: 2.8600 Epoch 152/500 9/9 [==============================] - 0s 2ms/step - loss: 2.6900 - val_loss: 2.8276 Epoch 153/500 9/9 [==============================] - 0s 2ms/step - loss: 2.6593 - val_loss: 2.7939 Epoch 154/500 9/9 [==============================] - 0s 2ms/step - loss: 2.6286 - val_loss: 2.7605 Epoch 155/500 9/9 [==============================] - 0s 2ms/step - loss: 2.5985 - val_loss: 2.7283 Epoch 156/500 9/9 [==============================] - 0s 1ms/step - loss: 2.5685 - val_loss: 2.6961 Epoch 157/500 9/9 [==============================] - 0s 2ms/step - loss: 2.5390 - val_loss: 2.6633 Epoch 158/500 9/9 [==============================] - 0s 1ms/step - loss: 2.5093 - val_loss: 2.6319 Epoch 159/500 9/9 [==============================] - 0s 1ms/step - loss: 2.4803 - val_loss: 2.6009 Epoch 160/500 9/9 [==============================] - 0s 2ms/step - loss: 2.4516 - val_loss: 2.5689 Epoch 161/500 9/9 [==============================] - 0s 1ms/step - loss: 2.4225 - val_loss: 2.5390 Epoch 162/500 9/9 [==============================] - 0s 2ms/step - loss: 2.3946 - val_loss: 2.5096 Epoch 163/500 9/9 [==============================] - 0s 2ms/step - loss: 2.3666 - val_loss: 2.4766 Epoch 164/500 9/9 [==============================] - 0s 2ms/step - loss: 2.3385 - val_loss: 2.4479 Epoch 165/500 9/9 [==============================] - 0s 1ms/step - loss: 2.3107 - val_loss: 2.4180 Epoch 166/500 9/9 [==============================] - 0s 2ms/step - loss: 2.2840 - val_loss: 2.3889 Epoch 167/500 9/9 [==============================] - 0s 2ms/step - loss: 2.2567 - val_loss: 2.3590 Epoch 168/500 9/9 [==============================] - 0s 2ms/step - loss: 2.2301 - val_loss: 2.3314 Epoch 169/500 9/9 [==============================] - 0s 2ms/step - loss: 2.2035 - val_loss: 2.3024 Epoch 170/500 9/9 [==============================] - 0s 2ms/step - loss: 2.1773 - val_loss: 2.2721 Epoch 171/500 9/9 [==============================] - 0s 1ms/step - loss: 2.1512 - val_loss: 2.2444 Epoch 172/500 9/9 [==============================] - 0s 1ms/step - loss: 2.1257 - val_loss: 2.2158 Epoch 173/500 9/9 [==============================] - 0s 1ms/step - loss: 2.0998 - val_loss: 2.1893 Epoch 174/500 9/9 [==============================] - 0s 2ms/step - loss: 2.0749 - val_loss: 2.1608 Epoch 175/500 9/9 [==============================] - 0s 2ms/step - loss: 2.0499 - val_loss: 2.1329 Epoch 176/500 9/9 [==============================] - 0s 2ms/step - loss: 2.0252 - val_loss: 2.1053 Epoch 177/500 9/9 [==============================] - 0s 2ms/step - loss: 2.0007 - val_loss: 2.0789 Epoch 178/500 9/9 [==============================] - 0s 2ms/step - loss: 1.9762 - val_loss: 2.0535 Epoch 179/500 9/9 [==============================] - 0s 2ms/step - loss: 1.9523 - val_loss: 2.0267 Epoch 180/500 9/9 [==============================] - 0s 2ms/step - loss: 1.9286 - val_loss: 1.9995 Epoch 181/500 9/9 [==============================] - 0s 2ms/step - loss: 1.9051 - val_loss: 1.9733 Epoch 182/500 9/9 [==============================] - 0s 2ms/step - loss: 1.8815 - val_loss: 1.9489 Epoch 183/500 9/9 [==============================] - 0s 1ms/step - loss: 1.8588 - val_loss: 1.9230 Epoch 184/500 9/9 [==============================] - 0s 2ms/step - loss: 1.8357 - val_loss: 1.8970 Epoch 185/500 9/9 [==============================] - 0s 2ms/step - loss: 1.8133 - val_loss: 1.8730 Epoch 186/500 9/9 [==============================] - 0s 1ms/step - loss: 1.7908 - val_loss: 1.8493 Epoch 187/500 9/9 [==============================] - 0s 1ms/step - loss: 1.7688 - val_loss: 1.8231 Epoch 188/500 9/9 [==============================] - 0s 2ms/step - loss: 1.7467 - val_loss: 1.7991 Epoch 189/500 9/9 [==============================] - 0s 2ms/step - loss: 1.7249 - val_loss: 1.7762 Epoch 190/500 9/9 [==============================] - 0s 1ms/step - loss: 1.7032 - val_loss: 1.7522 Epoch 191/500 9/9 [==============================] - 0s 2ms/step - loss: 1.6821 - val_loss: 1.7295 Epoch 192/500 9/9 [==============================] - 0s 1ms/step - loss: 1.6611 - val_loss: 1.7063 Epoch 193/500 9/9 [==============================] - 0s 1ms/step - loss: 1.6404 - val_loss: 1.6835 Epoch 194/500 9/9 [==============================] - 0s 2ms/step - loss: 1.6197 - val_loss: 1.6604 Epoch 195/500 9/9 [==============================] - 0s 2ms/step - loss: 1.5993 - val_loss: 1.6368 Epoch 196/500 9/9 [==============================] - 0s 2ms/step - loss: 1.5791 - val_loss: 1.6148 Epoch 197/500 9/9 [==============================] - 0s 1ms/step - loss: 1.5591 - val_loss: 1.5931 Epoch 198/500 9/9 [==============================] - 0s 1ms/step - loss: 1.5395 - val_loss: 1.5701 Epoch 199/500 9/9 [==============================] - 0s 2ms/step - loss: 1.5196 - val_loss: 1.5490 Epoch 200/500 9/9 [==============================] - 0s 1ms/step - loss: 1.5003 - val_loss: 1.5291 Epoch 201/500 9/9 [==============================] - 0s 1ms/step - loss: 1.4815 - val_loss: 1.5062 Epoch 202/500 9/9 [==============================] - 0s 2ms/step - loss: 1.4623 - val_loss: 1.4843 Epoch 203/500 9/9 [==============================] - 0s 2ms/step - loss: 1.4435 - val_loss: 1.4639 Epoch 204/500 9/9 [==============================] - 0s 2ms/step - loss: 1.4250 - val_loss: 1.4437 Epoch 205/500 9/9 [==============================] - 0s 2ms/step - loss: 1.4067 - val_loss: 1.4226 Epoch 206/500 9/9 [==============================] - 0s 1ms/step - loss: 1.3884 - val_loss: 1.4033 Epoch 207/500 9/9 [==============================] - 0s 1ms/step - loss: 1.3705 - val_loss: 1.3828 Epoch 208/500 9/9 [==============================] - 0s 2ms/step - loss: 1.3529 - val_loss: 1.3636 Epoch 209/500 9/9 [==============================] - 0s 2ms/step - loss: 1.3351 - val_loss: 1.3454 Epoch 210/500 9/9 [==============================] - 0s 2ms/step - loss: 1.3178 - val_loss: 1.3249 Epoch 211/500 9/9 [==============================] - 0s 2ms/step - loss: 1.3007 - val_loss: 1.3058 Epoch 212/500 9/9 [==============================] - 0s 2ms/step - loss: 1.2838 - val_loss: 1.2861 Epoch 213/500 9/9 [==============================] - 0s 2ms/step - loss: 1.2668 - val_loss: 1.2681 Epoch 214/500 9/9 [==============================] - 0s 2ms/step - loss: 1.2502 - val_loss: 1.2499 Epoch 215/500 9/9 [==============================] - 0s 1ms/step - loss: 1.2336 - val_loss: 1.2314 Epoch 216/500 9/9 [==============================] - 0s 2ms/step - loss: 1.2174 - val_loss: 1.2133 Epoch 217/500 9/9 [==============================] - 0s 1ms/step - loss: 1.2013 - val_loss: 1.1955 Epoch 218/500 9/9 [==============================] - 0s 2ms/step - loss: 1.1853 - val_loss: 1.1776 Epoch 219/500 9/9 [==============================] - 0s 1ms/step - loss: 1.1697 - val_loss: 1.1601 Epoch 220/500 9/9 [==============================] - 0s 1ms/step - loss: 1.1541 - val_loss: 1.1430 Epoch 221/500 9/9 [==============================] - 0s 2ms/step - loss: 1.1388 - val_loss: 1.1252 Epoch 222/500 9/9 [==============================] - 0s 1ms/step - loss: 1.1236 - val_loss: 1.1084 Epoch 223/500 9/9 [==============================] - 0s 2ms/step - loss: 1.1084 - val_loss: 1.0921 Epoch 224/500 9/9 [==============================] - ETA: 0s - loss: 1.099 - 0s 1ms/step - loss: 1.0936 - val_loss: 1.0763 Epoch 225/500 9/9 [==============================] - 0s 2ms/step - loss: 1.0789 - val_loss: 1.0605 Epoch 226/500 9/9 [==============================] - 0s 1ms/step - loss: 1.0643 - val_loss: 1.0438 Epoch 227/500 9/9 [==============================] - 0s 1ms/step - loss: 1.0500 - val_loss: 1.0274 Epoch 228/500 9/9 [==============================] - 0s 1ms/step - loss: 1.0359 - val_loss: 1.0106 Epoch 229/500 9/9 [==============================] - 0s 2ms/step - loss: 1.0217 - val_loss: 0.9951 Epoch 230/500 9/9 [==============================] - 0s 1ms/step - loss: 1.0079 - val_loss: 0.9800 Epoch 231/500 9/9 [==============================] - 0s 1ms/step - loss: 0.9942 - val_loss: 0.9651 Epoch 232/500 9/9 [==============================] - 0s 1ms/step - loss: 0.9807 - val_loss: 0.9507 Epoch 233/500 9/9 [==============================] - 0s 1ms/step - loss: 0.9674 - val_loss: 0.9347 Epoch 234/500 9/9 [==============================] - 0s 1ms/step - loss: 0.9539 - val_loss: 0.9199 Epoch 235/500 9/9 [==============================] - 0s 2ms/step - loss: 0.9408 - val_loss: 0.9063 Epoch 236/500 9/9 [==============================] - 0s 2ms/step - loss: 0.9279 - val_loss: 0.8911 Epoch 237/500 9/9 [==============================] - 0s 2ms/step - loss: 0.9152 - val_loss: 0.8767 Epoch 238/500 9/9 [==============================] - 0s 1ms/step - loss: 0.9026 - val_loss: 0.8635 Epoch 239/500 9/9 [==============================] - 0s 2ms/step - loss: 0.8902 - val_loss: 0.8494 Epoch 240/500 9/9 [==============================] - 0s 2ms/step - loss: 0.8778 - val_loss: 0.8349 Epoch 241/500 9/9 [==============================] - 0s 2ms/step - loss: 0.8655 - val_loss: 0.8223 Epoch 242/500 9/9 [==============================] - 0s 2ms/step - loss: 0.8536 - val_loss: 0.8099 Epoch 243/500 9/9 [==============================] - 0s 2ms/step - loss: 0.8417 - val_loss: 0.7958 Epoch 244/500 9/9 [==============================] - 0s 1ms/step - loss: 0.8298 - val_loss: 0.7832 Epoch 245/500 9/9 [==============================] - 0s 2ms/step - loss: 0.8182 - val_loss: 0.7705 Epoch 246/500 9/9 [==============================] - 0s 1ms/step - loss: 0.8068 - val_loss: 0.7583 Epoch 247/500 9/9 [==============================] - 0s 2ms/step - loss: 0.7955 - val_loss: 0.7460 Epoch 248/500 9/9 [==============================] - 0s 1ms/step - loss: 0.7844 - val_loss: 0.7334 Epoch 249/500 9/9 [==============================] - 0s 1ms/step - loss: 0.7732 - val_loss: 0.7206 Epoch 250/500 9/9 [==============================] - 0s 2ms/step - loss: 0.7623 - val_loss: 0.7089 Epoch 251/500 9/9 [==============================] - 0s 2ms/step - loss: 0.7516 - val_loss: 0.6966 Epoch 252/500 9/9 [==============================] - 0s 1ms/step - loss: 0.7409 - val_loss: 0.6849 Epoch 253/500 9/9 [==============================] - 0s 1ms/step - loss: 0.7304 - val_loss: 0.6738 Epoch 254/500 9/9 [==============================] - 0s 2ms/step - loss: 0.7199 - val_loss: 0.6619 Epoch 255/500 9/9 [==============================] - 0s 1ms/step - loss: 0.7097 - val_loss: 0.6512 Epoch 256/500 9/9 [==============================] - 0s 1ms/step - loss: 0.6997 - val_loss: 0.6401 Epoch 257/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6896 - val_loss: 0.6294 Epoch 258/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6798 - val_loss: 0.6186 Epoch 259/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6700 - val_loss: 0.6080 Epoch 260/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.5974 Epoch 261/500 9/9 [==============================] - 0s 1ms/step - loss: 0.6508 - val_loss: 0.5873 Epoch 262/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6415 - val_loss: 0.5765 Epoch 263/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6322 - val_loss: 0.5665 Epoch 264/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6231 - val_loss: 0.5565 Epoch 265/500 9/9 [==============================] - 0s 1ms/step - loss: 0.6140 - val_loss: 0.5466 Epoch 266/500 9/9 [==============================] - 0s 2ms/step - loss: 0.6052 - val_loss: 0.5370 Epoch 267/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5964 - val_loss: 0.5284 Epoch 268/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5876 - val_loss: 0.5181 Epoch 269/500 9/9 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5088 Epoch 270/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5706 - val_loss: 0.4992 Epoch 271/500 9/9 [==============================] - 0s 2ms/step - loss: 0.5622 - val_loss: 0.4903 Epoch 272/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5541 - val_loss: 0.4816 Epoch 273/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5459 - val_loss: 0.4731 Epoch 274/500 9/9 [==============================] - 0s 2ms/step - loss: 0.5379 - val_loss: 0.4642 Epoch 275/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5300 - val_loss: 0.4553 Epoch 276/500 9/9 [==============================] - 0s 1ms/step - loss: 0.5222 - val_loss: 0.4471 Epoch 277/500 9/9 [==============================] - 0s 2ms/step - loss: 0.5145 - val_loss: 0.4391 Epoch 278/500 9/9 [==============================] - 0s 2ms/step - loss: 0.5069 - val_loss: 0.4313 Epoch 279/500 9/9 [==============================] - 0s 1ms/step - loss: 0.4994 - val_loss: 0.4231 Epoch 280/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4921 - val_loss: 0.4154 Epoch 281/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4847 - val_loss: 0.4079 Epoch 282/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4776 - val_loss: 0.4006 Epoch 283/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4704 - val_loss: 0.3929 Epoch 284/500 9/9 [==============================] - 0s 1ms/step - loss: 0.4634 - val_loss: 0.3851 Epoch 285/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4565 - val_loss: 0.3780 Epoch 286/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4497 - val_loss: 0.3709 Epoch 287/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4430 - val_loss: 0.3632 Epoch 288/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4364 - val_loss: 0.3567 Epoch 289/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4298 - val_loss: 0.3499 Epoch 290/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4234 - val_loss: 0.3432 Epoch 291/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4171 - val_loss: 0.3366 Epoch 292/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4108 - val_loss: 0.3304 Epoch 293/500 9/9 [==============================] - 0s 2ms/step - loss: 0.4047 - val_loss: 0.3237 Epoch 294/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3985 - val_loss: 0.3174 Epoch 295/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3926 - val_loss: 0.3115 Epoch 296/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3867 - val_loss: 0.3055 Epoch 297/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3808 - val_loss: 0.2996 Epoch 298/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3751 - val_loss: 0.2935 Epoch 299/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3694 - val_loss: 0.2875 Epoch 300/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3639 - val_loss: 0.2818 Epoch 301/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3583 - val_loss: 0.2761 Epoch 302/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3530 - val_loss: 0.2708 Epoch 303/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3476 - val_loss: 0.2655 Epoch 304/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3423 - val_loss: 0.2605 Epoch 305/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3372 - val_loss: 0.2553 Epoch 306/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3320 - val_loss: 0.2504 Epoch 307/500 9/9 [==============================] - 0s 2ms/step - loss: 0.3270 - val_loss: 0.2452 Epoch 308/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3221 - val_loss: 0.2401 Epoch 309/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3171 - val_loss: 0.2351 Epoch 310/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3124 - val_loss: 0.2308 Epoch 311/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3076 - val_loss: 0.2260 Epoch 312/500 9/9 [==============================] - 0s 1ms/step - loss: 0.3030 - val_loss: 0.2216 Epoch 313/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2983 - val_loss: 0.2171 Epoch 314/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2938 - val_loss: 0.2126 Epoch 315/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2894 - val_loss: 0.2082 Epoch 316/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2850 - val_loss: 0.2039 Epoch 317/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2806 - val_loss: 0.1997 Epoch 318/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2764 - val_loss: 0.1957 Epoch 319/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2722 - val_loss: 0.1917 Epoch 320/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2681 - val_loss: 0.1879 Epoch 321/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2640 - val_loss: 0.1839 Epoch 322/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2600 - val_loss: 0.1802 Epoch 323/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2560 - val_loss: 0.1765 Epoch 324/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2522 - val_loss: 0.1728 Epoch 325/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2483 - val_loss: 0.1690 Epoch 326/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2446 - val_loss: 0.1656 Epoch 327/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2409 - val_loss: 0.1623 Epoch 328/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2372 - val_loss: 0.1589 Epoch 329/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2337 - val_loss: 0.1556 Epoch 330/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2301 - val_loss: 0.1525 Epoch 331/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2266 - val_loss: 0.1493 Epoch 332/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2232 - val_loss: 0.1462 Epoch 333/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2199 - val_loss: 0.1431 Epoch 334/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2165 - val_loss: 0.1402 Epoch 335/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2133 - val_loss: 0.1375 Epoch 336/500 9/9 [==============================] - 0s 2ms/step - loss: 0.2101 - val_loss: 0.1348 Epoch 337/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2069 - val_loss: 0.1317 Epoch 338/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2038 - val_loss: 0.1289 Epoch 339/500 9/9 [==============================] - 0s 1ms/step - loss: 0.2008 - val_loss: 0.1262 Epoch 340/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1978 - val_loss: 0.1238 Epoch 341/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1948 - val_loss: 0.1213 Epoch 342/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1919 - val_loss: 0.1188 Epoch 343/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1891 - val_loss: 0.1166 Epoch 344/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1863 - val_loss: 0.1142 Epoch 345/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1835 - val_loss: 0.1118 Epoch 346/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1808 - val_loss: 0.1095 Epoch 347/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1781 - val_loss: 0.1073 Epoch 348/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1755 - val_loss: 0.1052 Epoch 349/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1729 - val_loss: 0.1031 Epoch 350/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1704 - val_loss: 0.1009 Epoch 351/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1679 - val_loss: 0.0989 Epoch 352/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1654 - val_loss: 0.0971 Epoch 353/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1630 - val_loss: 0.0952 Epoch 354/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1606 - val_loss: 0.0934 Epoch 355/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1583 - val_loss: 0.0914 Epoch 356/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1560 - val_loss: 0.0896 Epoch 357/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1537 - val_loss: 0.0880 Epoch 358/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1515 - val_loss: 0.0863 Epoch 359/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1494 - val_loss: 0.0845 Epoch 360/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1472 - val_loss: 0.0830 Epoch 361/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1451 - val_loss: 0.0815 Epoch 362/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1430 - val_loss: 0.0799 Epoch 363/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1410 - val_loss: 0.0784 Epoch 364/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1390 - val_loss: 0.0770 Epoch 365/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1370 - val_loss: 0.0755 Epoch 366/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1351 - val_loss: 0.0741 Epoch 367/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1333 - val_loss: 0.0728 Epoch 368/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1314 - val_loss: 0.0715 Epoch 369/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1295 - val_loss: 0.0704 Epoch 370/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1278 - val_loss: 0.0691 Epoch 371/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1260 - val_loss: 0.0679 Epoch 372/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1242 - val_loss: 0.0667 Epoch 373/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1225 - val_loss: 0.0655 Epoch 374/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1209 - val_loss: 0.0645 Epoch 375/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1193 - val_loss: 0.0635 Epoch 376/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1176 - val_loss: 0.0624 Epoch 377/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1161 - val_loss: 0.0614 Epoch 378/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1145 - val_loss: 0.0603 Epoch 379/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.0594 Epoch 380/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1115 - val_loss: 0.0584 Epoch 381/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1100 - val_loss: 0.0575 Epoch 382/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1086 - val_loss: 0.0567 Epoch 383/500 9/9 [==============================] - 0s 1ms/step - loss: 0.1071 - val_loss: 0.0559 Epoch 384/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1058 - val_loss: 0.0551 Epoch 385/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1044 - val_loss: 0.0543 Epoch 386/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1030 - val_loss: 0.0536 Epoch 387/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1017 - val_loss: 0.0528 Epoch 388/500 9/9 [==============================] - 0s 2ms/step - loss: 0.1004 - val_loss: 0.0520 Epoch 389/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0992 - val_loss: 0.0513 Epoch 390/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0979 - val_loss: 0.0507 Epoch 391/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0967 - val_loss: 0.0500 Epoch 392/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0955 - val_loss: 0.0495 Epoch 393/500 9/9 [==============================] - 0s 4ms/step - loss: 0.0943 - val_loss: 0.0489 Epoch 394/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0931 - val_loss: 0.0482 Epoch 395/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0920 - val_loss: 0.0477 Epoch 396/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0909 - val_loss: 0.0471 Epoch 397/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0898 - val_loss: 0.0465 Epoch 398/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0887 - val_loss: 0.0461 Epoch 399/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0877 - val_loss: 0.0456 Epoch 400/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0866 - val_loss: 0.0451 Epoch 401/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0856 - val_loss: 0.0446 Epoch 402/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0846 - val_loss: 0.0442 Epoch 403/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0836 - val_loss: 0.0437 Epoch 404/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0827 - val_loss: 0.0433 Epoch 405/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 0.0430 Epoch 406/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0808 - val_loss: 0.0425 Epoch 407/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0799 - val_loss: 0.0422 Epoch 408/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0790 - val_loss: 0.0418 Epoch 409/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0782 - val_loss: 0.0415 Epoch 410/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0773 - val_loss: 0.0411 Epoch 411/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0765 - val_loss: 0.0408 Epoch 412/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0756 - val_loss: 0.0405 Epoch 413/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0749 - val_loss: 0.0403 Epoch 414/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0740 - val_loss: 0.0400 Epoch 415/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0733 - val_loss: 0.0397 Epoch 416/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0725 - val_loss: 0.0394 Epoch 417/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0718 - val_loss: 0.0392 Epoch 418/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0710 - val_loss: 0.0389 Epoch 419/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0703 - val_loss: 0.0387 Epoch 420/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0696 - val_loss: 0.0385 Epoch 421/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0689 - val_loss: 0.0383 Epoch 422/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0682 - val_loss: 0.0381 Epoch 423/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0676 - val_loss: 0.0379 Epoch 424/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0669 - val_loss: 0.0377 Epoch 425/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0663 - val_loss: 0.0376 Epoch 426/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0657 - val_loss: 0.0374 Epoch 427/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0650 - val_loss: 0.0372 Epoch 428/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0644 - val_loss: 0.0371 Epoch 429/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0638 - val_loss: 0.0370 Epoch 430/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0633 - val_loss: 0.0368 Epoch 431/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0627 - val_loss: 0.0367 Epoch 432/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0621 - val_loss: 0.0366 Epoch 433/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0616 - val_loss: 0.0365 Epoch 434/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0611 - val_loss: 0.0364 Epoch 435/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0605 - val_loss: 0.0363 Epoch 436/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0600 - val_loss: 0.0362 Epoch 437/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0595 - val_loss: 0.0361 Epoch 438/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0590 - val_loss: 0.0360 Epoch 439/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0585 - val_loss: 0.0359 Epoch 440/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0580 - val_loss: 0.0358 Epoch 441/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0576 - val_loss: 0.0357 Epoch 442/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0571 - val_loss: 0.0357 Epoch 443/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0567 - val_loss: 0.0356 Epoch 444/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0562 - val_loss: 0.0355 Epoch 445/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0558 - val_loss: 0.0355 Epoch 446/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0554 - val_loss: 0.0354 Epoch 447/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0550 - val_loss: 0.0354 Epoch 448/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0546 - val_loss: 0.0353 Epoch 449/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0542 - val_loss: 0.0353 Epoch 450/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0538 - val_loss: 0.0353 Epoch 451/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0534 - val_loss: 0.0352 Epoch 452/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0530 - val_loss: 0.0352 Epoch 453/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0527 - val_loss: 0.0352 Epoch 454/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0523 - val_loss: 0.0351 Epoch 455/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0519 - val_loss: 0.0351 Epoch 456/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0516 - val_loss: 0.0351 Epoch 457/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0513 - val_loss: 0.0351 Epoch 458/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0509 - val_loss: 0.0350 Epoch 459/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0506 - val_loss: 0.0350 Epoch 460/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0503 - val_loss: 0.0350 Epoch 461/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0500 - val_loss: 0.0350 Epoch 462/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0497 - val_loss: 0.0350 Epoch 463/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0494 - val_loss: 0.0350 Epoch 464/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0491 - val_loss: 0.0350 Epoch 465/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0488 - val_loss: 0.0350 Epoch 466/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0485 - val_loss: 0.0350 Epoch 467/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0482 - val_loss: 0.0350 Epoch 468/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0479 - val_loss: 0.0350 Epoch 469/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0477 - val_loss: 0.0350 Epoch 470/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0474 - val_loss: 0.0350 Epoch 471/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0472 - val_loss: 0.0350 Epoch 472/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0469 - val_loss: 0.0350 Epoch 473/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0467 - val_loss: 0.0350 Epoch 474/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0464 - val_loss: 0.0350 Epoch 475/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0462 - val_loss: 0.0350 Epoch 476/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0460 - val_loss: 0.0350 Epoch 477/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0457 - val_loss: 0.0350 Epoch 478/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0455 - val_loss: 0.0350 Epoch 479/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0453 - val_loss: 0.0350 Epoch 480/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0451 - val_loss: 0.0350 Epoch 481/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0449 - val_loss: 0.0350 Epoch 482/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0447 - val_loss: 0.0350 Epoch 483/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0445 - val_loss: 0.0350 Epoch 484/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0443 - val_loss: 0.0350 Epoch 485/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0441 - val_loss: 0.0350 Epoch 486/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0439 - val_loss: 0.0350 Epoch 487/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0437 - val_loss: 0.0350 Epoch 488/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0436 - val_loss: 0.0350 Epoch 489/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0434 - val_loss: 0.0351 Epoch 490/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0432 - val_loss: 0.0351 Epoch 491/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0430 - val_loss: 0.0351 Epoch 492/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0429 - val_loss: 0.0351 Epoch 493/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0427 - val_loss: 0.0351 Epoch 494/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0425 - val_loss: 0.0351 Epoch 495/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0424 - val_loss: 0.0351 Epoch 496/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0422 - val_loss: 0.0351 Epoch 497/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0421 - val_loss: 0.0351 Epoch 498/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0419 - val_loss: 0.0351 Epoch 499/500 9/9 [==============================] - 0s 2ms/step - loss: 0.0418 - val_loss: 0.0351 Epoch 500/500 9/9 [==============================] - 0s 1ms/step - loss: 0.0417 - val_loss: 0.0351 . &lt;keras.callbacks.History at 0x7f70988babc0&gt; . . #%tensorboard --logdir logs --host 0.0.0.0 . - 끼워넣을 오브젝트 만들기 . fig, ax = plt.subplots() ax.plot(y,&#39;.&#39;,alpha=0.2) ax.plot(net(X),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f6f5823bdf0&gt;] . fig . - 이제 fig 오브젝트를 끼워넣을 코드를 구성하자. (공식홈페이지 참고) . https://www.tensorflow.org/tensorboard/image_summaries | . #from datetime import datetime import io logdir = &quot;logs&quot; #logdir = &quot;logs&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;) def plot_to_image(fig): # 사용자가 지정한 그림오브젝트 fig를 넣으면 텐서보드에 끼워넣을수 있는 형태로 출력해주는 함수 &quot;&quot;&quot;Converts the matplotlib plot specified by &#39;figure&#39; to a PNG image and returns it. The supplied figure is closed and inaccessible after this call.&quot;&quot;&quot; # Save the plot to a PNG in memory. buf = io.BytesIO() fig.savefig(buf, format=&#39;png&#39;) # Closing the figure prevents it from being displayed directly inside # the notebook. plt.close(fig) buf.seek(0) # Convert PNG buffer to TF image image = tf.image.decode_png(buf.getvalue(), channels=4) # Add the batch dimension image = tf.expand_dims(image, 0) return image . with tf.summary.create_file_writer(logdir).as_default(): tf.summary.image(&quot;적합결과시각화&quot;, plot_to_image(fig), step=0) . #%tensorboard --logdir logs --host 0.0.0.0 . &#54617;&#49845;&#44284;&#51221;&#48516;&#49437; . &#53584;&#49436;&#48372;&#46300;: &#44032;&#51473;&#52824; &#49884;&#44033;&#54868; . - 에폭별로 가중치가 수렴하는 모양을 보고 싶다. . 3-(1) 아래와 같은 모형을 고려하자. . $$y_i= beta_0 + sum_{k=1}^{5} beta_k cos(k t_i)+ epsilon_i$$ . 여기에서 $t=(t_1, dots,t_{1000})=$ np.linspace(0,5,1000) 이다. 그리고 $ epsilon_i sim i.i.d~ N(0, sigma^2)$, 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자. . np.random.seed(43052) t= np.linspace(0,5,1000) y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2 plt.plot(t,y,&#39;.&#39;,alpha=0.1) . [&lt;matplotlib.lines.Line2D at 0x7f6f5814c8e0&gt;] . - 학습을 진행하면서 가중치가 어떻게 업데이트 되는지 시각화하자. . . !rm -rf logs net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) cb1= tf.keras.callbacks.TensorBoard(update_freq=&#39;epoch&#39;,histogram_freq=100) net.fit(X,y,epochs=2000, batch_size=100, validation_split=0.45,callbacks=cb1) . Epoch 1/2000 6/6 [==============================] - 0s 10ms/step - loss: 10.5761 - val_loss: 13.6319 Epoch 2/2000 6/6 [==============================] - 0s 3ms/step - loss: 10.5050 - val_loss: 13.5579 Epoch 3/2000 6/6 [==============================] - 0s 3ms/step - loss: 10.4352 - val_loss: 13.4839 Epoch 4/2000 6/6 [==============================] - 0s 3ms/step - loss: 10.3643 - val_loss: 13.4108 Epoch 5/2000 6/6 [==============================] - 0s 3ms/step - loss: 10.2954 - val_loss: 13.3387 Epoch 6/2000 6/6 [==============================] - 0s 4ms/step - loss: 10.2266 - val_loss: 13.2667 Epoch 7/2000 6/6 [==============================] - 0s 3ms/step - loss: 10.1574 - val_loss: 13.1948 Epoch 8/2000 6/6 [==============================] - 0s 3ms/step - loss: 10.0893 - val_loss: 13.1239 Epoch 9/2000 6/6 [==============================] - 0s 3ms/step - loss: 10.0220 - val_loss: 13.0539 Epoch 10/2000 6/6 [==============================] - 0s 3ms/step - loss: 9.9545 - val_loss: 12.9839 Epoch 11/2000 6/6 [==============================] - 0s 3ms/step - loss: 9.8885 - val_loss: 12.9143 Epoch 12/2000 6/6 [==============================] - 0s 3ms/step - loss: 9.8218 - val_loss: 12.8446 Epoch 13/2000 6/6 [==============================] - 0s 3ms/step - loss: 9.7546 - val_loss: 12.7763 Epoch 14/2000 6/6 [==============================] - 0s 3ms/step - loss: 9.6900 - val_loss: 12.7072 Epoch 15/2000 6/6 [==============================] - 0s 4ms/step - loss: 9.6251 - val_loss: 12.6390 Epoch 16/2000 6/6 [==============================] - 0s 4ms/step - loss: 9.5592 - val_loss: 12.5710 Epoch 17/2000 6/6 [==============================] - 0s 3ms/step - loss: 9.4956 - val_loss: 12.5030 Epoch 18/2000 6/6 [==============================] - 0s 3ms/step - loss: 9.4315 - val_loss: 12.4345 Epoch 19/2000 6/6 [==============================] - 0s 3ms/step - loss: 9.3670 - val_loss: 12.3678 Epoch 20/2000 6/6 [==============================] - 0s 3ms/step - loss: 9.3029 - val_loss: 12.3018 Epoch 21/2000 6/6 [==============================] - 0s 3ms/step - loss: 9.2407 - val_loss: 12.2358 Epoch 22/2000 6/6 [==============================] - 0s 3ms/step - loss: 9.1777 - val_loss: 12.1697 Epoch 23/2000 6/6 [==============================] - 0s 3ms/step - loss: 9.1158 - val_loss: 12.1035 Epoch 24/2000 6/6 [==============================] - 0s 3ms/step - loss: 9.0525 - val_loss: 12.0391 Epoch 25/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.9924 - val_loss: 11.9733 Epoch 26/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.9292 - val_loss: 11.9101 Epoch 27/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.8694 - val_loss: 11.8461 Epoch 28/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.8079 - val_loss: 11.7831 Epoch 29/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.7476 - val_loss: 11.7193 Epoch 30/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.6889 - val_loss: 11.6557 Epoch 31/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.6272 - val_loss: 11.5937 Epoch 32/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.5683 - val_loss: 11.5312 Epoch 33/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.5101 - val_loss: 11.4687 Epoch 34/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.4503 - val_loss: 11.4082 Epoch 35/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.3924 - val_loss: 11.3462 Epoch 36/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.3351 - val_loss: 11.2848 Epoch 37/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.2768 - val_loss: 11.2241 Epoch 38/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.2203 - val_loss: 11.1644 Epoch 39/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.1620 - val_loss: 11.1063 Epoch 40/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.1069 - val_loss: 11.0478 Epoch 41/2000 6/6 [==============================] - 0s 3ms/step - loss: 8.0510 - val_loss: 10.9894 Epoch 42/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.9946 - val_loss: 10.9322 Epoch 43/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.9396 - val_loss: 10.8745 Epoch 44/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.8851 - val_loss: 10.8172 Epoch 45/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.8309 - val_loss: 10.7601 Epoch 46/2000 6/6 [==============================] - 0s 4ms/step - loss: 7.7758 - val_loss: 10.7029 Epoch 47/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.7227 - val_loss: 10.6449 Epoch 48/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.6674 - val_loss: 10.5891 Epoch 49/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.6149 - val_loss: 10.5327 Epoch 50/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.5614 - val_loss: 10.4775 Epoch 51/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.5091 - val_loss: 10.4221 Epoch 52/2000 6/6 [==============================] - 0s 4ms/step - loss: 7.4576 - val_loss: 10.3669 Epoch 53/2000 6/6 [==============================] - 0s 4ms/step - loss: 7.4050 - val_loss: 10.3126 Epoch 54/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.3522 - val_loss: 10.2590 Epoch 55/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.3023 - val_loss: 10.2050 Epoch 56/2000 6/6 [==============================] - 0s 4ms/step - loss: 7.2503 - val_loss: 10.1516 Epoch 57/2000 6/6 [==============================] - 0s 4ms/step - loss: 7.2003 - val_loss: 10.0971 Epoch 58/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.1490 - val_loss: 10.0436 Epoch 59/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.0988 - val_loss: 9.9912 Epoch 60/2000 6/6 [==============================] - 0s 3ms/step - loss: 7.0486 - val_loss: 9.9394 Epoch 61/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.9988 - val_loss: 9.8870 Epoch 62/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.9508 - val_loss: 9.8347 Epoch 63/2000 6/6 [==============================] - 0s 4ms/step - loss: 6.9000 - val_loss: 9.7832 Epoch 64/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.8507 - val_loss: 9.7309 Epoch 65/2000 6/6 [==============================] - 0s 4ms/step - loss: 6.8023 - val_loss: 9.6794 Epoch 66/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.7536 - val_loss: 9.6278 Epoch 67/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.7058 - val_loss: 9.5765 Epoch 68/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.6573 - val_loss: 9.5260 Epoch 69/2000 6/6 [==============================] - 0s 4ms/step - loss: 6.6098 - val_loss: 9.4752 Epoch 70/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.5625 - val_loss: 9.4245 Epoch 71/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.5147 - val_loss: 9.3755 Epoch 72/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.4692 - val_loss: 9.3261 Epoch 73/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.4218 - val_loss: 9.2776 Epoch 74/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.3757 - val_loss: 9.2286 Epoch 75/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.3297 - val_loss: 9.1803 Epoch 76/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.2845 - val_loss: 9.1314 Epoch 77/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.2383 - val_loss: 9.0835 Epoch 78/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.1934 - val_loss: 9.0348 Epoch 79/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.1484 - val_loss: 8.9875 Epoch 80/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.1032 - val_loss: 8.9405 Epoch 81/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.0604 - val_loss: 8.8922 Epoch 82/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.0153 - val_loss: 8.8456 Epoch 83/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.9708 - val_loss: 8.7999 Epoch 84/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.9286 - val_loss: 8.7539 Epoch 85/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.8848 - val_loss: 8.7093 Epoch 86/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.8427 - val_loss: 8.6642 Epoch 87/2000 6/6 [==============================] - 0s 4ms/step - loss: 5.8006 - val_loss: 8.6189 Epoch 88/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.7591 - val_loss: 8.5736 Epoch 89/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.7162 - val_loss: 8.5291 Epoch 90/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.6751 - val_loss: 8.4846 Epoch 91/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.6328 - val_loss: 8.4410 Epoch 92/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.5924 - val_loss: 8.3969 Epoch 93/2000 6/6 [==============================] - 0s 4ms/step - loss: 5.5510 - val_loss: 8.3528 Epoch 94/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.5104 - val_loss: 8.3092 Epoch 95/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.4697 - val_loss: 8.2662 Epoch 96/2000 6/6 [==============================] - 0s 4ms/step - loss: 5.4291 - val_loss: 8.2228 Epoch 97/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.3892 - val_loss: 8.1801 Epoch 98/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.3491 - val_loss: 8.1378 Epoch 99/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.3104 - val_loss: 8.0953 Epoch 100/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.2704 - val_loss: 8.0537 Epoch 101/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.2316 - val_loss: 8.0121 Epoch 102/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.1927 - val_loss: 7.9702 Epoch 103/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.1547 - val_loss: 7.9291 Epoch 104/2000 6/6 [==============================] - 0s 4ms/step - loss: 5.1153 - val_loss: 7.8887 Epoch 105/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.0776 - val_loss: 7.8474 Epoch 106/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.0398 - val_loss: 7.8072 Epoch 107/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.0018 - val_loss: 7.7670 Epoch 108/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.9650 - val_loss: 7.7268 Epoch 109/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.9272 - val_loss: 7.6866 Epoch 110/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.8898 - val_loss: 7.6479 Epoch 111/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.8535 - val_loss: 7.6090 Epoch 112/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.8171 - val_loss: 7.5696 Epoch 113/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.7808 - val_loss: 7.5310 Epoch 114/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.7459 - val_loss: 7.4919 Epoch 115/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.7091 - val_loss: 7.4538 Epoch 116/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.6743 - val_loss: 7.4156 Epoch 117/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.6391 - val_loss: 7.3770 Epoch 118/2000 6/6 [==============================] - 0s 4ms/step - loss: 4.6037 - val_loss: 7.3395 Epoch 119/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.5691 - val_loss: 7.3022 Epoch 120/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.5346 - val_loss: 7.2650 Epoch 121/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.5012 - val_loss: 7.2272 Epoch 122/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.4662 - val_loss: 7.1898 Epoch 123/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.4323 - val_loss: 7.1527 Epoch 124/2000 6/6 [==============================] - 0s 4ms/step - loss: 4.3984 - val_loss: 7.1157 Epoch 125/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.3651 - val_loss: 7.0795 Epoch 126/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.3322 - val_loss: 7.0431 Epoch 127/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.2990 - val_loss: 7.0075 Epoch 128/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.2665 - val_loss: 6.9722 Epoch 129/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.2339 - val_loss: 6.9372 Epoch 130/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.2018 - val_loss: 6.9018 Epoch 131/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.1699 - val_loss: 6.8666 Epoch 132/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.1382 - val_loss: 6.8318 Epoch 133/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.1064 - val_loss: 6.7976 Epoch 134/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.0750 - val_loss: 6.7637 Epoch 135/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.0437 - val_loss: 6.7300 Epoch 136/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.0137 - val_loss: 6.6962 Epoch 137/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.9831 - val_loss: 6.6622 Epoch 138/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.9520 - val_loss: 6.6287 Epoch 139/2000 6/6 [==============================] - 0s 4ms/step - loss: 3.9218 - val_loss: 6.5953 Epoch 140/2000 6/6 [==============================] - 0s 4ms/step - loss: 3.8921 - val_loss: 6.5620 Epoch 141/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.8617 - val_loss: 6.5292 Epoch 142/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.8319 - val_loss: 6.4965 Epoch 143/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.8020 - val_loss: 6.4639 Epoch 144/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.7732 - val_loss: 6.4315 Epoch 145/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.7435 - val_loss: 6.3991 Epoch 146/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.7143 - val_loss: 6.3669 Epoch 147/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.6857 - val_loss: 6.3345 Epoch 148/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.6562 - val_loss: 6.3026 Epoch 149/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.6286 - val_loss: 6.2697 Epoch 150/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.5990 - val_loss: 6.2380 Epoch 151/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.5708 - val_loss: 6.2063 Epoch 152/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.5426 - val_loss: 6.1751 Epoch 153/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.5147 - val_loss: 6.1438 Epoch 154/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.4869 - val_loss: 6.1125 Epoch 155/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.4598 - val_loss: 6.0812 Epoch 156/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.4324 - val_loss: 6.0509 Epoch 157/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.4057 - val_loss: 6.0204 Epoch 158/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.3787 - val_loss: 5.9906 Epoch 159/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.3523 - val_loss: 5.9610 Epoch 160/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.3257 - val_loss: 5.9312 Epoch 161/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.2997 - val_loss: 5.9018 Epoch 162/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.2740 - val_loss: 5.8723 Epoch 163/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.2478 - val_loss: 5.8435 Epoch 164/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.2222 - val_loss: 5.8143 Epoch 165/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.1972 - val_loss: 5.7853 Epoch 166/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.1719 - val_loss: 5.7565 Epoch 167/2000 6/6 [==============================] - 0s 4ms/step - loss: 3.1462 - val_loss: 5.7283 Epoch 168/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.1213 - val_loss: 5.7000 Epoch 169/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.0966 - val_loss: 5.6719 Epoch 170/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.0715 - val_loss: 5.6443 Epoch 171/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.0475 - val_loss: 5.6166 Epoch 172/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.0236 - val_loss: 5.5888 Epoch 173/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.9992 - val_loss: 5.5612 Epoch 174/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.9749 - val_loss: 5.5341 Epoch 175/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.9514 - val_loss: 5.5070 Epoch 176/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.9280 - val_loss: 5.4798 Epoch 177/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.9046 - val_loss: 5.4530 Epoch 178/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.8809 - val_loss: 5.4262 Epoch 179/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.8579 - val_loss: 5.3988 Epoch 180/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.8347 - val_loss: 5.3721 Epoch 181/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.8117 - val_loss: 5.3455 Epoch 182/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7892 - val_loss: 5.3190 Epoch 183/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7668 - val_loss: 5.2926 Epoch 184/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7439 - val_loss: 5.2664 Epoch 185/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7219 - val_loss: 5.2400 Epoch 186/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6998 - val_loss: 5.2143 Epoch 187/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6775 - val_loss: 5.1886 Epoch 188/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6560 - val_loss: 5.1632 Epoch 189/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6344 - val_loss: 5.1375 Epoch 190/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6128 - val_loss: 5.1125 Epoch 191/2000 6/6 [==============================] - 0s 2ms/step - loss: 2.5919 - val_loss: 5.0868 Epoch 192/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5703 - val_loss: 5.0617 Epoch 193/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5491 - val_loss: 5.0373 Epoch 194/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5285 - val_loss: 5.0125 Epoch 195/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5071 - val_loss: 4.9879 Epoch 196/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4868 - val_loss: 4.9628 Epoch 197/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4659 - val_loss: 4.9382 Epoch 198/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.4455 - val_loss: 4.9135 Epoch 199/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.4248 - val_loss: 4.8893 Epoch 200/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4049 - val_loss: 4.8653 Epoch 201/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3847 - val_loss: 4.8420 Epoch 202/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3649 - val_loss: 4.8186 Epoch 203/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3451 - val_loss: 4.7950 Epoch 204/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3261 - val_loss: 4.7714 Epoch 205/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3060 - val_loss: 4.7487 Epoch 206/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2871 - val_loss: 4.7257 Epoch 207/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.2681 - val_loss: 4.7027 Epoch 208/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2489 - val_loss: 4.6803 Epoch 209/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2303 - val_loss: 4.6579 Epoch 210/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2115 - val_loss: 4.6353 Epoch 211/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1931 - val_loss: 4.6133 Epoch 212/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1750 - val_loss: 4.5909 Epoch 213/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1568 - val_loss: 4.5686 Epoch 214/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1387 - val_loss: 4.5462 Epoch 215/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1208 - val_loss: 4.5243 Epoch 216/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1031 - val_loss: 4.5023 Epoch 217/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0855 - val_loss: 4.4811 Epoch 218/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0677 - val_loss: 4.4596 Epoch 219/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0503 - val_loss: 4.4384 Epoch 220/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0331 - val_loss: 4.4170 Epoch 221/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0158 - val_loss: 4.3960 Epoch 222/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9988 - val_loss: 4.3747 Epoch 223/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9816 - val_loss: 4.3535 Epoch 224/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9646 - val_loss: 4.3322 Epoch 225/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9478 - val_loss: 4.3112 Epoch 226/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9309 - val_loss: 4.2906 Epoch 227/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9147 - val_loss: 4.2702 Epoch 228/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8978 - val_loss: 4.2495 Epoch 229/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8817 - val_loss: 4.2287 Epoch 230/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8654 - val_loss: 4.2084 Epoch 231/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8493 - val_loss: 4.1881 Epoch 232/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.8336 - val_loss: 4.1677 Epoch 233/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8177 - val_loss: 4.1479 Epoch 234/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8023 - val_loss: 4.1278 Epoch 235/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7865 - val_loss: 4.1084 Epoch 236/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7710 - val_loss: 4.0889 Epoch 237/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7556 - val_loss: 4.0694 Epoch 238/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.7405 - val_loss: 4.0502 Epoch 239/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.7254 - val_loss: 4.0309 Epoch 240/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7103 - val_loss: 4.0120 Epoch 241/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6959 - val_loss: 3.9928 Epoch 242/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6808 - val_loss: 3.9739 Epoch 243/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6661 - val_loss: 3.9554 Epoch 244/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6518 - val_loss: 3.9367 Epoch 245/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.6372 - val_loss: 3.9182 Epoch 246/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.6232 - val_loss: 3.8994 Epoch 247/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6087 - val_loss: 3.8812 Epoch 248/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5946 - val_loss: 3.8630 Epoch 249/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5808 - val_loss: 3.8450 Epoch 250/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5669 - val_loss: 3.8269 Epoch 251/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5532 - val_loss: 3.8090 Epoch 252/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5396 - val_loss: 3.7908 Epoch 253/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5259 - val_loss: 3.7723 Epoch 254/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5123 - val_loss: 3.7543 Epoch 255/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4990 - val_loss: 3.7366 Epoch 256/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4855 - val_loss: 3.7190 Epoch 257/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4724 - val_loss: 3.7013 Epoch 258/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4592 - val_loss: 3.6839 Epoch 259/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4462 - val_loss: 3.6666 Epoch 260/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4335 - val_loss: 3.6491 Epoch 261/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4207 - val_loss: 3.6321 Epoch 262/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4078 - val_loss: 3.6151 Epoch 263/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3955 - val_loss: 3.5979 Epoch 264/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3828 - val_loss: 3.5807 Epoch 265/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3705 - val_loss: 3.5637 Epoch 266/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3581 - val_loss: 3.5469 Epoch 267/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3462 - val_loss: 3.5302 Epoch 268/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3340 - val_loss: 3.5136 Epoch 269/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.3218 - val_loss: 3.4973 Epoch 270/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3100 - val_loss: 3.4807 Epoch 271/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2982 - val_loss: 3.4645 Epoch 272/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2865 - val_loss: 3.4483 Epoch 273/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.2750 - val_loss: 3.4321 Epoch 274/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2634 - val_loss: 3.4163 Epoch 275/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2521 - val_loss: 3.4002 Epoch 276/2000 6/6 [==============================] - 0s 2ms/step - loss: 1.2408 - val_loss: 3.3844 Epoch 277/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2293 - val_loss: 3.3688 Epoch 278/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2182 - val_loss: 3.3532 Epoch 279/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2075 - val_loss: 3.3375 Epoch 280/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1963 - val_loss: 3.3221 Epoch 281/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1854 - val_loss: 3.3067 Epoch 282/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1747 - val_loss: 3.2914 Epoch 283/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1640 - val_loss: 3.2761 Epoch 284/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1534 - val_loss: 3.2609 Epoch 285/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1429 - val_loss: 3.2457 Epoch 286/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1323 - val_loss: 3.2309 Epoch 287/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1218 - val_loss: 3.2161 Epoch 288/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1116 - val_loss: 3.2014 Epoch 289/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1014 - val_loss: 3.1869 Epoch 290/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.0913 - val_loss: 3.1723 Epoch 291/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0815 - val_loss: 3.1577 Epoch 292/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0712 - val_loss: 3.1432 Epoch 293/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0614 - val_loss: 3.1287 Epoch 294/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0515 - val_loss: 3.1145 Epoch 295/2000 6/6 [==============================] - 0s 2ms/step - loss: 1.0418 - val_loss: 3.1001 Epoch 296/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0321 - val_loss: 3.0862 Epoch 297/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0224 - val_loss: 3.0722 Epoch 298/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.0130 - val_loss: 3.0578 Epoch 299/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0034 - val_loss: 3.0437 Epoch 300/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9940 - val_loss: 3.0300 Epoch 301/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.9848 - val_loss: 3.0160 Epoch 302/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.9756 - val_loss: 3.0022 Epoch 303/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9666 - val_loss: 2.9887 Epoch 304/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9575 - val_loss: 2.9754 Epoch 305/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.9486 - val_loss: 2.9620 Epoch 306/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9398 - val_loss: 2.9483 Epoch 307/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.9309 - val_loss: 2.9350 Epoch 308/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9222 - val_loss: 2.9220 Epoch 309/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9136 - val_loss: 2.9090 Epoch 310/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9052 - val_loss: 2.8960 Epoch 311/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8967 - val_loss: 2.8830 Epoch 312/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8884 - val_loss: 2.8703 Epoch 313/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8800 - val_loss: 2.8577 Epoch 314/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8718 - val_loss: 2.8449 Epoch 315/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8636 - val_loss: 2.8324 Epoch 316/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8555 - val_loss: 2.8196 Epoch 317/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8475 - val_loss: 2.8069 Epoch 318/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8396 - val_loss: 2.7941 Epoch 319/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.8316 - val_loss: 2.7815 Epoch 320/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8237 - val_loss: 2.7691 Epoch 321/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8159 - val_loss: 2.7563 Epoch 322/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8081 - val_loss: 2.7440 Epoch 323/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8005 - val_loss: 2.7317 Epoch 324/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7930 - val_loss: 2.7191 Epoch 325/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7853 - val_loss: 2.7069 Epoch 326/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7778 - val_loss: 2.6946 Epoch 327/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7704 - val_loss: 2.6826 Epoch 328/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.7630 - val_loss: 2.6707 Epoch 329/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7557 - val_loss: 2.6586 Epoch 330/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7484 - val_loss: 2.6466 Epoch 331/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7412 - val_loss: 2.6347 Epoch 332/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7342 - val_loss: 2.6225 Epoch 333/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7270 - val_loss: 2.6108 Epoch 334/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7201 - val_loss: 2.5992 Epoch 335/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7131 - val_loss: 2.5877 Epoch 336/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7063 - val_loss: 2.5761 Epoch 337/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6995 - val_loss: 2.5646 Epoch 338/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 2.5533 Epoch 339/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6861 - val_loss: 2.5420 Epoch 340/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6795 - val_loss: 2.5309 Epoch 341/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6730 - val_loss: 2.5197 Epoch 342/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6665 - val_loss: 2.5087 Epoch 343/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6600 - val_loss: 2.4978 Epoch 344/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6537 - val_loss: 2.4868 Epoch 345/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6473 - val_loss: 2.4760 Epoch 346/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.6411 - val_loss: 2.4649 Epoch 347/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6348 - val_loss: 2.4543 Epoch 348/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6285 - val_loss: 2.4438 Epoch 349/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6225 - val_loss: 2.4331 Epoch 350/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6164 - val_loss: 2.4223 Epoch 351/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6103 - val_loss: 2.4117 Epoch 352/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6044 - val_loss: 2.4009 Epoch 353/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5984 - val_loss: 2.3903 Epoch 354/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5925 - val_loss: 2.3800 Epoch 355/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5867 - val_loss: 2.3697 Epoch 356/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5812 - val_loss: 2.3593 Epoch 357/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5754 - val_loss: 2.3490 Epoch 358/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5698 - val_loss: 2.3388 Epoch 359/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5642 - val_loss: 2.3287 Epoch 360/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5587 - val_loss: 2.3189 Epoch 361/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5533 - val_loss: 2.3090 Epoch 362/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.5478 - val_loss: 2.2990 Epoch 363/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5424 - val_loss: 2.2892 Epoch 364/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5372 - val_loss: 2.2795 Epoch 365/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5320 - val_loss: 2.2696 Epoch 366/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5268 - val_loss: 2.2600 Epoch 367/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5215 - val_loss: 2.2505 Epoch 368/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5165 - val_loss: 2.2408 Epoch 369/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5113 - val_loss: 2.2315 Epoch 370/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.5064 - val_loss: 2.2219 Epoch 371/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5013 - val_loss: 2.2122 Epoch 372/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4964 - val_loss: 2.2026 Epoch 373/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4914 - val_loss: 2.1931 Epoch 374/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4866 - val_loss: 2.1835 Epoch 375/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4818 - val_loss: 2.1744 Epoch 376/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4772 - val_loss: 2.1650 Epoch 377/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4723 - val_loss: 2.1559 Epoch 378/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4677 - val_loss: 2.1469 Epoch 379/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4631 - val_loss: 2.1378 Epoch 380/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4585 - val_loss: 2.1288 Epoch 381/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4540 - val_loss: 2.1199 Epoch 382/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4496 - val_loss: 2.1110 Epoch 383/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4452 - val_loss: 2.1021 Epoch 384/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4408 - val_loss: 2.0934 Epoch 385/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4365 - val_loss: 2.0848 Epoch 386/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4322 - val_loss: 2.0760 Epoch 387/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4278 - val_loss: 2.0677 Epoch 388/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4237 - val_loss: 2.0591 Epoch 389/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4196 - val_loss: 2.0505 Epoch 390/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4154 - val_loss: 2.0419 Epoch 391/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4113 - val_loss: 2.0335 Epoch 392/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4072 - val_loss: 2.0253 Epoch 393/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4031 - val_loss: 2.0169 Epoch 394/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3992 - val_loss: 2.0086 Epoch 395/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3952 - val_loss: 2.0003 Epoch 396/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3913 - val_loss: 1.9921 Epoch 397/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3875 - val_loss: 1.9839 Epoch 398/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3837 - val_loss: 1.9756 Epoch 399/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3798 - val_loss: 1.9674 Epoch 400/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3760 - val_loss: 1.9594 Epoch 401/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3723 - val_loss: 1.9515 Epoch 402/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3687 - val_loss: 1.9434 Epoch 403/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3650 - val_loss: 1.9356 Epoch 404/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3613 - val_loss: 1.9277 Epoch 405/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3578 - val_loss: 1.9199 Epoch 406/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3543 - val_loss: 1.9120 Epoch 407/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3508 - val_loss: 1.9041 Epoch 408/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3474 - val_loss: 1.8962 Epoch 409/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3438 - val_loss: 1.8886 Epoch 410/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3405 - val_loss: 1.8808 Epoch 411/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3370 - val_loss: 1.8730 Epoch 412/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3337 - val_loss: 1.8654 Epoch 413/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3304 - val_loss: 1.8578 Epoch 414/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3271 - val_loss: 1.8502 Epoch 415/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3238 - val_loss: 1.8428 Epoch 416/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3207 - val_loss: 1.8351 Epoch 417/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3175 - val_loss: 1.8276 Epoch 418/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3144 - val_loss: 1.8201 Epoch 419/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3113 - val_loss: 1.8128 Epoch 420/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3082 - val_loss: 1.8056 Epoch 421/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3052 - val_loss: 1.7985 Epoch 422/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3022 - val_loss: 1.7914 Epoch 423/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2992 - val_loss: 1.7842 Epoch 424/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2963 - val_loss: 1.7773 Epoch 425/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2935 - val_loss: 1.7702 Epoch 426/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2906 - val_loss: 1.7632 Epoch 427/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2877 - val_loss: 1.7563 Epoch 428/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2850 - val_loss: 1.7494 Epoch 429/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2822 - val_loss: 1.7424 Epoch 430/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2794 - val_loss: 1.7355 Epoch 431/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2767 - val_loss: 1.7285 Epoch 432/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2740 - val_loss: 1.7217 Epoch 433/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2713 - val_loss: 1.7148 Epoch 434/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2687 - val_loss: 1.7078 Epoch 435/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2660 - val_loss: 1.7011 Epoch 436/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2634 - val_loss: 1.6943 Epoch 437/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2609 - val_loss: 1.6876 Epoch 438/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2583 - val_loss: 1.6811 Epoch 439/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2559 - val_loss: 1.6744 Epoch 440/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2534 - val_loss: 1.6676 Epoch 441/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2509 - val_loss: 1.6609 Epoch 442/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2485 - val_loss: 1.6541 Epoch 443/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2461 - val_loss: 1.6475 Epoch 444/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2437 - val_loss: 1.6411 Epoch 445/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2414 - val_loss: 1.6346 Epoch 446/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2391 - val_loss: 1.6283 Epoch 447/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2367 - val_loss: 1.6220 Epoch 448/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2345 - val_loss: 1.6157 Epoch 449/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2322 - val_loss: 1.6094 Epoch 450/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2300 - val_loss: 1.6031 Epoch 451/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2278 - val_loss: 1.5968 Epoch 452/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2256 - val_loss: 1.5905 Epoch 453/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2234 - val_loss: 1.5844 Epoch 454/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2213 - val_loss: 1.5781 Epoch 455/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2192 - val_loss: 1.5718 Epoch 456/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2171 - val_loss: 1.5656 Epoch 457/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2151 - val_loss: 1.5595 Epoch 458/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2130 - val_loss: 1.5536 Epoch 459/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2111 - val_loss: 1.5476 Epoch 460/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2091 - val_loss: 1.5415 Epoch 461/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2071 - val_loss: 1.5357 Epoch 462/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2052 - val_loss: 1.5299 Epoch 463/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2033 - val_loss: 1.5241 Epoch 464/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2014 - val_loss: 1.5181 Epoch 465/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1995 - val_loss: 1.5122 Epoch 466/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1976 - val_loss: 1.5063 Epoch 467/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1958 - val_loss: 1.5006 Epoch 468/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1940 - val_loss: 1.4947 Epoch 469/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1922 - val_loss: 1.4890 Epoch 470/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1904 - val_loss: 1.4832 Epoch 471/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1887 - val_loss: 1.4777 Epoch 472/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1869 - val_loss: 1.4722 Epoch 473/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1852 - val_loss: 1.4668 Epoch 474/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1835 - val_loss: 1.4614 Epoch 475/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1819 - val_loss: 1.4560 Epoch 476/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1803 - val_loss: 1.4504 Epoch 477/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1787 - val_loss: 1.4450 Epoch 478/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1771 - val_loss: 1.4397 Epoch 479/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1755 - val_loss: 1.4342 Epoch 480/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1739 - val_loss: 1.4290 Epoch 481/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1724 - val_loss: 1.4235 Epoch 482/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1709 - val_loss: 1.4181 Epoch 483/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1693 - val_loss: 1.4129 Epoch 484/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1679 - val_loss: 1.4075 Epoch 485/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1664 - val_loss: 1.4023 Epoch 486/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1649 - val_loss: 1.3974 Epoch 487/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1635 - val_loss: 1.3922 Epoch 488/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1621 - val_loss: 1.3870 Epoch 489/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1606 - val_loss: 1.3818 Epoch 490/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 1.3766 Epoch 491/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1579 - val_loss: 1.3714 Epoch 492/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1565 - val_loss: 1.3665 Epoch 493/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1552 - val_loss: 1.3615 Epoch 494/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1539 - val_loss: 1.3565 Epoch 495/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1526 - val_loss: 1.3517 Epoch 496/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1513 - val_loss: 1.3466 Epoch 497/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1500 - val_loss: 1.3417 Epoch 498/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1488 - val_loss: 1.3366 Epoch 499/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1475 - val_loss: 1.3318 Epoch 500/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1463 - val_loss: 1.3271 Epoch 501/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1451 - val_loss: 1.3222 Epoch 502/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1439 - val_loss: 1.3173 Epoch 503/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1427 - val_loss: 1.3124 Epoch 504/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1416 - val_loss: 1.3075 Epoch 505/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1404 - val_loss: 1.3027 Epoch 506/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1392 - val_loss: 1.2979 Epoch 507/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1381 - val_loss: 1.2933 Epoch 508/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1370 - val_loss: 1.2886 Epoch 509/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1360 - val_loss: 1.2839 Epoch 510/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1349 - val_loss: 1.2792 Epoch 511/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1338 - val_loss: 1.2747 Epoch 512/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1327 - val_loss: 1.2702 Epoch 513/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1317 - val_loss: 1.2654 Epoch 514/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1307 - val_loss: 1.2608 Epoch 515/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1297 - val_loss: 1.2564 Epoch 516/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1287 - val_loss: 1.2521 Epoch 517/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1277 - val_loss: 1.2476 Epoch 518/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1267 - val_loss: 1.2432 Epoch 519/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 1.2386 Epoch 520/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 1.2342 Epoch 521/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1238 - val_loss: 1.2300 Epoch 522/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1229 - val_loss: 1.2255 Epoch 523/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1220 - val_loss: 1.2212 Epoch 524/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1211 - val_loss: 1.2168 Epoch 525/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1202 - val_loss: 1.2126 Epoch 526/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1193 - val_loss: 1.2082 Epoch 527/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1185 - val_loss: 1.2039 Epoch 528/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1176 - val_loss: 1.1997 Epoch 529/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1167 - val_loss: 1.1954 Epoch 530/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1159 - val_loss: 1.1911 Epoch 531/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1151 - val_loss: 1.1870 Epoch 532/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1142 - val_loss: 1.1831 Epoch 533/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1134 - val_loss: 1.1791 Epoch 534/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.1127 - val_loss: 1.1750 Epoch 535/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.1119 - val_loss: 1.1710 Epoch 536/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 1.1669 Epoch 537/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1103 - val_loss: 1.1629 Epoch 538/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1096 - val_loss: 1.1590 Epoch 539/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1089 - val_loss: 1.1551 Epoch 540/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1081 - val_loss: 1.1512 Epoch 541/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1074 - val_loss: 1.1471 Epoch 542/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1067 - val_loss: 1.1431 Epoch 543/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1060 - val_loss: 1.1388 Epoch 544/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1053 - val_loss: 1.1347 Epoch 545/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1046 - val_loss: 1.1307 Epoch 546/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1039 - val_loss: 1.1267 Epoch 547/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1032 - val_loss: 1.1226 Epoch 548/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1026 - val_loss: 1.1187 Epoch 549/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1020 - val_loss: 1.1147 Epoch 550/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 1.1107 Epoch 551/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1006 - val_loss: 1.1068 Epoch 552/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1000 - val_loss: 1.1028 Epoch 553/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0994 - val_loss: 1.0990 Epoch 554/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0988 - val_loss: 1.0952 Epoch 555/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0982 - val_loss: 1.0914 Epoch 556/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 1.0878 Epoch 557/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 1.0838 Epoch 558/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 1.0799 Epoch 559/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0958 - val_loss: 1.0761 Epoch 560/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0953 - val_loss: 1.0723 Epoch 561/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0947 - val_loss: 1.0685 Epoch 562/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0942 - val_loss: 1.0649 Epoch 563/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0936 - val_loss: 1.0613 Epoch 564/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0931 - val_loss: 1.0577 Epoch 565/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0926 - val_loss: 1.0539 Epoch 566/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0921 - val_loss: 1.0501 Epoch 567/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0915 - val_loss: 1.0466 Epoch 568/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0910 - val_loss: 1.0428 Epoch 569/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0905 - val_loss: 1.0394 Epoch 570/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 1.0356 Epoch 571/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0895 - val_loss: 1.0320 Epoch 572/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0891 - val_loss: 1.0284 Epoch 573/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0886 - val_loss: 1.0249 Epoch 574/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0881 - val_loss: 1.0212 Epoch 575/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0877 - val_loss: 1.0176 Epoch 576/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0872 - val_loss: 1.0141 Epoch 577/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0867 - val_loss: 1.0107 Epoch 578/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0863 - val_loss: 1.0072 Epoch 579/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0859 - val_loss: 1.0037 Epoch 580/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0854 - val_loss: 1.0002 Epoch 581/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0850 - val_loss: 0.9969 Epoch 582/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0846 - val_loss: 0.9933 Epoch 583/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0842 - val_loss: 0.9898 Epoch 584/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0838 - val_loss: 0.9865 Epoch 585/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0834 - val_loss: 0.9831 Epoch 586/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0830 - val_loss: 0.9796 Epoch 587/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0826 - val_loss: 0.9761 Epoch 588/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0822 - val_loss: 0.9726 Epoch 589/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0818 - val_loss: 0.9689 Epoch 590/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0814 - val_loss: 0.9655 Epoch 591/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0811 - val_loss: 0.9621 Epoch 592/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0807 - val_loss: 0.9589 Epoch 593/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0803 - val_loss: 0.9555 Epoch 594/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0800 - val_loss: 0.9522 Epoch 595/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0796 - val_loss: 0.9488 Epoch 596/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0792 - val_loss: 0.9454 Epoch 597/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0789 - val_loss: 0.9419 Epoch 598/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0786 - val_loss: 0.9388 Epoch 599/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 0.9355 Epoch 600/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 0.9324 Epoch 601/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0776 - val_loss: 0.9292 Epoch 602/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0773 - val_loss: 0.9261 Epoch 603/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0770 - val_loss: 0.9230 Epoch 604/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0767 - val_loss: 0.9196 Epoch 605/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 0.9165 Epoch 606/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0760 - val_loss: 0.9135 Epoch 607/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0758 - val_loss: 0.9102 Epoch 608/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0755 - val_loss: 0.9070 Epoch 609/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0752 - val_loss: 0.9039 Epoch 610/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0749 - val_loss: 0.9008 Epoch 611/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0746 - val_loss: 0.8976 Epoch 612/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0743 - val_loss: 0.8945 Epoch 613/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0740 - val_loss: 0.8914 Epoch 614/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0737 - val_loss: 0.8883 Epoch 615/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0735 - val_loss: 0.8852 Epoch 616/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0732 - val_loss: 0.8820 Epoch 617/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0729 - val_loss: 0.8789 Epoch 618/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0727 - val_loss: 0.8759 Epoch 619/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0724 - val_loss: 0.8729 Epoch 620/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0722 - val_loss: 0.8700 Epoch 621/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.8669 Epoch 622/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0717 - val_loss: 0.8641 Epoch 623/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.8610 Epoch 624/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0712 - val_loss: 0.8578 Epoch 625/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0709 - val_loss: 0.8547 Epoch 626/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0707 - val_loss: 0.8518 Epoch 627/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.8489 Epoch 628/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0702 - val_loss: 0.8459 Epoch 629/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.8430 Epoch 630/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0698 - val_loss: 0.8400 Epoch 631/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0696 - val_loss: 0.8370 Epoch 632/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0693 - val_loss: 0.8339 Epoch 633/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0691 - val_loss: 0.8309 Epoch 634/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.8279 Epoch 635/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0687 - val_loss: 0.8249 Epoch 636/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0685 - val_loss: 0.8221 Epoch 637/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.8193 Epoch 638/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8163 Epoch 639/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0679 - val_loss: 0.8137 Epoch 640/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0677 - val_loss: 0.8107 Epoch 641/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0675 - val_loss: 0.8078 Epoch 642/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0673 - val_loss: 0.8048 Epoch 643/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0671 - val_loss: 0.8015 Epoch 644/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0669 - val_loss: 0.7986 Epoch 645/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0667 - val_loss: 0.7956 Epoch 646/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0665 - val_loss: 0.7927 Epoch 647/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.7898 Epoch 648/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0662 - val_loss: 0.7870 Epoch 649/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.7843 Epoch 650/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.7816 Epoch 651/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.7787 Epoch 652/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0654 - val_loss: 0.7759 Epoch 653/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.7731 Epoch 654/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0651 - val_loss: 0.7703 Epoch 655/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0649 - val_loss: 0.7676 Epoch 656/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0647 - val_loss: 0.7649 Epoch 657/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0646 - val_loss: 0.7621 Epoch 658/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 0.7593 Epoch 659/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0642 - val_loss: 0.7565 Epoch 660/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.7537 Epoch 661/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7511 Epoch 662/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0638 - val_loss: 0.7482 Epoch 663/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.7454 Epoch 664/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0635 - val_loss: 0.7426 Epoch 665/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0633 - val_loss: 0.7400 Epoch 666/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0631 - val_loss: 0.7373 Epoch 667/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0630 - val_loss: 0.7345 Epoch 668/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0628 - val_loss: 0.7320 Epoch 669/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.7292 Epoch 670/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.7266 Epoch 671/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0624 - val_loss: 0.7240 Epoch 672/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0623 - val_loss: 0.7214 Epoch 673/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0621 - val_loss: 0.7188 Epoch 674/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0620 - val_loss: 0.7162 Epoch 675/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.7136 Epoch 676/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0617 - val_loss: 0.7108 Epoch 677/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0615 - val_loss: 0.7081 Epoch 678/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0614 - val_loss: 0.7053 Epoch 679/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.7027 Epoch 680/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0611 - val_loss: 0.7002 Epoch 681/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.6976 Epoch 682/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.6952 Epoch 683/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0607 - val_loss: 0.6927 Epoch 684/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0606 - val_loss: 0.6903 Epoch 685/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0605 - val_loss: 0.6877 Epoch 686/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0603 - val_loss: 0.6851 Epoch 687/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0602 - val_loss: 0.6827 Epoch 688/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0601 - val_loss: 0.6801 Epoch 689/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0600 - val_loss: 0.6775 Epoch 690/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0598 - val_loss: 0.6750 Epoch 691/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.6725 Epoch 692/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0596 - val_loss: 0.6700 Epoch 693/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.6674 Epoch 694/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0593 - val_loss: 0.6648 Epoch 695/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0592 - val_loss: 0.6624 Epoch 696/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.6600 Epoch 697/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6576 Epoch 698/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0589 - val_loss: 0.6551 Epoch 699/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.6525 Epoch 700/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0586 - val_loss: 0.6500 Epoch 701/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0585 - val_loss: 0.6474 Epoch 702/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.6449 Epoch 703/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0583 - val_loss: 0.6424 Epoch 704/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0582 - val_loss: 0.6398 Epoch 705/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0581 - val_loss: 0.6376 Epoch 706/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0580 - val_loss: 0.6352 Epoch 707/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.6328 Epoch 708/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0577 - val_loss: 0.6303 Epoch 709/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.6278 Epoch 710/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0575 - val_loss: 0.6253 Epoch 711/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0574 - val_loss: 0.6231 Epoch 712/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6207 Epoch 713/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0572 - val_loss: 0.6183 Epoch 714/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0571 - val_loss: 0.6157 Epoch 715/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0570 - val_loss: 0.6132 Epoch 716/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.6107 Epoch 717/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0568 - val_loss: 0.6083 Epoch 718/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0566 - val_loss: 0.6059 Epoch 719/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0566 - val_loss: 0.6035 Epoch 720/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0564 - val_loss: 0.6012 Epoch 721/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0563 - val_loss: 0.5989 Epoch 722/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.5964 Epoch 723/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0561 - val_loss: 0.5941 Epoch 724/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0560 - val_loss: 0.5915 Epoch 725/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0559 - val_loss: 0.5893 Epoch 726/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0558 - val_loss: 0.5869 Epoch 727/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5844 Epoch 728/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0556 - val_loss: 0.5821 Epoch 729/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.5799 Epoch 730/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0554 - val_loss: 0.5777 Epoch 731/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.5753 Epoch 732/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.5728 Epoch 733/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0551 - val_loss: 0.5701 Epoch 734/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.5676 Epoch 735/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0549 - val_loss: 0.5653 Epoch 736/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.5630 Epoch 737/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.5606 Epoch 738/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0546 - val_loss: 0.5584 Epoch 739/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.5560 Epoch 740/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0544 - val_loss: 0.5537 Epoch 741/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0543 - val_loss: 0.5514 Epoch 742/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5491 Epoch 743/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0541 - val_loss: 0.5468 Epoch 744/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5446 Epoch 745/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5421 Epoch 746/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0539 - val_loss: 0.5399 Epoch 747/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0538 - val_loss: 0.5374 Epoch 748/2000 6/6 [==============================] - 0s 6ms/step - loss: 0.0537 - val_loss: 0.5350 Epoch 749/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0536 - val_loss: 0.5327 Epoch 750/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.5305 Epoch 751/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0534 - val_loss: 0.5284 Epoch 752/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0533 - val_loss: 0.5262 Epoch 753/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0532 - val_loss: 0.5242 Epoch 754/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.5220 Epoch 755/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0530 - val_loss: 0.5198 Epoch 756/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5177 Epoch 757/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5156 Epoch 758/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0528 - val_loss: 0.5135 Epoch 759/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.5112 Epoch 760/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5089 Epoch 761/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0525 - val_loss: 0.5067 Epoch 762/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5046 Epoch 763/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0523 - val_loss: 0.5024 Epoch 764/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0522 - val_loss: 0.5002 Epoch 765/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.4980 Epoch 766/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0520 - val_loss: 0.4958 Epoch 767/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0520 - val_loss: 0.4936 Epoch 768/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.4915 Epoch 769/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.4895 Epoch 770/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4875 Epoch 771/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0516 - val_loss: 0.4855 Epoch 772/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.4833 Epoch 773/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4813 Epoch 774/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4791 Epoch 775/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.4772 Epoch 776/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4752 Epoch 777/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0511 - val_loss: 0.4731 Epoch 778/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.4709 Epoch 779/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4689 Epoch 780/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4670 Epoch 781/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0508 - val_loss: 0.4652 Epoch 782/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.4631 Epoch 783/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.4610 Epoch 784/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0505 - val_loss: 0.4586 Epoch 785/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4565 Epoch 786/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4545 Epoch 787/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.4524 Epoch 788/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0502 - val_loss: 0.4504 Epoch 789/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4484 Epoch 790/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4464 Epoch 791/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4442 Epoch 792/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.4421 Epoch 793/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.4401 Epoch 794/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0497 - val_loss: 0.4380 Epoch 795/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.4359 Epoch 796/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.4339 Epoch 797/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0495 - val_loss: 0.4318 Epoch 798/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0494 - val_loss: 0.4299 Epoch 799/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.4280 Epoch 800/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4262 Epoch 801/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.4242 Epoch 802/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.4221 Epoch 803/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.4202 Epoch 804/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0489 - val_loss: 0.4183 Epoch 805/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4164 Epoch 806/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4144 Epoch 807/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0487 - val_loss: 0.4125 Epoch 808/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4106 Epoch 809/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.4085 Epoch 810/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4066 Epoch 811/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4047 Epoch 812/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0483 - val_loss: 0.4027 Epoch 813/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.4008 Epoch 814/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.3989 Epoch 815/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.3970 Epoch 816/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0480 - val_loss: 0.3949 Epoch 817/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3930 Epoch 818/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0478 - val_loss: 0.3912 Epoch 819/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0478 - val_loss: 0.3894 Epoch 820/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0477 - val_loss: 0.3874 Epoch 821/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3857 Epoch 822/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3841 Epoch 823/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3824 Epoch 824/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.3807 Epoch 825/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0473 - val_loss: 0.3787 Epoch 826/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3770 Epoch 827/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3750 Epoch 828/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3731 Epoch 829/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3715 Epoch 830/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3696 Epoch 831/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3677 Epoch 832/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0468 - val_loss: 0.3660 Epoch 833/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.3641 Epoch 834/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3624 Epoch 835/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3604 Epoch 836/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.3585 Epoch 837/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3564 Epoch 838/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3546 Epoch 839/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3528 Epoch 840/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0462 - val_loss: 0.3510 Epoch 841/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0461 - val_loss: 0.3492 Epoch 842/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0461 - val_loss: 0.3473 Epoch 843/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3457 Epoch 844/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3439 Epoch 845/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3422 Epoch 846/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0458 - val_loss: 0.3405 Epoch 847/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3388 Epoch 848/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3372 Epoch 849/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.3356 Epoch 850/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.3339 Epoch 851/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.3322 Epoch 852/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0454 - val_loss: 0.3303 Epoch 853/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3286 Epoch 854/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3270 Epoch 855/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3253 Epoch 856/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3237 Epoch 857/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3220 Epoch 858/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.3204 Epoch 859/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3187 Epoch 860/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3169 Epoch 861/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.3150 Epoch 862/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.3133 Epoch 863/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0447 - val_loss: 0.3115 Epoch 864/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3098 Epoch 865/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3080 Epoch 866/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3063 Epoch 867/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0444 - val_loss: 0.3048 Epoch 868/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.3032 Epoch 869/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.3017 Epoch 870/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.3002 Epoch 871/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2985 Epoch 872/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2970 Epoch 873/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2955 Epoch 874/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2940 Epoch 875/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.2925 Epoch 876/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2908 Epoch 877/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2892 Epoch 878/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.2877 Epoch 879/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.2861 Epoch 880/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0436 - val_loss: 0.2846 Epoch 881/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2829 Epoch 882/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2812 Epoch 883/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2797 Epoch 884/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.2782 Epoch 885/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.2769 Epoch 886/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2753 Epoch 887/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0432 - val_loss: 0.2739 Epoch 888/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2724 Epoch 889/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2710 Epoch 890/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2694 Epoch 891/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2679 Epoch 892/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2663 Epoch 893/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0428 - val_loss: 0.2647 Epoch 894/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2633 Epoch 895/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2617 Epoch 896/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0426 - val_loss: 0.2602 Epoch 897/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2587 Epoch 898/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.2571 Epoch 899/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2558 Epoch 900/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2544 Epoch 901/2000 6/6 [==============================] - 0s 5ms/step - loss: 0.0423 - val_loss: 0.2529 Epoch 902/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0423 - val_loss: 0.2514 Epoch 903/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2499 Epoch 904/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2484 Epoch 905/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2470 Epoch 906/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2456 Epoch 907/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0420 - val_loss: 0.2441 Epoch 908/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2425 Epoch 909/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2410 Epoch 910/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2395 Epoch 911/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2381 Epoch 912/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2366 Epoch 913/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2355 Epoch 914/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2341 Epoch 915/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2328 Epoch 916/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2314 Epoch 917/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2301 Epoch 918/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0414 - val_loss: 0.2288 Epoch 919/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0413 - val_loss: 0.2274 Epoch 920/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2262 Epoch 921/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2251 Epoch 922/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0412 - val_loss: 0.2237 Epoch 923/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2223 Epoch 924/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0411 - val_loss: 0.2210 Epoch 925/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2197 Epoch 926/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2184 Epoch 927/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2171 Epoch 928/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2157 Epoch 929/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2144 Epoch 930/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2131 Epoch 931/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2119 Epoch 932/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2106 Epoch 933/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2094 Epoch 934/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2080 Epoch 935/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2067 Epoch 936/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2056 Epoch 937/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2044 Epoch 938/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0404 - val_loss: 0.2032 Epoch 939/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2021 Epoch 940/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2008 Epoch 941/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1997 Epoch 942/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.1985 Epoch 943/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0401 - val_loss: 0.1976 Epoch 944/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0401 - val_loss: 0.1964 Epoch 945/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0400 - val_loss: 0.1952 Epoch 946/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1941 Epoch 947/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1928 Epoch 948/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1916 Epoch 949/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1903 Epoch 950/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1892 Epoch 951/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1882 Epoch 952/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0397 - val_loss: 0.1870 Epoch 953/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1859 Epoch 954/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1847 Epoch 955/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1835 Epoch 956/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1824 Epoch 957/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1814 Epoch 958/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1803 Epoch 959/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1794 Epoch 960/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1783 Epoch 961/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1771 Epoch 962/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1760 Epoch 963/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1749 Epoch 964/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1738 Epoch 965/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1726 Epoch 966/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1716 Epoch 967/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1705 Epoch 968/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1694 Epoch 969/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1685 Epoch 970/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1674 Epoch 971/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1662 Epoch 972/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1650 Epoch 973/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1639 Epoch 974/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1628 Epoch 975/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1618 Epoch 976/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0387 - val_loss: 0.1609 Epoch 977/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1600 Epoch 978/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1590 Epoch 979/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1579 Epoch 980/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1569 Epoch 981/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1560 Epoch 982/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1552 Epoch 983/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1543 Epoch 984/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1533 Epoch 985/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1523 Epoch 986/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1514 Epoch 987/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1503 Epoch 988/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1493 Epoch 989/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1484 Epoch 990/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1475 Epoch 991/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0381 - val_loss: 0.1465 Epoch 992/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1457 Epoch 993/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1449 Epoch 994/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1439 Epoch 995/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1430 Epoch 996/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1420 Epoch 997/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1411 Epoch 998/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1401 Epoch 999/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1392 Epoch 1000/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1382 Epoch 1001/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0378 - val_loss: 0.1374 Epoch 1002/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1364 Epoch 1003/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1357 Epoch 1004/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1349 Epoch 1005/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1341 Epoch 1006/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1333 Epoch 1007/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1325 Epoch 1008/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1318 Epoch 1009/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1311 Epoch 1010/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1301 Epoch 1011/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1293 Epoch 1012/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1285 Epoch 1013/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1277 Epoch 1014/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1269 Epoch 1015/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1262 Epoch 1016/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1254 Epoch 1017/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0373 - val_loss: 0.1246 Epoch 1018/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1239 Epoch 1019/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1231 Epoch 1020/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1223 Epoch 1021/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1216 Epoch 1022/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1209 Epoch 1023/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1202 Epoch 1024/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1194 Epoch 1025/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0371 - val_loss: 0.1187 Epoch 1026/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1179 Epoch 1027/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0370 - val_loss: 0.1170 Epoch 1028/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1162 Epoch 1029/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1155 Epoch 1030/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1148 Epoch 1031/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0369 - val_loss: 0.1141 Epoch 1032/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0369 - val_loss: 0.1135 Epoch 1033/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0368 - val_loss: 0.1128 Epoch 1034/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1120 Epoch 1035/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1114 Epoch 1036/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1107 Epoch 1037/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1102 Epoch 1038/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1096 Epoch 1039/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1090 Epoch 1040/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1084 Epoch 1041/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0366 - val_loss: 0.1077 Epoch 1042/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1071 Epoch 1043/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1065 Epoch 1044/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1059 Epoch 1045/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1052 Epoch 1046/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.1045 Epoch 1047/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1039 Epoch 1048/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1031 Epoch 1049/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1025 Epoch 1050/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1018 Epoch 1051/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1013 Epoch 1052/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1006 Epoch 1053/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0999 Epoch 1054/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0992 Epoch 1055/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0985 Epoch 1056/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0980 Epoch 1057/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0362 - val_loss: 0.0974 Epoch 1058/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0968 Epoch 1059/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0962 Epoch 1060/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0957 Epoch 1061/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0950 Epoch 1062/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0945 Epoch 1063/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0939 Epoch 1064/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0933 Epoch 1065/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0926 Epoch 1066/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0921 Epoch 1067/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0916 Epoch 1068/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0911 Epoch 1069/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0905 Epoch 1070/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0900 Epoch 1071/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0895 Epoch 1072/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0890 Epoch 1073/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0885 Epoch 1074/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0880 Epoch 1075/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0876 Epoch 1076/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0871 Epoch 1077/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0865 Epoch 1078/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0860 Epoch 1079/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0857 Epoch 1080/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0358 - val_loss: 0.0852 Epoch 1081/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0846 Epoch 1082/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0842 Epoch 1083/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0836 Epoch 1084/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0830 Epoch 1085/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0825 Epoch 1086/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0821 Epoch 1087/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0816 Epoch 1088/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0811 Epoch 1089/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0806 Epoch 1090/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0801 Epoch 1091/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0356 - val_loss: 0.0796 Epoch 1092/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0792 Epoch 1093/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0787 Epoch 1094/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0782 Epoch 1095/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0778 Epoch 1096/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0774 Epoch 1097/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0770 Epoch 1098/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0766 Epoch 1099/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0762 Epoch 1100/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0759 Epoch 1101/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0755 Epoch 1102/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0751 Epoch 1103/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0747 Epoch 1104/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0743 Epoch 1105/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0739 Epoch 1106/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0735 Epoch 1107/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0731 Epoch 1108/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0727 Epoch 1109/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0724 Epoch 1110/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0721 Epoch 1111/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0717 Epoch 1112/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0714 Epoch 1113/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0710 Epoch 1114/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0706 Epoch 1115/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0702 Epoch 1116/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0698 Epoch 1117/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0352 - val_loss: 0.0694 Epoch 1118/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0690 Epoch 1119/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0686 Epoch 1120/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0683 Epoch 1121/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0679 Epoch 1122/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0675 Epoch 1123/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0673 Epoch 1124/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0669 Epoch 1125/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0664 Epoch 1126/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0660 Epoch 1127/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0656 Epoch 1128/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0652 Epoch 1129/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0649 Epoch 1130/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0646 Epoch 1131/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0643 Epoch 1132/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0640 Epoch 1133/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0637 Epoch 1134/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0634 Epoch 1135/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0631 Epoch 1136/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0628 Epoch 1137/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0626 Epoch 1138/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0623 Epoch 1139/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0620 Epoch 1140/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0617 Epoch 1141/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0615 Epoch 1142/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0613 Epoch 1143/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0610 Epoch 1144/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0607 Epoch 1145/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0603 Epoch 1146/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0601 Epoch 1147/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0598 Epoch 1148/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0596 Epoch 1149/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0594 Epoch 1150/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0591 Epoch 1151/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0588 Epoch 1152/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0585 Epoch 1153/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0582 Epoch 1154/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0581 Epoch 1155/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0579 Epoch 1156/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0576 Epoch 1157/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0574 Epoch 1158/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0572 Epoch 1159/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0570 Epoch 1160/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0568 Epoch 1161/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0566 Epoch 1162/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0564 Epoch 1163/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0563 Epoch 1164/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0560 Epoch 1165/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0558 Epoch 1166/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0556 Epoch 1167/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0553 Epoch 1168/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0551 Epoch 1169/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0549 Epoch 1170/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0548 Epoch 1171/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0546 Epoch 1172/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0544 Epoch 1173/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0542 Epoch 1174/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0540 Epoch 1175/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0538 Epoch 1176/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0536 Epoch 1177/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0534 Epoch 1178/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0533 Epoch 1179/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0531 Epoch 1180/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0530 Epoch 1181/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0529 Epoch 1182/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0527 Epoch 1183/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0525 Epoch 1184/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0524 Epoch 1185/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0522 Epoch 1186/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0520 Epoch 1187/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0519 Epoch 1188/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0517 Epoch 1189/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0516 Epoch 1190/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0514 Epoch 1191/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0512 Epoch 1192/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0510 Epoch 1193/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0508 Epoch 1194/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0508 Epoch 1195/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0506 Epoch 1196/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0504 Epoch 1197/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0502 Epoch 1198/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0501 Epoch 1199/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0499 Epoch 1200/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0497 Epoch 1201/2000 6/6 [==============================] - 0s 5ms/step - loss: 0.0345 - val_loss: 0.0496 Epoch 1202/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0494 Epoch 1203/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0492 Epoch 1204/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0491 Epoch 1205/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0490 Epoch 1206/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0489 Epoch 1207/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0487 Epoch 1208/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0486 Epoch 1209/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0483 Epoch 1210/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0482 Epoch 1211/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0481 Epoch 1212/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0479 Epoch 1213/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0478 Epoch 1214/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0477 Epoch 1215/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0476 Epoch 1216/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0475 Epoch 1217/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473 Epoch 1218/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0472 Epoch 1219/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0471 Epoch 1220/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0470 Epoch 1221/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0469 Epoch 1222/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0468 Epoch 1223/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0467 Epoch 1224/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0466 Epoch 1225/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0465 Epoch 1226/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0464 Epoch 1227/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463 Epoch 1228/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462 Epoch 1229/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0461 Epoch 1230/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0460 Epoch 1231/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0459 Epoch 1232/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458 Epoch 1233/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456 Epoch 1234/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0455 Epoch 1235/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454 Epoch 1236/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454 Epoch 1237/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0453 Epoch 1238/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452 Epoch 1239/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0451 Epoch 1240/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0450 Epoch 1241/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0449 Epoch 1242/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448 Epoch 1243/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0447 Epoch 1244/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446 Epoch 1245/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446 Epoch 1246/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0444 Epoch 1247/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444 Epoch 1248/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443 Epoch 1249/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443 Epoch 1250/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442 Epoch 1251/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0441 Epoch 1252/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440 Epoch 1253/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439 Epoch 1254/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438 Epoch 1255/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0437 Epoch 1256/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0436 Epoch 1257/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435 Epoch 1258/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434 Epoch 1259/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433 Epoch 1260/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432 Epoch 1261/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431 Epoch 1262/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431 Epoch 1263/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431 Epoch 1264/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430 Epoch 1265/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0429 Epoch 1266/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428 Epoch 1267/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428 Epoch 1268/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427 Epoch 1269/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427 Epoch 1270/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426 Epoch 1271/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426 Epoch 1272/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425 Epoch 1273/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425 Epoch 1274/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424 Epoch 1275/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424 Epoch 1276/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423 Epoch 1277/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422 Epoch 1278/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422 Epoch 1279/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421 Epoch 1280/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421 Epoch 1281/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0420 Epoch 1282/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419 Epoch 1283/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419 Epoch 1284/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419 Epoch 1285/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419 Epoch 1286/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418 Epoch 1287/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418 Epoch 1288/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417 Epoch 1289/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0417 Epoch 1290/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416 Epoch 1291/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416 Epoch 1292/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415 Epoch 1293/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0415 Epoch 1294/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414 Epoch 1295/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414 Epoch 1296/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413 Epoch 1297/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412 Epoch 1298/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412 Epoch 1299/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412 Epoch 1300/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412 Epoch 1301/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0411 Epoch 1302/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411 Epoch 1303/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411 Epoch 1304/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410 Epoch 1305/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410 Epoch 1306/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409 Epoch 1307/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409 Epoch 1308/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409 Epoch 1309/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409 Epoch 1310/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408 Epoch 1311/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408 Epoch 1312/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408 Epoch 1313/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408 Epoch 1314/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407 Epoch 1315/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406 Epoch 1316/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406 Epoch 1317/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406 Epoch 1318/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405 Epoch 1319/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405 Epoch 1320/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405 Epoch 1321/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404 Epoch 1322/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404 Epoch 1323/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404 Epoch 1324/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1325/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1326/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1327/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1328/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1329/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1330/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1331/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1332/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1333/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1334/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1335/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1336/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1337/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400 Epoch 1338/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0400 Epoch 1339/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1340/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1341/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1342/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1343/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1344/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1345/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1346/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1347/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1348/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1349/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1350/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1351/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1352/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1353/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1354/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1355/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1356/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1357/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1358/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1359/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1360/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1361/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1362/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1363/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1364/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1365/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1366/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1367/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1368/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1369/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1370/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1371/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1372/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1373/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1374/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1375/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1376/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1377/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1378/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1379/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1380/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1381/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1382/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1383/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1384/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1385/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1386/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1387/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1388/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1389/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1390/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1391/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1392/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1393/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1394/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1395/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1396/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1397/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1398/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1399/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1400/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1401/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1402/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1403/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1404/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1405/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1406/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1407/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1408/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1409/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1410/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1411/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1412/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1413/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1414/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1415/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1416/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1417/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1418/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1419/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1420/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1421/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1422/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1423/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1424/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1425/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1426/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1427/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1428/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1429/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1430/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1431/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1432/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1433/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1434/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1435/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1436/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1437/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1438/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1439/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1440/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1441/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1442/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1443/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1444/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1445/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1446/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1447/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1448/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1449/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1450/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1451/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1452/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1453/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1454/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1455/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1456/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1457/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1458/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1459/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1460/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1461/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1462/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1463/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1464/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1465/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1466/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1467/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1468/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1469/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1470/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1471/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1472/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1473/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1474/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1475/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1476/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1477/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1478/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1479/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1480/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1481/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1482/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1483/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1484/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1485/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1486/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1487/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1488/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1489/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1490/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1491/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1492/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1493/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1494/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1495/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1496/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1497/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1498/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1499/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1500/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1501/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1502/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1503/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1504/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1505/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1506/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1507/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1508/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1509/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1510/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1511/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1512/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1513/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1514/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1515/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1516/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1517/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1518/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1519/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1520/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1521/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1522/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1523/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1524/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1525/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1526/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1527/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1528/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1529/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1530/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1531/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1532/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1533/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1534/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1535/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1536/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1537/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1538/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1539/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1540/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1541/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1542/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1543/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1544/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1545/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1546/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1547/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1548/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1549/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1550/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1551/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1552/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1553/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1554/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1555/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1556/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1557/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1558/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1559/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1560/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1561/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1562/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1563/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1564/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1565/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1566/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1567/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1568/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1569/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1570/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1571/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1572/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1573/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1574/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1575/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1576/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1577/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1578/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1579/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1580/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1581/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1582/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1583/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1584/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1585/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1586/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1587/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1588/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1589/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1590/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1591/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1592/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1593/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1594/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1595/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1596/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1597/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1598/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1599/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1600/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1601/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1602/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1603/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1604/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1605/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1606/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1607/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1608/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1609/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1610/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1611/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1612/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1613/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1614/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1615/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1616/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1617/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1618/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1619/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1620/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1621/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1622/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1623/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1624/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1625/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1626/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1627/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1628/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1629/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1630/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1631/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1632/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1633/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1634/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1635/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1636/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1637/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1638/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1639/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1640/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1641/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1642/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1643/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1644/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1645/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1646/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1647/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1648/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1649/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1650/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1651/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1652/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1653/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1654/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1655/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1656/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1657/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1658/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1659/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1660/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1661/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1662/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1663/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1664/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1665/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1666/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1667/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1668/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1669/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1670/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1671/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1672/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1673/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1674/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1675/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1676/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1677/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1678/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1679/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1680/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1681/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1682/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1683/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1684/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1685/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1686/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1687/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1688/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1689/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1690/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1691/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1692/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1693/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1694/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1695/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1696/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1697/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1698/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1699/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1700/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1701/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1702/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1703/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1704/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1705/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1706/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1707/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1708/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1709/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1710/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1711/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1712/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1713/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1714/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1715/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1716/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1717/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1718/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1719/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1720/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1721/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1722/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1723/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1724/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1725/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1726/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1727/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1728/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1729/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1730/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1731/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1732/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1733/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1734/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1735/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1736/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1737/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1738/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1739/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1740/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1741/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1742/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1743/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1744/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1745/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1746/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1747/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1748/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1749/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1750/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1751/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1752/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1753/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1754/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1755/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1756/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1757/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1758/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1759/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1760/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1761/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1762/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1763/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1764/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1765/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1766/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1767/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1768/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1769/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1770/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1771/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1772/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1773/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1774/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1775/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1776/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1777/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1778/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1779/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1780/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1781/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1782/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1783/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1784/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1785/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1786/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1787/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1788/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1789/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1790/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1791/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1792/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1793/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1794/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1795/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1796/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1797/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1798/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1799/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1800/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1801/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1802/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1803/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1804/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1805/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1806/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1807/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1808/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1809/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1810/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1811/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1812/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1813/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1814/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1815/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1816/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1817/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1818/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1819/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1820/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1821/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1822/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1823/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1824/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1825/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1826/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1827/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1828/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1829/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1830/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1831/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1832/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1833/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1834/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1835/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1836/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1837/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1838/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1839/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1840/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1841/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1842/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1843/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1844/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1845/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1846/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1847/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1848/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1849/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1850/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1851/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1852/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1853/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1854/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1855/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1856/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1857/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1858/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1859/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1860/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1861/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1862/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1863/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1864/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1865/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1866/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1867/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1868/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1869/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1870/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1871/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1872/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1873/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1874/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1875/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1876/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1877/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1878/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1879/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1880/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1881/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1882/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1883/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1884/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1885/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1886/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1887/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1888/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1889/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1890/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1891/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1892/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1893/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1894/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1895/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1896/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1897/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1898/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1899/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1900/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1901/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1902/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1903/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1904/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1905/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1906/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1907/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1908/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1909/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1910/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1911/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1912/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1913/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1914/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1915/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1916/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1917/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1918/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1919/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1920/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1921/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1922/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1923/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1924/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1925/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1926/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1927/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1928/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1929/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1930/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1931/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1932/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1933/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1934/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1935/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1936/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1937/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1938/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1939/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1940/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1941/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1942/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1943/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1944/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1945/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1946/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1947/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1948/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1949/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1950/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1951/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1952/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1953/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1954/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1955/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1956/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1957/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1958/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1959/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1960/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1961/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1962/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1963/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1964/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1965/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1966/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1967/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1968/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1969/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1970/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1971/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1972/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1973/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1974/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1975/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1976/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1977/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1978/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1979/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1980/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1981/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1982/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1983/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1984/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1985/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1986/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1987/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1988/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1989/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1990/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1991/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1992/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1993/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1994/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1995/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1996/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1997/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1998/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1999/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 2000/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 . &lt;keras.callbacks.History at 0x7f708335a740&gt; . . net.weights . [&lt;tf.Variable &#39;dense_9/kernel:0&#39; shape=(5, 1) dtype=float32, numpy= array([[ 2.9966218e+00], [ 1.0097879e+00], [-1.4235497e-02], [ 3.8510602e-04], [ 4.8625717e-01]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_9/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-2.0080342], dtype=float32)&gt;] . net.summary() . Model: &#34;sequential_9&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_9 (Dense) (None, 1) 6 ================================================================= Total params: 6 Trainable params: 6 Non-trainable params: 0 _________________________________________________________________ . #%tensorboard --logdir logs --host 0.0.0.0 . &#53584;&#49436;&#48372;&#46300;: &#49324;&#50857;&#51088;&#51648;&#51221;&#44536;&#47548; &#50640;&#54253;&#48324;&#47196; &#49884;&#44033;&#54868; (1) . - 100에폭마다 적합결과를 시각화 하고 싶다 + 가중치와 같이!! . https://www.tensorflow.org/guide/keras/custom_callback | . - 일단 &quot;100에폭마다 가중치적합과정 시각화 + 최종적합곡선 시각화&quot; 까지 구현 . !rm -rf logs net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) cb1= tf.keras.callbacks.TensorBoard(update_freq=&#39;epoch&#39;,histogram_freq=100) net.fit(X,y,epochs=2000, batch_size=100, validation_split=0.45,callbacks=cb1) . Epoch 1/2000 6/6 [==============================] - 0s 11ms/step - loss: 6.6990 - val_loss: 14.1016 Epoch 2/2000 6/6 [==============================] - 0s 4ms/step - loss: 6.6442 - val_loss: 14.0394 Epoch 3/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.5913 - val_loss: 13.9775 Epoch 4/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.5361 - val_loss: 13.9172 Epoch 5/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.4823 - val_loss: 13.8573 Epoch 6/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.4314 - val_loss: 13.7958 Epoch 7/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.3769 - val_loss: 13.7367 Epoch 8/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.3262 - val_loss: 13.6770 Epoch 9/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.2734 - val_loss: 13.6171 Epoch 10/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.2212 - val_loss: 13.5591 Epoch 11/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.1701 - val_loss: 13.5023 Epoch 12/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.1201 - val_loss: 13.4448 Epoch 13/2000 6/6 [==============================] - 0s 3ms/step - loss: 6.0705 - val_loss: 13.3874 Epoch 14/2000 6/6 [==============================] - 0s 4ms/step - loss: 6.0197 - val_loss: 13.3311 Epoch 15/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.9705 - val_loss: 13.2742 Epoch 16/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.9195 - val_loss: 13.2184 Epoch 17/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.8713 - val_loss: 13.1627 Epoch 18/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.8223 - val_loss: 13.1072 Epoch 19/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.7729 - val_loss: 13.0523 Epoch 20/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.7259 - val_loss: 12.9976 Epoch 21/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.6771 - val_loss: 12.9449 Epoch 22/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.6299 - val_loss: 12.8918 Epoch 23/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.5827 - val_loss: 12.8385 Epoch 24/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.5371 - val_loss: 12.7853 Epoch 25/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.4892 - val_loss: 12.7333 Epoch 26/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.4440 - val_loss: 12.6799 Epoch 27/2000 6/6 [==============================] - 0s 4ms/step - loss: 5.3974 - val_loss: 12.6281 Epoch 28/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.3519 - val_loss: 12.5757 Epoch 29/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.3069 - val_loss: 12.5253 Epoch 30/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.2628 - val_loss: 12.4748 Epoch 31/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.2181 - val_loss: 12.4244 Epoch 32/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.1746 - val_loss: 12.3737 Epoch 33/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.1311 - val_loss: 12.3234 Epoch 34/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.0871 - val_loss: 12.2756 Epoch 35/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.0448 - val_loss: 12.2277 Epoch 36/2000 6/6 [==============================] - 0s 3ms/step - loss: 5.0017 - val_loss: 12.1806 Epoch 37/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.9595 - val_loss: 12.1327 Epoch 38/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.9173 - val_loss: 12.0851 Epoch 39/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.8764 - val_loss: 12.0371 Epoch 40/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.8342 - val_loss: 11.9906 Epoch 41/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.7936 - val_loss: 11.9438 Epoch 42/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.7533 - val_loss: 11.8970 Epoch 43/2000 6/6 [==============================] - 0s 4ms/step - loss: 4.7121 - val_loss: 11.8512 Epoch 44/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.6723 - val_loss: 11.8053 Epoch 45/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.6318 - val_loss: 11.7611 Epoch 46/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.5923 - val_loss: 11.7176 Epoch 47/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.5532 - val_loss: 11.6732 Epoch 48/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.5130 - val_loss: 11.6284 Epoch 49/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.4748 - val_loss: 11.5833 Epoch 50/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.4350 - val_loss: 11.5400 Epoch 51/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.3965 - val_loss: 11.4966 Epoch 52/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.3589 - val_loss: 11.4541 Epoch 53/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.3203 - val_loss: 11.4124 Epoch 54/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.2825 - val_loss: 11.3700 Epoch 55/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.2451 - val_loss: 11.3271 Epoch 56/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.2078 - val_loss: 11.2843 Epoch 57/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.1703 - val_loss: 11.2416 Epoch 58/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.1330 - val_loss: 11.1995 Epoch 59/2000 6/6 [==============================] - 0s 4ms/step - loss: 4.0959 - val_loss: 11.1581 Epoch 60/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.0600 - val_loss: 11.1171 Epoch 61/2000 6/6 [==============================] - 0s 3ms/step - loss: 4.0237 - val_loss: 11.0773 Epoch 62/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.9879 - val_loss: 11.0384 Epoch 63/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.9532 - val_loss: 10.9997 Epoch 64/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.9196 - val_loss: 10.9601 Epoch 65/2000 6/6 [==============================] - 0s 4ms/step - loss: 3.8845 - val_loss: 10.9219 Epoch 66/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.8504 - val_loss: 10.8832 Epoch 67/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.8163 - val_loss: 10.8449 Epoch 68/2000 6/6 [==============================] - 0s 4ms/step - loss: 3.7821 - val_loss: 10.8072 Epoch 69/2000 6/6 [==============================] - 0s 4ms/step - loss: 3.7485 - val_loss: 10.7692 Epoch 70/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.7154 - val_loss: 10.7316 Epoch 71/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.6823 - val_loss: 10.6955 Epoch 72/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.6498 - val_loss: 10.6591 Epoch 73/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.6170 - val_loss: 10.6233 Epoch 74/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.5846 - val_loss: 10.5880 Epoch 75/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.5533 - val_loss: 10.5517 Epoch 76/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.5209 - val_loss: 10.5146 Epoch 77/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.4886 - val_loss: 10.4796 Epoch 78/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.4579 - val_loss: 10.4437 Epoch 79/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.4264 - val_loss: 10.4078 Epoch 80/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.3956 - val_loss: 10.3729 Epoch 81/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.3637 - val_loss: 10.3384 Epoch 82/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.3335 - val_loss: 10.3034 Epoch 83/2000 6/6 [==============================] - 0s 4ms/step - loss: 3.3032 - val_loss: 10.2686 Epoch 84/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.2735 - val_loss: 10.2348 Epoch 85/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.2434 - val_loss: 10.2016 Epoch 86/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.2150 - val_loss: 10.1684 Epoch 87/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.1850 - val_loss: 10.1359 Epoch 88/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.1563 - val_loss: 10.1026 Epoch 89/2000 6/6 [==============================] - 0s 4ms/step - loss: 3.1270 - val_loss: 10.0714 Epoch 90/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.0990 - val_loss: 10.0392 Epoch 91/2000 6/6 [==============================] - 0s 4ms/step - loss: 3.0712 - val_loss: 10.0072 Epoch 92/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.0426 - val_loss: 9.9763 Epoch 93/2000 6/6 [==============================] - 0s 3ms/step - loss: 3.0153 - val_loss: 9.9454 Epoch 94/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.9878 - val_loss: 9.9147 Epoch 95/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.9603 - val_loss: 9.8845 Epoch 96/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.9336 - val_loss: 9.8549 Epoch 97/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.9068 - val_loss: 9.8259 Epoch 98/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.8804 - val_loss: 9.7963 Epoch 99/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.8538 - val_loss: 9.7673 Epoch 100/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.8276 - val_loss: 9.7382 Epoch 101/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.8017 - val_loss: 9.7100 Epoch 102/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7755 - val_loss: 9.6811 Epoch 103/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7499 - val_loss: 9.6516 Epoch 104/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7244 - val_loss: 9.6223 Epoch 105/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6994 - val_loss: 9.5934 Epoch 106/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6744 - val_loss: 9.5652 Epoch 107/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6495 - val_loss: 9.5379 Epoch 108/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6246 - val_loss: 9.5108 Epoch 109/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6008 - val_loss: 9.4831 Epoch 110/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5761 - val_loss: 9.4555 Epoch 111/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.5520 - val_loss: 9.4283 Epoch 112/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.5287 - val_loss: 9.4010 Epoch 113/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5052 - val_loss: 9.3744 Epoch 114/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4813 - val_loss: 9.3473 Epoch 115/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4583 - val_loss: 9.3206 Epoch 116/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4353 - val_loss: 9.2952 Epoch 117/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4123 - val_loss: 9.2694 Epoch 118/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3897 - val_loss: 9.2431 Epoch 119/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3671 - val_loss: 9.2181 Epoch 120/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3451 - val_loss: 9.1919 Epoch 121/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3226 - val_loss: 9.1674 Epoch 122/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3013 - val_loss: 9.1414 Epoch 123/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2790 - val_loss: 9.1171 Epoch 124/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2575 - val_loss: 9.0930 Epoch 125/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2361 - val_loss: 9.0694 Epoch 126/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.2154 - val_loss: 9.0448 Epoch 127/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1942 - val_loss: 9.0199 Epoch 128/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1733 - val_loss: 8.9960 Epoch 129/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1526 - val_loss: 8.9720 Epoch 130/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1320 - val_loss: 8.9488 Epoch 131/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1119 - val_loss: 8.9255 Epoch 132/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0918 - val_loss: 8.9022 Epoch 133/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0718 - val_loss: 8.8786 Epoch 134/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0517 - val_loss: 8.8560 Epoch 135/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0321 - val_loss: 8.8331 Epoch 136/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0127 - val_loss: 8.8099 Epoch 137/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9936 - val_loss: 8.7871 Epoch 138/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9746 - val_loss: 8.7652 Epoch 139/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9557 - val_loss: 8.7429 Epoch 140/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9368 - val_loss: 8.7217 Epoch 141/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9192 - val_loss: 8.7001 Epoch 142/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9003 - val_loss: 8.6779 Epoch 143/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8826 - val_loss: 8.6553 Epoch 144/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8645 - val_loss: 8.6343 Epoch 145/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8470 - val_loss: 8.6119 Epoch 146/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8295 - val_loss: 8.5907 Epoch 147/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8117 - val_loss: 8.5700 Epoch 148/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7949 - val_loss: 8.5491 Epoch 149/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7779 - val_loss: 8.5288 Epoch 150/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7607 - val_loss: 8.5087 Epoch 151/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7440 - val_loss: 8.4884 Epoch 152/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7277 - val_loss: 8.4686 Epoch 153/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7107 - val_loss: 8.4488 Epoch 154/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6944 - val_loss: 8.4297 Epoch 155/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6783 - val_loss: 8.4099 Epoch 156/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6626 - val_loss: 8.3904 Epoch 157/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6466 - val_loss: 8.3712 Epoch 158/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6305 - val_loss: 8.3510 Epoch 159/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6148 - val_loss: 8.3309 Epoch 160/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5992 - val_loss: 8.3106 Epoch 161/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5839 - val_loss: 8.2924 Epoch 162/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5687 - val_loss: 8.2737 Epoch 163/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5537 - val_loss: 8.2558 Epoch 164/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5393 - val_loss: 8.2368 Epoch 165/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5241 - val_loss: 8.2189 Epoch 166/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5098 - val_loss: 8.2008 Epoch 167/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4951 - val_loss: 8.1831 Epoch 168/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4808 - val_loss: 8.1653 Epoch 169/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4669 - val_loss: 8.1461 Epoch 170/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4523 - val_loss: 8.1289 Epoch 171/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4385 - val_loss: 8.1115 Epoch 172/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4247 - val_loss: 8.0941 Epoch 173/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4108 - val_loss: 8.0772 Epoch 174/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3972 - val_loss: 8.0592 Epoch 175/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3837 - val_loss: 8.0412 Epoch 176/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3700 - val_loss: 8.0239 Epoch 177/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3571 - val_loss: 8.0066 Epoch 178/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3440 - val_loss: 7.9895 Epoch 179/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3308 - val_loss: 7.9722 Epoch 180/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3181 - val_loss: 7.9550 Epoch 181/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3055 - val_loss: 7.9387 Epoch 182/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2926 - val_loss: 7.9229 Epoch 183/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2805 - val_loss: 7.9067 Epoch 184/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2681 - val_loss: 7.8900 Epoch 185/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2556 - val_loss: 7.8737 Epoch 186/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2437 - val_loss: 7.8576 Epoch 187/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2318 - val_loss: 7.8403 Epoch 188/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2196 - val_loss: 7.8245 Epoch 189/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2079 - val_loss: 7.8084 Epoch 190/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1965 - val_loss: 7.7924 Epoch 191/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1850 - val_loss: 7.7777 Epoch 192/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1735 - val_loss: 7.7618 Epoch 193/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1625 - val_loss: 7.7459 Epoch 194/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1512 - val_loss: 7.7310 Epoch 195/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1403 - val_loss: 7.7166 Epoch 196/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1294 - val_loss: 7.7019 Epoch 197/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1186 - val_loss: 7.6871 Epoch 198/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1078 - val_loss: 7.6719 Epoch 199/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0973 - val_loss: 7.6572 Epoch 200/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0866 - val_loss: 7.6426 Epoch 201/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.0762 - val_loss: 7.6278 Epoch 202/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0657 - val_loss: 7.6135 Epoch 203/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0558 - val_loss: 7.5977 Epoch 204/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0456 - val_loss: 7.5829 Epoch 205/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.0357 - val_loss: 7.5681 Epoch 206/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0256 - val_loss: 7.5545 Epoch 207/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0160 - val_loss: 7.5397 Epoch 208/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0064 - val_loss: 7.5247 Epoch 209/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9968 - val_loss: 7.5098 Epoch 210/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9873 - val_loss: 7.4949 Epoch 211/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9781 - val_loss: 7.4804 Epoch 212/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9685 - val_loss: 7.4660 Epoch 213/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.9593 - val_loss: 7.4515 Epoch 214/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9503 - val_loss: 7.4373 Epoch 215/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9412 - val_loss: 7.4229 Epoch 216/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9323 - val_loss: 7.4090 Epoch 217/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9237 - val_loss: 7.3950 Epoch 218/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9149 - val_loss: 7.3820 Epoch 219/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9065 - val_loss: 7.3686 Epoch 220/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8979 - val_loss: 7.3558 Epoch 221/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8896 - val_loss: 7.3422 Epoch 222/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8814 - val_loss: 7.3290 Epoch 223/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8731 - val_loss: 7.3155 Epoch 224/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8654 - val_loss: 7.3014 Epoch 225/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8572 - val_loss: 7.2881 Epoch 226/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8492 - val_loss: 7.2753 Epoch 227/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.8416 - val_loss: 7.2624 Epoch 228/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8338 - val_loss: 7.2492 Epoch 229/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8262 - val_loss: 7.2358 Epoch 230/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.8185 - val_loss: 7.2236 Epoch 231/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.8110 - val_loss: 7.2112 Epoch 232/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8036 - val_loss: 7.1979 Epoch 233/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.7962 - val_loss: 7.1849 Epoch 234/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7890 - val_loss: 7.1720 Epoch 235/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7817 - val_loss: 7.1592 Epoch 236/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7746 - val_loss: 7.1468 Epoch 237/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7674 - val_loss: 7.1337 Epoch 238/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7605 - val_loss: 7.1196 Epoch 239/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7536 - val_loss: 7.1064 Epoch 240/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7467 - val_loss: 7.0940 Epoch 241/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7399 - val_loss: 7.0820 Epoch 242/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7333 - val_loss: 7.0690 Epoch 243/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7267 - val_loss: 7.0568 Epoch 244/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7202 - val_loss: 7.0443 Epoch 245/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7139 - val_loss: 7.0311 Epoch 246/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7075 - val_loss: 7.0185 Epoch 247/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7012 - val_loss: 7.0054 Epoch 248/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6948 - val_loss: 6.9938 Epoch 249/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6887 - val_loss: 6.9809 Epoch 250/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6827 - val_loss: 6.9690 Epoch 251/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6768 - val_loss: 6.9565 Epoch 252/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6709 - val_loss: 6.9443 Epoch 253/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6650 - val_loss: 6.9322 Epoch 254/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6590 - val_loss: 6.9206 Epoch 255/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6533 - val_loss: 6.9084 Epoch 256/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6477 - val_loss: 6.8959 Epoch 257/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6422 - val_loss: 6.8833 Epoch 258/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6366 - val_loss: 6.8716 Epoch 259/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6312 - val_loss: 6.8591 Epoch 260/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6258 - val_loss: 6.8477 Epoch 261/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6206 - val_loss: 6.8359 Epoch 262/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6153 - val_loss: 6.8237 Epoch 263/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6101 - val_loss: 6.8123 Epoch 264/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6050 - val_loss: 6.8006 Epoch 265/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6000 - val_loss: 6.7892 Epoch 266/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5949 - val_loss: 6.7776 Epoch 267/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5900 - val_loss: 6.7652 Epoch 268/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5849 - val_loss: 6.7529 Epoch 269/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5801 - val_loss: 6.7406 Epoch 270/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5753 - val_loss: 6.7281 Epoch 271/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5705 - val_loss: 6.7161 Epoch 272/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5658 - val_loss: 6.7042 Epoch 273/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5612 - val_loss: 6.6921 Epoch 274/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5567 - val_loss: 6.6803 Epoch 275/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5521 - val_loss: 6.6690 Epoch 276/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5478 - val_loss: 6.6569 Epoch 277/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5433 - val_loss: 6.6454 Epoch 278/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5391 - val_loss: 6.6337 Epoch 279/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5346 - val_loss: 6.6222 Epoch 280/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5306 - val_loss: 6.6105 Epoch 281/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.5264 - val_loss: 6.5989 Epoch 282/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5223 - val_loss: 6.5878 Epoch 283/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5183 - val_loss: 6.5764 Epoch 284/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5143 - val_loss: 6.5650 Epoch 285/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5103 - val_loss: 6.5537 Epoch 286/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5065 - val_loss: 6.5421 Epoch 287/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5025 - val_loss: 6.5305 Epoch 288/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4988 - val_loss: 6.5192 Epoch 289/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4951 - val_loss: 6.5072 Epoch 290/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4914 - val_loss: 6.4958 Epoch 291/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4876 - val_loss: 6.4849 Epoch 292/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4840 - val_loss: 6.4732 Epoch 293/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4805 - val_loss: 6.4623 Epoch 294/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4768 - val_loss: 6.4509 Epoch 295/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4733 - val_loss: 6.4391 Epoch 296/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4698 - val_loss: 6.4276 Epoch 297/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4664 - val_loss: 6.4161 Epoch 298/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4630 - val_loss: 6.4040 Epoch 299/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4595 - val_loss: 6.3927 Epoch 300/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4562 - val_loss: 6.3814 Epoch 301/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4529 - val_loss: 6.3698 Epoch 302/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4497 - val_loss: 6.3581 Epoch 303/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4463 - val_loss: 6.3466 Epoch 304/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4432 - val_loss: 6.3346 Epoch 305/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4402 - val_loss: 6.3230 Epoch 306/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4370 - val_loss: 6.3121 Epoch 307/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4340 - val_loss: 6.3007 Epoch 308/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4310 - val_loss: 6.2892 Epoch 309/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4280 - val_loss: 6.2780 Epoch 310/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4251 - val_loss: 6.2666 Epoch 311/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4222 - val_loss: 6.2554 Epoch 312/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4194 - val_loss: 6.2441 Epoch 313/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4165 - val_loss: 6.2326 Epoch 314/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4137 - val_loss: 6.2205 Epoch 315/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4110 - val_loss: 6.2087 Epoch 316/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4082 - val_loss: 6.1973 Epoch 317/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4056 - val_loss: 6.1859 Epoch 318/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4030 - val_loss: 6.1745 Epoch 319/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4004 - val_loss: 6.1624 Epoch 320/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3977 - val_loss: 6.1507 Epoch 321/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3952 - val_loss: 6.1391 Epoch 322/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3927 - val_loss: 6.1269 Epoch 323/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3902 - val_loss: 6.1151 Epoch 324/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3878 - val_loss: 6.1037 Epoch 325/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3854 - val_loss: 6.0923 Epoch 326/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3829 - val_loss: 6.0804 Epoch 327/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3805 - val_loss: 6.0692 Epoch 328/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3782 - val_loss: 6.0571 Epoch 329/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3759 - val_loss: 6.0447 Epoch 330/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3735 - val_loss: 6.0329 Epoch 331/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3713 - val_loss: 6.0203 Epoch 332/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3690 - val_loss: 6.0089 Epoch 333/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3668 - val_loss: 5.9969 Epoch 334/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3645 - val_loss: 5.9852 Epoch 335/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3625 - val_loss: 5.9734 Epoch 336/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3603 - val_loss: 5.9614 Epoch 337/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3581 - val_loss: 5.9501 Epoch 338/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3561 - val_loss: 5.9386 Epoch 339/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3541 - val_loss: 5.9270 Epoch 340/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3521 - val_loss: 5.9156 Epoch 341/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3501 - val_loss: 5.9042 Epoch 342/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3481 - val_loss: 5.8924 Epoch 343/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3463 - val_loss: 5.8808 Epoch 344/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3442 - val_loss: 5.8688 Epoch 345/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3424 - val_loss: 5.8569 Epoch 346/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3405 - val_loss: 5.8452 Epoch 347/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3386 - val_loss: 5.8337 Epoch 348/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3369 - val_loss: 5.8213 Epoch 349/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3350 - val_loss: 5.8097 Epoch 350/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3332 - val_loss: 5.7979 Epoch 351/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3315 - val_loss: 5.7865 Epoch 352/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3297 - val_loss: 5.7747 Epoch 353/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3280 - val_loss: 5.7631 Epoch 354/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3263 - val_loss: 5.7515 Epoch 355/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3246 - val_loss: 5.7400 Epoch 356/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3230 - val_loss: 5.7283 Epoch 357/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3213 - val_loss: 5.7165 Epoch 358/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3197 - val_loss: 5.7047 Epoch 359/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3181 - val_loss: 5.6931 Epoch 360/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3165 - val_loss: 5.6812 Epoch 361/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3150 - val_loss: 5.6690 Epoch 362/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3134 - val_loss: 5.6579 Epoch 363/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3118 - val_loss: 5.6461 Epoch 364/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3104 - val_loss: 5.6340 Epoch 365/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3089 - val_loss: 5.6223 Epoch 366/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3074 - val_loss: 5.6100 Epoch 367/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3060 - val_loss: 5.5979 Epoch 368/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3045 - val_loss: 5.5858 Epoch 369/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3030 - val_loss: 5.5742 Epoch 370/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3016 - val_loss: 5.5622 Epoch 371/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3002 - val_loss: 5.5502 Epoch 372/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2989 - val_loss: 5.5378 Epoch 373/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2975 - val_loss: 5.5254 Epoch 374/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2961 - val_loss: 5.5131 Epoch 375/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2948 - val_loss: 5.5012 Epoch 376/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2936 - val_loss: 5.4891 Epoch 377/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2923 - val_loss: 5.4766 Epoch 378/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2910 - val_loss: 5.4642 Epoch 379/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2897 - val_loss: 5.4516 Epoch 380/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2885 - val_loss: 5.4399 Epoch 381/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2872 - val_loss: 5.4283 Epoch 382/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2860 - val_loss: 5.4164 Epoch 383/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2848 - val_loss: 5.4039 Epoch 384/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2836 - val_loss: 5.3920 Epoch 385/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2824 - val_loss: 5.3798 Epoch 386/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2812 - val_loss: 5.3679 Epoch 387/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2801 - val_loss: 5.3552 Epoch 388/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2790 - val_loss: 5.3430 Epoch 389/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2778 - val_loss: 5.3307 Epoch 390/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2767 - val_loss: 5.3186 Epoch 391/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2756 - val_loss: 5.3061 Epoch 392/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2745 - val_loss: 5.2937 Epoch 393/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2734 - val_loss: 5.2816 Epoch 394/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2724 - val_loss: 5.2698 Epoch 395/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2713 - val_loss: 5.2575 Epoch 396/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2702 - val_loss: 5.2452 Epoch 397/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.2692 - val_loss: 5.2331 Epoch 398/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2681 - val_loss: 5.2213 Epoch 399/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2671 - val_loss: 5.2088 Epoch 400/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2661 - val_loss: 5.1963 Epoch 401/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2651 - val_loss: 5.1842 Epoch 402/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2641 - val_loss: 5.1719 Epoch 403/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2631 - val_loss: 5.1593 Epoch 404/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2620 - val_loss: 5.1472 Epoch 405/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2611 - val_loss: 5.1346 Epoch 406/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2601 - val_loss: 5.1224 Epoch 407/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2592 - val_loss: 5.1101 Epoch 408/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2583 - val_loss: 5.0975 Epoch 409/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2573 - val_loss: 5.0852 Epoch 410/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2564 - val_loss: 5.0724 Epoch 411/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2555 - val_loss: 5.0598 Epoch 412/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2546 - val_loss: 5.0475 Epoch 413/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2536 - val_loss: 5.0351 Epoch 414/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2528 - val_loss: 5.0226 Epoch 415/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2519 - val_loss: 5.0102 Epoch 416/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2510 - val_loss: 4.9976 Epoch 417/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2501 - val_loss: 4.9857 Epoch 418/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2492 - val_loss: 4.9732 Epoch 419/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2483 - val_loss: 4.9606 Epoch 420/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2475 - val_loss: 4.9481 Epoch 421/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2466 - val_loss: 4.9357 Epoch 422/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2458 - val_loss: 4.9231 Epoch 423/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2450 - val_loss: 4.9108 Epoch 424/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2442 - val_loss: 4.8982 Epoch 425/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.2433 - val_loss: 4.8859 Epoch 426/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.2425 - val_loss: 4.8733 Epoch 427/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2417 - val_loss: 4.8607 Epoch 428/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2409 - val_loss: 4.8482 Epoch 429/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2402 - val_loss: 4.8354 Epoch 430/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2393 - val_loss: 4.8233 Epoch 431/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2386 - val_loss: 4.8107 Epoch 432/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2378 - val_loss: 4.7986 Epoch 433/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2371 - val_loss: 4.7858 Epoch 434/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2363 - val_loss: 4.7728 Epoch 435/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2355 - val_loss: 4.7603 Epoch 436/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2347 - val_loss: 4.7470 Epoch 437/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2340 - val_loss: 4.7336 Epoch 438/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2331 - val_loss: 4.7212 Epoch 439/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2324 - val_loss: 4.7081 Epoch 440/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2317 - val_loss: 4.6952 Epoch 441/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2309 - val_loss: 4.6824 Epoch 442/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2301 - val_loss: 4.6697 Epoch 443/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2294 - val_loss: 4.6565 Epoch 444/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2286 - val_loss: 4.6438 Epoch 445/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2280 - val_loss: 4.6312 Epoch 446/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2272 - val_loss: 4.6187 Epoch 447/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2265 - val_loss: 4.6062 Epoch 448/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2258 - val_loss: 4.5933 Epoch 449/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2251 - val_loss: 4.5809 Epoch 450/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2244 - val_loss: 4.5682 Epoch 451/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2237 - val_loss: 4.5557 Epoch 452/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2230 - val_loss: 4.5432 Epoch 453/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2223 - val_loss: 4.5304 Epoch 454/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2216 - val_loss: 4.5172 Epoch 455/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2209 - val_loss: 4.5045 Epoch 456/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2203 - val_loss: 4.4913 Epoch 457/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2196 - val_loss: 4.4780 Epoch 458/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2189 - val_loss: 4.4653 Epoch 459/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2182 - val_loss: 4.4529 Epoch 460/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2176 - val_loss: 4.4401 Epoch 461/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2169 - val_loss: 4.4275 Epoch 462/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2163 - val_loss: 4.4144 Epoch 463/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2156 - val_loss: 4.4018 Epoch 464/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2150 - val_loss: 4.3893 Epoch 465/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2143 - val_loss: 4.3762 Epoch 466/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2137 - val_loss: 4.3634 Epoch 467/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2131 - val_loss: 4.3504 Epoch 468/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2125 - val_loss: 4.3378 Epoch 469/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2118 - val_loss: 4.3251 Epoch 470/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2112 - val_loss: 4.3126 Epoch 471/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2106 - val_loss: 4.2997 Epoch 472/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2100 - val_loss: 4.2867 Epoch 473/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2093 - val_loss: 4.2739 Epoch 474/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2086 - val_loss: 4.2609 Epoch 475/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2080 - val_loss: 4.2479 Epoch 476/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2074 - val_loss: 4.2348 Epoch 477/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2068 - val_loss: 4.2221 Epoch 478/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2062 - val_loss: 4.2093 Epoch 479/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2056 - val_loss: 4.1964 Epoch 480/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2050 - val_loss: 4.1831 Epoch 481/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2044 - val_loss: 4.1703 Epoch 482/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2038 - val_loss: 4.1576 Epoch 483/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2031 - val_loss: 4.1450 Epoch 484/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2025 - val_loss: 4.1318 Epoch 485/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2019 - val_loss: 4.1189 Epoch 486/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2013 - val_loss: 4.1055 Epoch 487/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2007 - val_loss: 4.0927 Epoch 488/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2001 - val_loss: 4.0799 Epoch 489/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1995 - val_loss: 4.0670 Epoch 490/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1989 - val_loss: 4.0543 Epoch 491/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1983 - val_loss: 4.0412 Epoch 492/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1977 - val_loss: 4.0288 Epoch 493/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1971 - val_loss: 4.0160 Epoch 494/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1965 - val_loss: 4.0029 Epoch 495/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1960 - val_loss: 3.9893 Epoch 496/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1954 - val_loss: 3.9760 Epoch 497/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1948 - val_loss: 3.9628 Epoch 498/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1942 - val_loss: 3.9498 Epoch 499/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1936 - val_loss: 3.9373 Epoch 500/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1931 - val_loss: 3.9246 Epoch 501/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1925 - val_loss: 3.9113 Epoch 502/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1919 - val_loss: 3.8987 Epoch 503/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1914 - val_loss: 3.8862 Epoch 504/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1907 - val_loss: 3.8739 Epoch 505/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1901 - val_loss: 3.8608 Epoch 506/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1896 - val_loss: 3.8478 Epoch 507/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1889 - val_loss: 3.8350 Epoch 508/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1884 - val_loss: 3.8218 Epoch 509/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1878 - val_loss: 3.8091 Epoch 510/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1872 - val_loss: 3.7960 Epoch 511/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1867 - val_loss: 3.7827 Epoch 512/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1861 - val_loss: 3.7695 Epoch 513/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1855 - val_loss: 3.7564 Epoch 514/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1849 - val_loss: 3.7437 Epoch 515/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1843 - val_loss: 3.7308 Epoch 516/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1837 - val_loss: 3.7178 Epoch 517/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1832 - val_loss: 3.7048 Epoch 518/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1826 - val_loss: 3.6917 Epoch 519/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1821 - val_loss: 3.6781 Epoch 520/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1815 - val_loss: 3.6648 Epoch 521/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1809 - val_loss: 3.6515 Epoch 522/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1804 - val_loss: 3.6382 Epoch 523/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1799 - val_loss: 3.6252 Epoch 524/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1793 - val_loss: 3.6128 Epoch 525/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1787 - val_loss: 3.5998 Epoch 526/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1781 - val_loss: 3.5868 Epoch 527/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1776 - val_loss: 3.5735 Epoch 528/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1770 - val_loss: 3.5601 Epoch 529/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1765 - val_loss: 3.5467 Epoch 530/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1759 - val_loss: 3.5337 Epoch 531/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1753 - val_loss: 3.5207 Epoch 532/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1748 - val_loss: 3.5074 Epoch 533/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1743 - val_loss: 3.4943 Epoch 534/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1737 - val_loss: 3.4814 Epoch 535/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1731 - val_loss: 3.4686 Epoch 536/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1726 - val_loss: 3.4557 Epoch 537/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1721 - val_loss: 3.4433 Epoch 538/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1715 - val_loss: 3.4307 Epoch 539/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1710 - val_loss: 3.4174 Epoch 540/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1704 - val_loss: 3.4044 Epoch 541/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1699 - val_loss: 3.3917 Epoch 542/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1693 - val_loss: 3.3786 Epoch 543/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1688 - val_loss: 3.3658 Epoch 544/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1683 - val_loss: 3.3522 Epoch 545/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1677 - val_loss: 3.3393 Epoch 546/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1672 - val_loss: 3.3264 Epoch 547/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1667 - val_loss: 3.3137 Epoch 548/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1661 - val_loss: 3.3012 Epoch 549/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1656 - val_loss: 3.2885 Epoch 550/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1651 - val_loss: 3.2761 Epoch 551/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1646 - val_loss: 3.2636 Epoch 552/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1641 - val_loss: 3.2506 Epoch 553/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1635 - val_loss: 3.2386 Epoch 554/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.1630 - val_loss: 3.2257 Epoch 555/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1625 - val_loss: 3.2130 Epoch 556/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1619 - val_loss: 3.2007 Epoch 557/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1614 - val_loss: 3.1881 Epoch 558/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1609 - val_loss: 3.1755 Epoch 559/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1604 - val_loss: 3.1625 Epoch 560/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1598 - val_loss: 3.1501 Epoch 561/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 3.1372 Epoch 562/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1588 - val_loss: 3.1248 Epoch 563/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1582 - val_loss: 3.1119 Epoch 564/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1577 - val_loss: 3.0991 Epoch 565/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1572 - val_loss: 3.0867 Epoch 566/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1567 - val_loss: 3.0744 Epoch 567/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1562 - val_loss: 3.0622 Epoch 568/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1557 - val_loss: 3.0497 Epoch 569/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1552 - val_loss: 3.0377 Epoch 570/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1547 - val_loss: 3.0254 Epoch 571/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1541 - val_loss: 3.0136 Epoch 572/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1536 - val_loss: 3.0013 Epoch 573/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1531 - val_loss: 2.9889 Epoch 574/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1526 - val_loss: 2.9764 Epoch 575/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1521 - val_loss: 2.9640 Epoch 576/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1516 - val_loss: 2.9517 Epoch 577/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1511 - val_loss: 2.9392 Epoch 578/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1505 - val_loss: 2.9271 Epoch 579/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1500 - val_loss: 2.9147 Epoch 580/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1495 - val_loss: 2.9022 Epoch 581/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1490 - val_loss: 2.8904 Epoch 582/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1485 - val_loss: 2.8783 Epoch 583/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1480 - val_loss: 2.8657 Epoch 584/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1475 - val_loss: 2.8539 Epoch 585/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1470 - val_loss: 2.8414 Epoch 586/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1465 - val_loss: 2.8292 Epoch 587/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1460 - val_loss: 2.8170 Epoch 588/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1455 - val_loss: 2.8047 Epoch 589/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1451 - val_loss: 2.7925 Epoch 590/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1445 - val_loss: 2.7802 Epoch 591/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1440 - val_loss: 2.7680 Epoch 592/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1435 - val_loss: 2.7559 Epoch 593/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1430 - val_loss: 2.7437 Epoch 594/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1426 - val_loss: 2.7318 Epoch 595/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1420 - val_loss: 2.7202 Epoch 596/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1415 - val_loss: 2.7084 Epoch 597/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1411 - val_loss: 2.6963 Epoch 598/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1405 - val_loss: 2.6843 Epoch 599/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1400 - val_loss: 2.6728 Epoch 600/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1396 - val_loss: 2.6609 Epoch 601/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1391 - val_loss: 2.6491 Epoch 602/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1386 - val_loss: 2.6370 Epoch 603/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1381 - val_loss: 2.6249 Epoch 604/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1376 - val_loss: 2.6129 Epoch 605/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1371 - val_loss: 2.6011 Epoch 606/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1366 - val_loss: 2.5889 Epoch 607/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1361 - val_loss: 2.5768 Epoch 608/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1356 - val_loss: 2.5650 Epoch 609/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1351 - val_loss: 2.5529 Epoch 610/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1347 - val_loss: 2.5414 Epoch 611/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1342 - val_loss: 2.5297 Epoch 612/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1337 - val_loss: 2.5178 Epoch 613/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1332 - val_loss: 2.5065 Epoch 614/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1327 - val_loss: 2.4948 Epoch 615/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1323 - val_loss: 2.4828 Epoch 616/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1318 - val_loss: 2.4708 Epoch 617/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1313 - val_loss: 2.4591 Epoch 618/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1308 - val_loss: 2.4475 Epoch 619/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1304 - val_loss: 2.4358 Epoch 620/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1299 - val_loss: 2.4243 Epoch 621/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1294 - val_loss: 2.4127 Epoch 622/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1290 - val_loss: 2.4011 Epoch 623/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1285 - val_loss: 2.3899 Epoch 624/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1280 - val_loss: 2.3783 Epoch 625/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1275 - val_loss: 2.3666 Epoch 626/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1271 - val_loss: 2.3553 Epoch 627/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1266 - val_loss: 2.3438 Epoch 628/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1262 - val_loss: 2.3323 Epoch 629/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1258 - val_loss: 2.3208 Epoch 630/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1253 - val_loss: 2.3097 Epoch 631/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 2.2984 Epoch 632/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 2.2874 Epoch 633/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1239 - val_loss: 2.2767 Epoch 634/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1235 - val_loss: 2.2654 Epoch 635/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1230 - val_loss: 2.2545 Epoch 636/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1225 - val_loss: 2.2435 Epoch 637/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.1221 - val_loss: 2.2321 Epoch 638/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1217 - val_loss: 2.2217 Epoch 639/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1213 - val_loss: 2.2106 Epoch 640/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1208 - val_loss: 2.2000 Epoch 641/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1204 - val_loss: 2.1890 Epoch 642/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1199 - val_loss: 2.1783 Epoch 643/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1194 - val_loss: 2.1677 Epoch 644/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1190 - val_loss: 2.1568 Epoch 645/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1186 - val_loss: 2.1462 Epoch 646/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1182 - val_loss: 2.1353 Epoch 647/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1177 - val_loss: 2.1245 Epoch 648/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1173 - val_loss: 2.1141 Epoch 649/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1168 - val_loss: 2.1036 Epoch 650/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1164 - val_loss: 2.0928 Epoch 651/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1160 - val_loss: 2.0823 Epoch 652/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1155 - val_loss: 2.0714 Epoch 653/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1151 - val_loss: 2.0609 Epoch 654/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 2.0502 Epoch 655/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1143 - val_loss: 2.0392 Epoch 656/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1138 - val_loss: 2.0286 Epoch 657/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1134 - val_loss: 2.0182 Epoch 658/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1130 - val_loss: 2.0077 Epoch 659/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1125 - val_loss: 1.9972 Epoch 660/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1121 - val_loss: 1.9869 Epoch 661/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1117 - val_loss: 1.9767 Epoch 662/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1113 - val_loss: 1.9659 Epoch 663/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1109 - val_loss: 1.9554 Epoch 664/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1104 - val_loss: 1.9449 Epoch 665/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1100 - val_loss: 1.9350 Epoch 666/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1096 - val_loss: 1.9250 Epoch 667/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1091 - val_loss: 1.9149 Epoch 668/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1087 - val_loss: 1.9043 Epoch 669/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 1.8937 Epoch 670/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1079 - val_loss: 1.8833 Epoch 671/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1075 - val_loss: 1.8730 Epoch 672/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1071 - val_loss: 1.8626 Epoch 673/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1066 - val_loss: 1.8525 Epoch 674/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1062 - val_loss: 1.8425 Epoch 675/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1058 - val_loss: 1.8325 Epoch 676/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1054 - val_loss: 1.8222 Epoch 677/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1050 - val_loss: 1.8121 Epoch 678/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1045 - val_loss: 1.8023 Epoch 679/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1042 - val_loss: 1.7921 Epoch 680/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1038 - val_loss: 1.7819 Epoch 681/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1033 - val_loss: 1.7721 Epoch 682/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1029 - val_loss: 1.7620 Epoch 683/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1025 - val_loss: 1.7520 Epoch 684/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1021 - val_loss: 1.7421 Epoch 685/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 1.7322 Epoch 686/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1013 - val_loss: 1.7224 Epoch 687/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1010 - val_loss: 1.7127 Epoch 688/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1006 - val_loss: 1.7031 Epoch 689/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1002 - val_loss: 1.6937 Epoch 690/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0998 - val_loss: 1.6841 Epoch 691/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0994 - val_loss: 1.6741 Epoch 692/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0990 - val_loss: 1.6640 Epoch 693/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0986 - val_loss: 1.6543 Epoch 694/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0982 - val_loss: 1.6454 Epoch 695/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0979 - val_loss: 1.6361 Epoch 696/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0975 - val_loss: 1.6266 Epoch 697/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0971 - val_loss: 1.6170 Epoch 698/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0967 - val_loss: 1.6073 Epoch 699/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0963 - val_loss: 1.5979 Epoch 700/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0960 - val_loss: 1.5886 Epoch 701/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0956 - val_loss: 1.5796 Epoch 702/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0952 - val_loss: 1.5706 Epoch 703/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0949 - val_loss: 1.5613 Epoch 704/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0945 - val_loss: 1.5523 Epoch 705/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0941 - val_loss: 1.5429 Epoch 706/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0938 - val_loss: 1.5336 Epoch 707/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0934 - val_loss: 1.5247 Epoch 708/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0930 - val_loss: 1.5156 Epoch 709/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0926 - val_loss: 1.5068 Epoch 710/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0923 - val_loss: 1.4980 Epoch 711/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0919 - val_loss: 1.4888 Epoch 712/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0915 - val_loss: 1.4796 Epoch 713/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0912 - val_loss: 1.4702 Epoch 714/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0908 - val_loss: 1.4612 Epoch 715/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0904 - val_loss: 1.4523 Epoch 716/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0901 - val_loss: 1.4433 Epoch 717/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0897 - val_loss: 1.4345 Epoch 718/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0894 - val_loss: 1.4258 Epoch 719/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0890 - val_loss: 1.4173 Epoch 720/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0886 - val_loss: 1.4085 Epoch 721/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0883 - val_loss: 1.3997 Epoch 722/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0879 - val_loss: 1.3913 Epoch 723/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0876 - val_loss: 1.3827 Epoch 724/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0873 - val_loss: 1.3742 Epoch 725/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0869 - val_loss: 1.3656 Epoch 726/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0866 - val_loss: 1.3571 Epoch 727/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0862 - val_loss: 1.3485 Epoch 728/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0859 - val_loss: 1.3402 Epoch 729/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0856 - val_loss: 1.3317 Epoch 730/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0852 - val_loss: 1.3236 Epoch 731/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0849 - val_loss: 1.3152 Epoch 732/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0845 - val_loss: 1.3069 Epoch 733/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0842 - val_loss: 1.2986 Epoch 734/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0839 - val_loss: 1.2901 Epoch 735/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0835 - val_loss: 1.2821 Epoch 736/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0832 - val_loss: 1.2735 Epoch 737/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0828 - val_loss: 1.2655 Epoch 738/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0825 - val_loss: 1.2574 Epoch 739/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0822 - val_loss: 1.2491 Epoch 740/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0819 - val_loss: 1.2410 Epoch 741/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0815 - val_loss: 1.2326 Epoch 742/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0812 - val_loss: 1.2245 Epoch 743/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0809 - val_loss: 1.2162 Epoch 744/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0806 - val_loss: 1.2082 Epoch 745/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0802 - val_loss: 1.2005 Epoch 746/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0799 - val_loss: 1.1927 Epoch 747/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0796 - val_loss: 1.1848 Epoch 748/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0793 - val_loss: 1.1771 Epoch 749/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0790 - val_loss: 1.1692 Epoch 750/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0787 - val_loss: 1.1613 Epoch 751/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0784 - val_loss: 1.1534 Epoch 752/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0780 - val_loss: 1.1456 Epoch 753/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0778 - val_loss: 1.1380 Epoch 754/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0774 - val_loss: 1.1301 Epoch 755/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0771 - val_loss: 1.1222 Epoch 756/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0768 - val_loss: 1.1146 Epoch 757/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0765 - val_loss: 1.1073 Epoch 758/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0762 - val_loss: 1.0998 Epoch 759/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0759 - val_loss: 1.0928 Epoch 760/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0756 - val_loss: 1.0854 Epoch 761/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0754 - val_loss: 1.0781 Epoch 762/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 1.0707 Epoch 763/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 1.0630 Epoch 764/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0745 - val_loss: 1.0560 Epoch 765/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0742 - val_loss: 1.0486 Epoch 766/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0739 - val_loss: 1.0412 Epoch 767/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0736 - val_loss: 1.0341 Epoch 768/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 1.0270 Epoch 769/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0730 - val_loss: 1.0200 Epoch 770/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0727 - val_loss: 1.0131 Epoch 771/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0725 - val_loss: 1.0066 Epoch 772/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0722 - val_loss: 0.9998 Epoch 773/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.9929 Epoch 774/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.9858 Epoch 775/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0713 - val_loss: 0.9790 Epoch 776/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.9719 Epoch 777/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0708 - val_loss: 0.9653 Epoch 778/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0705 - val_loss: 0.9583 Epoch 779/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0702 - val_loss: 0.9515 Epoch 780/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0700 - val_loss: 0.9449 Epoch 781/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.9380 Epoch 782/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0694 - val_loss: 0.9312 Epoch 783/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.9246 Epoch 784/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0689 - val_loss: 0.9182 Epoch 785/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0686 - val_loss: 0.9118 Epoch 786/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0684 - val_loss: 0.9054 Epoch 787/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8986 Epoch 788/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0679 - val_loss: 0.8915 Epoch 789/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0676 - val_loss: 0.8849 Epoch 790/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0673 - val_loss: 0.8786 Epoch 791/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0671 - val_loss: 0.8724 Epoch 792/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0668 - val_loss: 0.8663 Epoch 793/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0666 - val_loss: 0.8602 Epoch 794/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0663 - val_loss: 0.8539 Epoch 795/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0661 - val_loss: 0.8475 Epoch 796/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.8416 Epoch 797/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.8354 Epoch 798/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0653 - val_loss: 0.8295 Epoch 799/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.8233 Epoch 800/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0648 - val_loss: 0.8172 Epoch 801/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0646 - val_loss: 0.8115 Epoch 802/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0644 - val_loss: 0.8055 Epoch 803/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.7994 Epoch 804/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7937 Epoch 805/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0636 - val_loss: 0.7877 Epoch 806/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0634 - val_loss: 0.7819 Epoch 807/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.7761 Epoch 808/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0629 - val_loss: 0.7702 Epoch 809/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0627 - val_loss: 0.7640 Epoch 810/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0625 - val_loss: 0.7584 Epoch 811/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0622 - val_loss: 0.7529 Epoch 812/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0620 - val_loss: 0.7469 Epoch 813/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0618 - val_loss: 0.7411 Epoch 814/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0615 - val_loss: 0.7350 Epoch 815/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0613 - val_loss: 0.7292 Epoch 816/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.7238 Epoch 817/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0609 - val_loss: 0.7180 Epoch 818/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0606 - val_loss: 0.7123 Epoch 819/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0604 - val_loss: 0.7069 Epoch 820/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.7016 Epoch 821/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0599 - val_loss: 0.6959 Epoch 822/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.6902 Epoch 823/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0595 - val_loss: 0.6845 Epoch 824/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0592 - val_loss: 0.6790 Epoch 825/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6735 Epoch 826/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.6681 Epoch 827/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0586 - val_loss: 0.6627 Epoch 828/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.6575 Epoch 829/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0582 - val_loss: 0.6524 Epoch 830/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0580 - val_loss: 0.6472 Epoch 831/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.6421 Epoch 832/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.6370 Epoch 833/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6318 Epoch 834/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0572 - val_loss: 0.6265 Epoch 835/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0569 - val_loss: 0.6211 Epoch 836/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0567 - val_loss: 0.6162 Epoch 837/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.6111 Epoch 838/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0563 - val_loss: 0.6061 Epoch 839/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0561 - val_loss: 0.6013 Epoch 840/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0559 - val_loss: 0.5965 Epoch 841/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5917 Epoch 842/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0556 - val_loss: 0.5869 Epoch 843/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0554 - val_loss: 0.5819 Epoch 844/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.5771 Epoch 845/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.5722 Epoch 846/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.5672 Epoch 847/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0546 - val_loss: 0.5627 Epoch 848/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0544 - val_loss: 0.5580 Epoch 849/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5534 Epoch 850/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0540 - val_loss: 0.5487 Epoch 851/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0539 - val_loss: 0.5441 Epoch 852/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0537 - val_loss: 0.5395 Epoch 853/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.5350 Epoch 854/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0533 - val_loss: 0.5304 Epoch 855/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0531 - val_loss: 0.5259 Epoch 856/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5214 Epoch 857/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0528 - val_loss: 0.5170 Epoch 858/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5125 Epoch 859/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5082 Epoch 860/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0522 - val_loss: 0.5039 Epoch 861/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0521 - val_loss: 0.4995 Epoch 862/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.4952 Epoch 863/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4909 Epoch 864/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.4865 Epoch 865/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0514 - val_loss: 0.4823 Epoch 866/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4780 Epoch 867/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.4738 Epoch 868/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4696 Epoch 869/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.4655 Epoch 870/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0505 - val_loss: 0.4615 Epoch 871/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4575 Epoch 872/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0502 - val_loss: 0.4533 Epoch 873/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4495 Epoch 874/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.4459 Epoch 875/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0498 - val_loss: 0.4420 Epoch 876/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.4379 Epoch 877/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0494 - val_loss: 0.4340 Epoch 878/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.4303 Epoch 879/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4264 Epoch 880/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.4226 Epoch 881/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0489 - val_loss: 0.4190 Epoch 882/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0487 - val_loss: 0.4154 Epoch 883/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4118 Epoch 884/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4082 Epoch 885/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0483 - val_loss: 0.4049 Epoch 886/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.4012 Epoch 887/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.3976 Epoch 888/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3939 Epoch 889/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0477 - val_loss: 0.3903 Epoch 890/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3869 Epoch 891/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.3831 Epoch 892/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0473 - val_loss: 0.3796 Epoch 893/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3762 Epoch 894/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3729 Epoch 895/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3695 Epoch 896/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0467 - val_loss: 0.3662 Epoch 897/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3629 Epoch 898/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.3596 Epoch 899/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3563 Epoch 900/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.3530 Epoch 901/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0461 - val_loss: 0.3499 Epoch 902/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3468 Epoch 903/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0458 - val_loss: 0.3435 Epoch 904/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0457 - val_loss: 0.3403 Epoch 905/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0456 - val_loss: 0.3371 Epoch 906/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0455 - val_loss: 0.3340 Epoch 907/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0453 - val_loss: 0.3310 Epoch 908/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3280 Epoch 909/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3251 Epoch 910/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.3219 Epoch 911/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3189 Epoch 912/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0448 - val_loss: 0.3159 Epoch 913/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3130 Epoch 914/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3100 Epoch 915/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.3072 Epoch 916/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0443 - val_loss: 0.3044 Epoch 917/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0442 - val_loss: 0.3015 Epoch 918/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2987 Epoch 919/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0440 - val_loss: 0.2960 Epoch 920/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.2933 Epoch 921/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2905 Epoch 922/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0437 - val_loss: 0.2878 Epoch 923/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2853 Epoch 924/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2827 Epoch 925/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0433 - val_loss: 0.2801 Epoch 926/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2777 Epoch 927/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2752 Epoch 928/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2727 Epoch 929/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2703 Epoch 930/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.2679 Epoch 931/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2655 Epoch 932/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.2633 Epoch 933/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2607 Epoch 934/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.2584 Epoch 935/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2559 Epoch 936/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.2534 Epoch 937/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2510 Epoch 938/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2486 Epoch 939/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0420 - val_loss: 0.2461 Epoch 940/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0419 - val_loss: 0.2437 Epoch 941/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2413 Epoch 942/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2390 Epoch 943/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2367 Epoch 944/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2345 Epoch 945/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2322 Epoch 946/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2300 Epoch 947/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2276 Epoch 948/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2254 Epoch 949/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0411 - val_loss: 0.2232 Epoch 950/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2211 Epoch 951/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2190 Epoch 952/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2170 Epoch 953/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2151 Epoch 954/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2131 Epoch 955/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2111 Epoch 956/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2091 Epoch 957/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2071 Epoch 958/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2051 Epoch 959/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2030 Epoch 960/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0402 - val_loss: 0.2010 Epoch 961/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1989 Epoch 962/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1972 Epoch 963/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1953 Epoch 964/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1932 Epoch 965/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1913 Epoch 966/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1894 Epoch 967/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1875 Epoch 968/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0396 - val_loss: 0.1856 Epoch 969/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0396 - val_loss: 0.1839 Epoch 970/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1821 Epoch 971/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1802 Epoch 972/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1784 Epoch 973/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0393 - val_loss: 0.1767 Epoch 974/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1751 Epoch 975/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1734 Epoch 976/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1719 Epoch 977/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1703 Epoch 978/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1687 Epoch 979/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1670 Epoch 980/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0388 - val_loss: 0.1656 Epoch 981/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1639 Epoch 982/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1624 Epoch 983/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1608 Epoch 984/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1592 Epoch 985/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1577 Epoch 986/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0385 - val_loss: 0.1563 Epoch 987/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1548 Epoch 988/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1533 Epoch 989/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1518 Epoch 990/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1504 Epoch 991/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1488 Epoch 992/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1475 Epoch 993/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1462 Epoch 994/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1448 Epoch 995/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0380 - val_loss: 0.1434 Epoch 996/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1421 Epoch 997/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1407 Epoch 998/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.0378 - val_loss: 0.1393 Epoch 999/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1380 Epoch 1000/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1367 Epoch 1001/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1355 Epoch 1002/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1343 Epoch 1003/2000 6/6 [==============================] - ETA: 0s - loss: 0.032 - 0s 4ms/step - loss: 0.0376 - val_loss: 0.1331 Epoch 1004/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1320 Epoch 1005/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1308 Epoch 1006/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1296 Epoch 1007/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1284 Epoch 1008/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1272 Epoch 1009/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1261 Epoch 1010/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1250 Epoch 1011/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1239 Epoch 1012/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1229 Epoch 1013/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1218 Epoch 1014/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1207 Epoch 1015/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1196 Epoch 1016/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1185 Epoch 1017/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1174 Epoch 1018/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1163 Epoch 1019/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1153 Epoch 1020/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1143 Epoch 1021/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1133 Epoch 1022/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1123 Epoch 1023/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1114 Epoch 1024/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0367 - val_loss: 0.1104 Epoch 1025/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1093 Epoch 1026/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1082 Epoch 1027/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1073 Epoch 1028/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1063 Epoch 1029/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1054 Epoch 1030/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1044 Epoch 1031/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0365 - val_loss: 0.1036 Epoch 1032/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1027 Epoch 1033/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1019 Epoch 1034/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1009 Epoch 1035/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.1001 Epoch 1036/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0992 Epoch 1037/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0983 Epoch 1038/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0975 Epoch 1039/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0965 Epoch 1040/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0956 Epoch 1041/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0948 Epoch 1042/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0940 Epoch 1043/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0932 Epoch 1044/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0925 Epoch 1045/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0917 Epoch 1046/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0910 Epoch 1047/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0360 - val_loss: 0.0902 Epoch 1048/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0894 Epoch 1049/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0886 Epoch 1050/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0878 Epoch 1051/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0870 Epoch 1052/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0863 Epoch 1053/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0857 Epoch 1054/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0851 Epoch 1055/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0844 Epoch 1056/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0838 Epoch 1057/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0831 Epoch 1058/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0825 Epoch 1059/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0819 Epoch 1060/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0813 Epoch 1061/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0807 Epoch 1062/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0801 Epoch 1063/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0795 Epoch 1064/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0789 Epoch 1065/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0783 Epoch 1066/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0777 Epoch 1067/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0771 Epoch 1068/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0765 Epoch 1069/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0759 Epoch 1070/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0753 Epoch 1071/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0748 Epoch 1072/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0742 Epoch 1073/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0736 Epoch 1074/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0730 Epoch 1075/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0724 Epoch 1076/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0719 Epoch 1077/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0714 Epoch 1078/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0710 Epoch 1079/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0705 Epoch 1080/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0700 Epoch 1081/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0695 Epoch 1082/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0690 Epoch 1083/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0685 Epoch 1084/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0351 - val_loss: 0.0680 Epoch 1085/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0675 Epoch 1086/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0671 Epoch 1087/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0667 Epoch 1088/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0662 Epoch 1089/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0659 Epoch 1090/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0350 - val_loss: 0.0655 Epoch 1091/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0651 Epoch 1092/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0646 Epoch 1093/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0642 Epoch 1094/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0637 Epoch 1095/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0633 Epoch 1096/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0629 Epoch 1097/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0625 Epoch 1098/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0622 Epoch 1099/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0618 Epoch 1100/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0614 Epoch 1101/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0611 Epoch 1102/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 0.0607 Epoch 1103/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0604 Epoch 1104/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0601 Epoch 1105/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0598 Epoch 1106/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0595 Epoch 1107/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0591 Epoch 1108/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0587 Epoch 1109/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0585 Epoch 1110/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0582 Epoch 1111/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0579 Epoch 1112/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0575 Epoch 1113/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0572 Epoch 1114/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0570 Epoch 1115/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0567 Epoch 1116/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0563 Epoch 1117/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0560 Epoch 1118/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0557 Epoch 1119/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0555 Epoch 1120/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0551 Epoch 1121/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0548 Epoch 1122/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0545 Epoch 1123/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0542 Epoch 1124/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0539 Epoch 1125/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0538 Epoch 1126/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0536 Epoch 1127/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0533 Epoch 1128/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0531 Epoch 1129/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0528 Epoch 1130/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0526 Epoch 1131/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0524 Epoch 1132/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0522 Epoch 1133/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0520 Epoch 1134/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0518 Epoch 1135/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0515 Epoch 1136/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0513 Epoch 1137/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0511 Epoch 1138/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0509 Epoch 1139/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0507 Epoch 1140/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0505 Epoch 1141/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0503 Epoch 1142/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0502 Epoch 1143/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0500 Epoch 1144/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0498 Epoch 1145/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0496 Epoch 1146/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0494 Epoch 1147/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0492 Epoch 1148/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0490 Epoch 1149/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0488 Epoch 1150/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0345 - val_loss: 0.0486 Epoch 1151/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0484 Epoch 1152/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0482 Epoch 1153/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0481 Epoch 1154/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0480 Epoch 1155/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0478 Epoch 1156/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0476 Epoch 1157/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0474 Epoch 1158/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0472 Epoch 1159/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0471 Epoch 1160/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0470 Epoch 1161/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0468 Epoch 1162/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0467 Epoch 1163/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0465 Epoch 1164/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463 Epoch 1165/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462 Epoch 1166/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0461 Epoch 1167/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0460 Epoch 1168/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0459 Epoch 1169/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458 Epoch 1170/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456 Epoch 1171/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0455 Epoch 1172/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454 Epoch 1173/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0453 Epoch 1174/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452 Epoch 1175/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0451 Epoch 1176/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0450 Epoch 1177/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0449 Epoch 1178/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448 Epoch 1179/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0447 Epoch 1180/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446 Epoch 1181/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444 Epoch 1182/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443 Epoch 1183/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442 Epoch 1184/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441 Epoch 1185/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440 Epoch 1186/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439 Epoch 1187/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438 Epoch 1188/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0436 Epoch 1189/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435 Epoch 1190/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0435 Epoch 1191/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434 Epoch 1192/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433 Epoch 1193/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432 Epoch 1194/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431 Epoch 1195/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430 Epoch 1196/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0430 Epoch 1197/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0429 Epoch 1198/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428 Epoch 1199/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0428 Epoch 1200/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427 Epoch 1201/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426 Epoch 1202/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425 Epoch 1203/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425 Epoch 1204/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424 Epoch 1205/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423 Epoch 1206/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422 Epoch 1207/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422 Epoch 1208/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421 Epoch 1209/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421 Epoch 1210/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420 Epoch 1211/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419 Epoch 1212/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419 Epoch 1213/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0419 Epoch 1214/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0418 Epoch 1215/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0418 Epoch 1216/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417 Epoch 1217/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417 Epoch 1218/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416 Epoch 1219/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0415 Epoch 1220/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415 Epoch 1221/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414 Epoch 1222/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414 Epoch 1223/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413 Epoch 1224/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412 Epoch 1225/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412 Epoch 1226/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411 Epoch 1227/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411 Epoch 1228/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411 Epoch 1229/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410 Epoch 1230/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410 Epoch 1231/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409 Epoch 1232/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409 Epoch 1233/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408 Epoch 1234/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0408 Epoch 1235/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407 Epoch 1236/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407 Epoch 1237/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406 Epoch 1238/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406 Epoch 1239/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405 Epoch 1240/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405 Epoch 1241/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405 Epoch 1242/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404 Epoch 1243/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404 Epoch 1244/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404 Epoch 1245/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1246/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1247/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1248/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1249/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1250/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1251/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1252/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1253/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1254/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1255/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400 Epoch 1256/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400 Epoch 1257/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0400 Epoch 1258/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400 Epoch 1259/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1260/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1261/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1262/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1263/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1264/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1265/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1266/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1267/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1268/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1269/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1270/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1271/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1272/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1273/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1274/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1275/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1276/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1277/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1278/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1279/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1280/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1281/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1282/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1283/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1284/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1285/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1286/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1287/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1288/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1289/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1290/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1291/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1292/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1293/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1294/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1295/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1296/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1297/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1298/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1299/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1300/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1301/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1302/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1303/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1304/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1305/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1306/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1307/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1308/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1309/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1310/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1311/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1312/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1313/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1314/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1315/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1316/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1317/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1318/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1319/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1320/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1321/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1322/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1323/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1324/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1325/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1326/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1327/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1328/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1329/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1330/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1331/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1332/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1333/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1334/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1335/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1336/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1337/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1338/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1339/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1340/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1341/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1342/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1343/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1344/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1345/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1346/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1347/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1348/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1349/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1350/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1351/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1352/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1353/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1354/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1355/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1356/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1357/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1358/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1359/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1360/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1361/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1362/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1363/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1364/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1365/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1366/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1367/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1368/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1369/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1370/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1371/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1372/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1373/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1374/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1375/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1376/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1377/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1378/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1379/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1380/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1381/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1382/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1383/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1384/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1385/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1386/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1387/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1388/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1389/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1390/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1391/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1392/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1393/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1394/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1395/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1396/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1397/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1398/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1399/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1400/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1401/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1402/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1403/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1404/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1405/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1406/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1407/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1408/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1409/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1410/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1411/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1412/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1413/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1414/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1415/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1416/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1417/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1418/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1419/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1420/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1421/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1422/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1423/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1424/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1425/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1426/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1427/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1428/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1429/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1430/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1431/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1432/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1433/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1434/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1435/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1436/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1437/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1438/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1439/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1440/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1441/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1442/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1443/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1444/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1445/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1446/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1447/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1448/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1449/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1450/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1451/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1452/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1453/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1454/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1455/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1456/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1457/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1458/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1459/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1460/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1461/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1462/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1463/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1464/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1465/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1466/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1467/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1468/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1469/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1470/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1471/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1472/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1473/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1474/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1475/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1476/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1477/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1478/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1479/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1480/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1481/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1482/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1483/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1484/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1485/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1486/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1487/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1488/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1489/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1490/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1491/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1492/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1493/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1494/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1495/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1496/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1497/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1498/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1499/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1500/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1501/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1502/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1503/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1504/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1505/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1506/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1507/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1508/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1509/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1510/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1511/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1512/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1513/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1514/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1515/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1516/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1517/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1518/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1519/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1520/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1521/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1522/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1523/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1524/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1525/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1526/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1527/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1528/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1529/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1530/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1531/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1532/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1533/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1534/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1535/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1536/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1537/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1538/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1539/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1540/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1541/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1542/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1543/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1544/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1545/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1546/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1547/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1548/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1549/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1550/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1551/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1552/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1553/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1554/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1555/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1556/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1557/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1558/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1559/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1560/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1561/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1562/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1563/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1564/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1565/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1566/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1567/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1568/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1569/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1570/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1571/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1572/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1573/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1574/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1575/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1576/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1577/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1578/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1579/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1580/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1581/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1582/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1583/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1584/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1585/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1586/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1587/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1588/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1589/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1590/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1591/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1592/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1593/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1594/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1595/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1596/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1597/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1598/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1599/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1600/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1601/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1602/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1603/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1604/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1605/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1606/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1607/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1608/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1609/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1610/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1611/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1612/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1613/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1614/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1615/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1616/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1617/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1618/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1619/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1620/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1621/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1622/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1623/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1624/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1625/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1626/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1627/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1628/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1629/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1630/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1631/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1632/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1633/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1634/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1635/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1636/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1637/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1638/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1639/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1640/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1641/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1642/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1643/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1644/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1645/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1646/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1647/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1648/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1649/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1650/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1651/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1652/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1653/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1654/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1655/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1656/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1657/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1658/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1659/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1660/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1661/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1662/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1663/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1664/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1665/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1666/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1667/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1668/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1669/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1670/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1671/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1672/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1673/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1674/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1675/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1676/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1677/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1678/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1679/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1680/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1681/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1682/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1683/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1684/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1685/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1686/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1687/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1688/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1689/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1690/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1691/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1692/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1693/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1694/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1695/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1696/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1697/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1698/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1699/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1700/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1701/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1702/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1703/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1704/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1705/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1706/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1707/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1708/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1709/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1710/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1711/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1712/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1713/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1714/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1715/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1716/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1717/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1718/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1719/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1720/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1721/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1722/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1723/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1724/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1725/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1726/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1727/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1728/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1729/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1730/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1731/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1732/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1733/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1734/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1735/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1736/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1737/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1738/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1739/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1740/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1741/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1742/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1743/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1744/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1745/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1746/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1747/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1748/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1749/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1750/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1751/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1752/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1753/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1754/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1755/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1756/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1757/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1758/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1759/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1760/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1761/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1762/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1763/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1764/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1765/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1766/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1767/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1768/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1769/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1770/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1771/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1772/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1773/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1774/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1775/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1776/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1777/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1778/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1779/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1780/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1781/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1782/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1783/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1784/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1785/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1786/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1787/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1788/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1789/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1790/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1791/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1792/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1793/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1794/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1795/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1796/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1797/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1798/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1799/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1800/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1801/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1802/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1803/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1804/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1805/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1806/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1807/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1808/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1809/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1810/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1811/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1812/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1813/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1814/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1815/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1816/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1817/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1818/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1819/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1820/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1821/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1822/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1823/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1824/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1825/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1826/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1827/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1828/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1829/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1830/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1831/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1832/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1833/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1834/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1835/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1836/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1837/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1838/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1839/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1840/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1841/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1842/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1843/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1844/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1845/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1846/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1847/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1848/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1849/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1850/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1851/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1852/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1853/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1854/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1855/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1856/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1857/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1858/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1859/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1860/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1861/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1862/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1863/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1864/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1865/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1866/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1867/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1868/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1869/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1870/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1871/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1872/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1873/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1874/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1875/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1876/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1877/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1878/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1879/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1880/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1881/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1882/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1883/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1884/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1885/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1886/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1887/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1888/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1889/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1890/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1891/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1892/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1893/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1894/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1895/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1896/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1897/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1898/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1899/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1900/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1901/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1902/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1903/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1904/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1905/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1906/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1907/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1908/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1909/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1910/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1911/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1912/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1913/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1914/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1915/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1916/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1917/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1918/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1919/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1920/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1921/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1922/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1923/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1924/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1925/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1926/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1927/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1928/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1929/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1930/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1931/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1932/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1933/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1934/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1935/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1936/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1937/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1938/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1939/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1940/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1941/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1942/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1943/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1944/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1945/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1946/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1947/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1948/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1949/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1950/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1951/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1952/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1953/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1954/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1955/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1956/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1957/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1958/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1959/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1960/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1961/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1962/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1963/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1964/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1965/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1966/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1967/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1968/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1969/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1970/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1971/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1972/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1973/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1974/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1975/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1976/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1977/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1978/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1979/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1980/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1981/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1982/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1983/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1984/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1985/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1986/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1987/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1988/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1989/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1990/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1991/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1992/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1993/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1994/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1995/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1996/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1997/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1998/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1999/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 2000/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 . &lt;keras.callbacks.History at 0x7f708353a650&gt; . . fig, ax = plt.subplots() ax.plot(y,&#39;.&#39;,alpha=0.2) ax.plot(net(X),&#39;--&#39;) with tf.summary.create_file_writer(logdir).as_default(): tf.summary.image(&quot;적합결과시각화&quot;, plot_to_image(fig), step=0) . #%tensorboard --logdir logs --host 0.0.0.0 . - 아래의 코드를 100에폭마다 실행하고 싶다. . fig, ax = plt.subplots() ax.plot(y,&#39;.&#39;,alpha=0.2) ax.plot(net(X),&#39;--&#39;) with tf.summary.create_file_writer(logdir).as_default(): tf.summary.image(&quot;적합결과시각화&quot;, plot_to_image(fig), step=0) . - 일단 net.fit직전까지의 코드를 구현 . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) . - 사용자정의 콜백클래스를 만듬 . class PlotYhat(tf.keras.callbacks.Callback): def on_epoch_begin(self,epoch,logs): # 입력은 무조건 self, epoch, logs를 써야합니다 --&gt; 이 함수안에 에폭이 끝날때마다 할 동작을 정의한다. if epoch % 100 ==0: fig, ax = plt.subplots() ax.plot(y,&#39;.&#39;,alpha=0.2) ax.plot(net(X),&#39;--&#39;) with tf.summary.create_file_writer(logdir).as_default(): tf.summary.image(&quot;적합결과시각화&quot;+str(epoch), plot_to_image(fig), step=0) . - 내가 만든 클래스에서 cb2를 생성 . !rm -rf logs cb1= tf.keras.callbacks.TensorBoard(update_freq=&#39;epoch&#39;,histogram_freq=100) cb2= PlotYhat() net.fit(X,y,epochs=2000, batch_size=100, validation_split=0.45,callbacks=[cb1,cb2]) . Epoch 1/2000 1/6 [====&gt;.........................] - ETA: 0s - loss: 2.9239WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0007s vs `on_train_batch_end` time: 0.0019s). Check your callbacks. 6/6 [==============================] - 0s 6ms/step - loss: 2.8708 - val_loss: 9.1608 Epoch 2/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.8444 - val_loss: 9.1250 Epoch 3/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.8186 - val_loss: 9.0901 Epoch 4/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7930 - val_loss: 9.0545 Epoch 5/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7673 - val_loss: 9.0188 Epoch 6/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7417 - val_loss: 8.9828 Epoch 7/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.7169 - val_loss: 8.9476 Epoch 8/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6913 - val_loss: 8.9134 Epoch 9/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.6667 - val_loss: 8.8786 Epoch 10/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6420 - val_loss: 8.8450 Epoch 11/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.6175 - val_loss: 8.8107 Epoch 12/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5932 - val_loss: 8.7766 Epoch 13/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5690 - val_loss: 8.7433 Epoch 14/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.5453 - val_loss: 8.7103 Epoch 15/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.5214 - val_loss: 8.6774 Epoch 16/2000 6/6 [==============================] - 0s 4ms/step - loss: 2.4980 - val_loss: 8.6453 Epoch 17/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4749 - val_loss: 8.6120 Epoch 18/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4520 - val_loss: 8.5786 Epoch 19/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4293 - val_loss: 8.5454 Epoch 20/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.4065 - val_loss: 8.5130 Epoch 21/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3840 - val_loss: 8.4813 Epoch 22/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3618 - val_loss: 8.4501 Epoch 23/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3398 - val_loss: 8.4176 Epoch 24/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.3182 - val_loss: 8.3857 Epoch 25/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2960 - val_loss: 8.3532 Epoch 26/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2748 - val_loss: 8.3213 Epoch 27/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2534 - val_loss: 8.2902 Epoch 28/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2321 - val_loss: 8.2592 Epoch 29/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.2113 - val_loss: 8.2272 Epoch 30/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1904 - val_loss: 8.1962 Epoch 31/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1696 - val_loss: 8.1648 Epoch 32/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1493 - val_loss: 8.1346 Epoch 33/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1288 - val_loss: 8.1042 Epoch 34/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.1088 - val_loss: 8.0739 Epoch 35/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0887 - val_loss: 8.0442 Epoch 36/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0688 - val_loss: 8.0146 Epoch 37/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0491 - val_loss: 7.9854 Epoch 38/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0297 - val_loss: 7.9553 Epoch 39/2000 6/6 [==============================] - 0s 3ms/step - loss: 2.0102 - val_loss: 7.9261 Epoch 40/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9913 - val_loss: 7.8968 Epoch 41/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9720 - val_loss: 7.8684 Epoch 42/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9533 - val_loss: 7.8398 Epoch 43/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.9346 - val_loss: 7.8111 Epoch 44/2000 6/6 [==============================] - 0s 2ms/step - loss: 1.9162 - val_loss: 7.7827 Epoch 45/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8976 - val_loss: 7.7536 Epoch 46/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8796 - val_loss: 7.7262 Epoch 47/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8613 - val_loss: 7.6986 Epoch 48/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.8437 - val_loss: 7.6714 Epoch 49/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.8257 - val_loss: 7.6441 Epoch 50/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.8083 - val_loss: 7.6171 Epoch 51/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.7911 - val_loss: 7.5898 Epoch 52/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7739 - val_loss: 7.5620 Epoch 53/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7568 - val_loss: 7.5342 Epoch 54/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7399 - val_loss: 7.5075 Epoch 55/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7231 - val_loss: 7.4809 Epoch 56/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.7066 - val_loss: 7.4545 Epoch 57/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6898 - val_loss: 7.4279 Epoch 58/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6734 - val_loss: 7.4019 Epoch 59/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6573 - val_loss: 7.3760 Epoch 60/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6410 - val_loss: 7.3502 Epoch 61/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6251 - val_loss: 7.3247 Epoch 62/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.6095 - val_loss: 7.2996 Epoch 63/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5937 - val_loss: 7.2744 Epoch 64/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.5782 - val_loss: 7.2490 Epoch 65/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5626 - val_loss: 7.2237 Epoch 66/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5476 - val_loss: 7.1979 Epoch 67/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5324 - val_loss: 7.1726 Epoch 68/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.5175 - val_loss: 7.1478 Epoch 69/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.5027 - val_loss: 7.1222 Epoch 70/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4878 - val_loss: 7.0974 Epoch 71/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4732 - val_loss: 7.0722 Epoch 72/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4588 - val_loss: 7.0463 Epoch 73/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4445 - val_loss: 7.0214 Epoch 74/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4301 - val_loss: 6.9963 Epoch 75/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.4162 - val_loss: 6.9712 Epoch 76/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.4024 - val_loss: 6.9467 Epoch 77/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3885 - val_loss: 6.9229 Epoch 78/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3749 - val_loss: 6.8992 Epoch 79/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3612 - val_loss: 6.8755 Epoch 80/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3479 - val_loss: 6.8522 Epoch 81/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3345 - val_loss: 6.8282 Epoch 82/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3213 - val_loss: 6.8048 Epoch 83/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.3083 - val_loss: 6.7817 Epoch 84/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2955 - val_loss: 6.7583 Epoch 85/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2826 - val_loss: 6.7351 Epoch 86/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2700 - val_loss: 6.7123 Epoch 87/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2574 - val_loss: 6.6893 Epoch 88/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2449 - val_loss: 6.6665 Epoch 89/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2327 - val_loss: 6.6433 Epoch 90/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2205 - val_loss: 6.6207 Epoch 91/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.2085 - val_loss: 6.5983 Epoch 92/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1964 - val_loss: 6.5757 Epoch 93/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1844 - val_loss: 6.5534 Epoch 94/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1727 - val_loss: 6.5304 Epoch 95/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.1610 - val_loss: 6.5080 Epoch 96/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1494 - val_loss: 6.4857 Epoch 97/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.1381 - val_loss: 6.4647 Epoch 98/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1267 - val_loss: 6.4424 Epoch 99/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.1155 - val_loss: 6.4206 Epoch 100/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.1044 - val_loss: 6.3984 Epoch 101/2000 6/6 [==============================] - 0s 4ms/step - loss: 1.0934 - val_loss: 6.3769 Epoch 102/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0824 - val_loss: 6.3556 Epoch 103/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0716 - val_loss: 6.3349 Epoch 104/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0609 - val_loss: 6.3136 Epoch 105/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0506 - val_loss: 6.2930 Epoch 106/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0399 - val_loss: 6.2721 Epoch 107/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0296 - val_loss: 6.2521 Epoch 108/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0194 - val_loss: 6.2312 Epoch 109/2000 6/6 [==============================] - 0s 3ms/step - loss: 1.0093 - val_loss: 6.2111 Epoch 110/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9992 - val_loss: 6.1916 Epoch 111/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9893 - val_loss: 6.1708 Epoch 112/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.9792 - val_loss: 6.1501 Epoch 113/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9696 - val_loss: 6.1293 Epoch 114/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9600 - val_loss: 6.1098 Epoch 115/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9503 - val_loss: 6.0898 Epoch 116/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9407 - val_loss: 6.0703 Epoch 117/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9313 - val_loss: 6.0498 Epoch 118/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9219 - val_loss: 6.0300 Epoch 119/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9129 - val_loss: 6.0099 Epoch 120/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.9036 - val_loss: 5.9900 Epoch 121/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.8947 - val_loss: 5.9708 Epoch 122/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8857 - val_loss: 5.9519 Epoch 123/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8769 - val_loss: 5.9320 Epoch 124/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8682 - val_loss: 5.9135 Epoch 125/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8595 - val_loss: 5.8947 Epoch 126/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8508 - val_loss: 5.8760 Epoch 127/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8425 - val_loss: 5.8576 Epoch 128/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.8340 - val_loss: 5.8392 Epoch 129/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8256 - val_loss: 5.8206 Epoch 130/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8175 - val_loss: 5.8018 Epoch 131/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8094 - val_loss: 5.7832 Epoch 132/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.8012 - val_loss: 5.7644 Epoch 133/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7932 - val_loss: 5.7460 Epoch 134/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7853 - val_loss: 5.7276 Epoch 135/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7774 - val_loss: 5.7094 Epoch 136/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7698 - val_loss: 5.6909 Epoch 137/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7620 - val_loss: 5.6725 Epoch 138/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7544 - val_loss: 5.6538 Epoch 139/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7470 - val_loss: 5.6356 Epoch 140/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7397 - val_loss: 5.6178 Epoch 141/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7322 - val_loss: 5.6006 Epoch 142/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7251 - val_loss: 5.5828 Epoch 143/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7178 - val_loss: 5.5649 Epoch 144/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7107 - val_loss: 5.5471 Epoch 145/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.7036 - val_loss: 5.5300 Epoch 146/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.6967 - val_loss: 5.5130 Epoch 147/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.6900 - val_loss: 5.4949 Epoch 148/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.6831 - val_loss: 5.4773 Epoch 149/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6765 - val_loss: 5.4599 Epoch 150/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6698 - val_loss: 5.4428 Epoch 151/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6633 - val_loss: 5.4258 Epoch 152/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6568 - val_loss: 5.4088 Epoch 153/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6504 - val_loss: 5.3916 Epoch 154/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6440 - val_loss: 5.3746 Epoch 155/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6376 - val_loss: 5.3580 Epoch 156/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6313 - val_loss: 5.3409 Epoch 157/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6252 - val_loss: 5.3243 Epoch 158/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6191 - val_loss: 5.3078 Epoch 159/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6132 - val_loss: 5.2917 Epoch 160/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6071 - val_loss: 5.2757 Epoch 161/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.6012 - val_loss: 5.2593 Epoch 162/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5954 - val_loss: 5.2427 Epoch 163/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5896 - val_loss: 5.2265 Epoch 164/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5839 - val_loss: 5.2100 Epoch 165/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5783 - val_loss: 5.1937 Epoch 166/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5726 - val_loss: 5.1774 Epoch 167/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5672 - val_loss: 5.1611 Epoch 168/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.5616 - val_loss: 5.1447 Epoch 169/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5562 - val_loss: 5.1285 Epoch 170/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5508 - val_loss: 5.1128 Epoch 171/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5455 - val_loss: 5.0969 Epoch 172/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5403 - val_loss: 5.0809 Epoch 173/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5351 - val_loss: 5.0651 Epoch 174/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5299 - val_loss: 5.0495 Epoch 175/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5248 - val_loss: 5.0335 Epoch 176/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5199 - val_loss: 5.0180 Epoch 177/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5150 - val_loss: 5.0023 Epoch 178/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5100 - val_loss: 4.9867 Epoch 179/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5052 - val_loss: 4.9713 Epoch 180/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.5005 - val_loss: 4.9561 Epoch 181/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4957 - val_loss: 4.9413 Epoch 182/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4911 - val_loss: 4.9263 Epoch 183/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4864 - val_loss: 4.9110 Epoch 184/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4818 - val_loss: 4.8960 Epoch 185/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4773 - val_loss: 4.8807 Epoch 186/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4727 - val_loss: 4.8652 Epoch 187/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4684 - val_loss: 4.8500 Epoch 188/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4640 - val_loss: 4.8352 Epoch 189/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4597 - val_loss: 4.8199 Epoch 190/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4554 - val_loss: 4.8049 Epoch 191/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4511 - val_loss: 4.7902 Epoch 192/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4470 - val_loss: 4.7757 Epoch 193/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4430 - val_loss: 4.7610 Epoch 194/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4388 - val_loss: 4.7460 Epoch 195/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4348 - val_loss: 4.7315 Epoch 196/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4308 - val_loss: 4.7174 Epoch 197/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4270 - val_loss: 4.7021 Epoch 198/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4231 - val_loss: 4.6869 Epoch 199/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4192 - val_loss: 4.6721 Epoch 200/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4154 - val_loss: 4.6581 Epoch 201/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.4117 - val_loss: 4.6438 Epoch 202/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4080 - val_loss: 4.6298 Epoch 203/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4042 - val_loss: 4.6157 Epoch 204/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.4007 - val_loss: 4.6012 Epoch 205/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3971 - val_loss: 4.5873 Epoch 206/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3935 - val_loss: 4.5730 Epoch 207/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3900 - val_loss: 4.5583 Epoch 208/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3865 - val_loss: 4.5440 Epoch 209/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3830 - val_loss: 4.5298 Epoch 210/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3796 - val_loss: 4.5153 Epoch 211/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3763 - val_loss: 4.5010 Epoch 212/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3730 - val_loss: 4.4872 Epoch 213/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3697 - val_loss: 4.4732 Epoch 214/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3665 - val_loss: 4.4590 Epoch 215/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3633 - val_loss: 4.4456 Epoch 216/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3602 - val_loss: 4.4320 Epoch 217/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3571 - val_loss: 4.4182 Epoch 218/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3540 - val_loss: 4.4044 Epoch 219/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3510 - val_loss: 4.3908 Epoch 220/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3480 - val_loss: 4.3772 Epoch 221/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3451 - val_loss: 4.3643 Epoch 222/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3422 - val_loss: 4.3514 Epoch 223/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3392 - val_loss: 4.3379 Epoch 224/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3364 - val_loss: 4.3246 Epoch 225/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3336 - val_loss: 4.3112 Epoch 226/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3309 - val_loss: 4.2979 Epoch 227/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3281 - val_loss: 4.2849 Epoch 228/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3255 - val_loss: 4.2719 Epoch 229/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3228 - val_loss: 4.2590 Epoch 230/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3202 - val_loss: 4.2463 Epoch 231/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3176 - val_loss: 4.2340 Epoch 232/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3150 - val_loss: 4.2210 Epoch 233/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3125 - val_loss: 4.2083 Epoch 234/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3100 - val_loss: 4.1954 Epoch 235/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3075 - val_loss: 4.1827 Epoch 236/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3051 - val_loss: 4.1695 Epoch 237/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.3026 - val_loss: 4.1566 Epoch 238/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.3002 - val_loss: 4.1435 Epoch 239/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2978 - val_loss: 4.1301 Epoch 240/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2955 - val_loss: 4.1168 Epoch 241/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2931 - val_loss: 4.1040 Epoch 242/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2908 - val_loss: 4.0912 Epoch 243/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2886 - val_loss: 4.0783 Epoch 244/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2864 - val_loss: 4.0650 Epoch 245/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2842 - val_loss: 4.0519 Epoch 246/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2820 - val_loss: 4.0394 Epoch 247/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2799 - val_loss: 4.0260 Epoch 248/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2778 - val_loss: 4.0132 Epoch 249/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2756 - val_loss: 4.0004 Epoch 250/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2736 - val_loss: 3.9875 Epoch 251/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2716 - val_loss: 3.9748 Epoch 252/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2695 - val_loss: 3.9623 Epoch 253/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2675 - val_loss: 3.9500 Epoch 254/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2656 - val_loss: 3.9379 Epoch 255/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2637 - val_loss: 3.9257 Epoch 256/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2618 - val_loss: 3.9131 Epoch 257/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2598 - val_loss: 3.9008 Epoch 258/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2581 - val_loss: 3.8884 Epoch 259/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2562 - val_loss: 3.8763 Epoch 260/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2544 - val_loss: 3.8639 Epoch 261/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2526 - val_loss: 3.8518 Epoch 262/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2508 - val_loss: 3.8400 Epoch 263/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2491 - val_loss: 3.8280 Epoch 264/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2474 - val_loss: 3.8163 Epoch 265/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2457 - val_loss: 3.8043 Epoch 266/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2440 - val_loss: 3.7922 Epoch 267/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2424 - val_loss: 3.7796 Epoch 268/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2407 - val_loss: 3.7679 Epoch 269/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2391 - val_loss: 3.7561 Epoch 270/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2375 - val_loss: 3.7440 Epoch 271/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2358 - val_loss: 3.7320 Epoch 272/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2342 - val_loss: 3.7200 Epoch 273/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2327 - val_loss: 3.7085 Epoch 274/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2311 - val_loss: 3.6968 Epoch 275/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2296 - val_loss: 3.6854 Epoch 276/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2282 - val_loss: 3.6738 Epoch 277/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2267 - val_loss: 3.6621 Epoch 278/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2252 - val_loss: 3.6506 Epoch 279/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2238 - val_loss: 3.6389 Epoch 280/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2225 - val_loss: 3.6272 Epoch 281/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2210 - val_loss: 3.6159 Epoch 282/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2197 - val_loss: 3.6041 Epoch 283/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2184 - val_loss: 3.5925 Epoch 284/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2170 - val_loss: 3.5810 Epoch 285/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2157 - val_loss: 3.5693 Epoch 286/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2144 - val_loss: 3.5573 Epoch 287/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2132 - val_loss: 3.5455 Epoch 288/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2118 - val_loss: 3.5341 Epoch 289/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2106 - val_loss: 3.5220 Epoch 290/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2093 - val_loss: 3.5100 Epoch 291/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.2080 - val_loss: 3.4983 Epoch 292/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2068 - val_loss: 3.4868 Epoch 293/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2056 - val_loss: 3.4754 Epoch 294/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2044 - val_loss: 3.4642 Epoch 295/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2033 - val_loss: 3.4525 Epoch 296/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2021 - val_loss: 3.4414 Epoch 297/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.2010 - val_loss: 3.4307 Epoch 298/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1998 - val_loss: 3.4204 Epoch 299/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1987 - val_loss: 3.4090 Epoch 300/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1976 - val_loss: 3.3979 Epoch 301/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1966 - val_loss: 3.3865 Epoch 302/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1955 - val_loss: 3.3759 Epoch 303/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1944 - val_loss: 3.3648 Epoch 304/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1934 - val_loss: 3.3537 Epoch 305/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1923 - val_loss: 3.3431 Epoch 306/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1913 - val_loss: 3.3316 Epoch 307/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1903 - val_loss: 3.3203 Epoch 308/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1893 - val_loss: 3.3087 Epoch 309/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1882 - val_loss: 3.2972 Epoch 310/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1873 - val_loss: 3.2862 Epoch 311/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1864 - val_loss: 3.2753 Epoch 312/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1854 - val_loss: 3.2644 Epoch 313/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1844 - val_loss: 3.2537 Epoch 314/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1836 - val_loss: 3.2427 Epoch 315/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1826 - val_loss: 3.2318 Epoch 316/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1817 - val_loss: 3.2210 Epoch 317/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1808 - val_loss: 3.2101 Epoch 318/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1799 - val_loss: 3.1994 Epoch 319/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1790 - val_loss: 3.1886 Epoch 320/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1781 - val_loss: 3.1771 Epoch 321/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1772 - val_loss: 3.1665 Epoch 322/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1764 - val_loss: 3.1559 Epoch 323/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1756 - val_loss: 3.1453 Epoch 324/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1748 - val_loss: 3.1339 Epoch 325/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1739 - val_loss: 3.1228 Epoch 326/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1731 - val_loss: 3.1123 Epoch 327/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1723 - val_loss: 3.1017 Epoch 328/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1716 - val_loss: 3.0913 Epoch 329/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1708 - val_loss: 3.0808 Epoch 330/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1700 - val_loss: 3.0705 Epoch 331/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1692 - val_loss: 3.0602 Epoch 332/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1685 - val_loss: 3.0498 Epoch 333/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1677 - val_loss: 3.0388 Epoch 334/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1670 - val_loss: 3.0285 Epoch 335/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1662 - val_loss: 3.0177 Epoch 336/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1655 - val_loss: 3.0074 Epoch 337/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1648 - val_loss: 2.9975 Epoch 338/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1641 - val_loss: 2.9873 Epoch 339/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1634 - val_loss: 2.9772 Epoch 340/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1627 - val_loss: 2.9667 Epoch 341/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1620 - val_loss: 2.9566 Epoch 342/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1613 - val_loss: 2.9468 Epoch 343/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1607 - val_loss: 2.9365 Epoch 344/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1599 - val_loss: 2.9265 Epoch 345/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1593 - val_loss: 2.9164 Epoch 346/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1586 - val_loss: 2.9060 Epoch 347/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1580 - val_loss: 2.8955 Epoch 348/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1573 - val_loss: 2.8854 Epoch 349/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1567 - val_loss: 2.8753 Epoch 350/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1560 - val_loss: 2.8653 Epoch 351/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1554 - val_loss: 2.8548 Epoch 352/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1548 - val_loss: 2.8449 Epoch 353/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1542 - val_loss: 2.8345 Epoch 354/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1536 - val_loss: 2.8244 Epoch 355/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1530 - val_loss: 2.8139 Epoch 356/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1524 - val_loss: 2.8030 Epoch 357/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1517 - val_loss: 2.7928 Epoch 358/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1512 - val_loss: 2.7827 Epoch 359/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1505 - val_loss: 2.7725 Epoch 360/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1500 - val_loss: 2.7618 Epoch 361/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1494 - val_loss: 2.7521 Epoch 362/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1488 - val_loss: 2.7423 Epoch 363/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1483 - val_loss: 2.7323 Epoch 364/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1477 - val_loss: 2.7222 Epoch 365/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1471 - val_loss: 2.7124 Epoch 366/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1466 - val_loss: 2.7025 Epoch 367/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1460 - val_loss: 2.6925 Epoch 368/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1455 - val_loss: 2.6823 Epoch 369/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1449 - val_loss: 2.6724 Epoch 370/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1444 - val_loss: 2.6625 Epoch 371/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1438 - val_loss: 2.6529 Epoch 372/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1433 - val_loss: 2.6425 Epoch 373/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1428 - val_loss: 2.6325 Epoch 374/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1422 - val_loss: 2.6229 Epoch 375/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1417 - val_loss: 2.6133 Epoch 376/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1412 - val_loss: 2.6035 Epoch 377/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1407 - val_loss: 2.5939 Epoch 378/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1402 - val_loss: 2.5842 Epoch 379/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1397 - val_loss: 2.5744 Epoch 380/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1392 - val_loss: 2.5642 Epoch 381/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1387 - val_loss: 2.5543 Epoch 382/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1382 - val_loss: 2.5446 Epoch 383/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1378 - val_loss: 2.5352 Epoch 384/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1372 - val_loss: 2.5256 Epoch 385/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1368 - val_loss: 2.5152 Epoch 386/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1363 - val_loss: 2.5051 Epoch 387/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1358 - val_loss: 2.4956 Epoch 388/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1353 - val_loss: 2.4856 Epoch 389/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1348 - val_loss: 2.4758 Epoch 390/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1344 - val_loss: 2.4662 Epoch 391/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1339 - val_loss: 2.4572 Epoch 392/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1335 - val_loss: 2.4477 Epoch 393/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1330 - val_loss: 2.4390 Epoch 394/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1325 - val_loss: 2.4299 Epoch 395/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1321 - val_loss: 2.4209 Epoch 396/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1316 - val_loss: 2.4115 Epoch 397/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1312 - val_loss: 2.4028 Epoch 398/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1307 - val_loss: 2.3940 Epoch 399/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1303 - val_loss: 2.3844 Epoch 400/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1299 - val_loss: 2.3751 Epoch 401/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1295 - val_loss: 2.3659 Epoch 402/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1290 - val_loss: 2.3568 Epoch 403/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1286 - val_loss: 2.3472 Epoch 404/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1282 - val_loss: 2.3378 Epoch 405/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1277 - val_loss: 2.3288 Epoch 406/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1273 - val_loss: 2.3193 Epoch 407/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1269 - val_loss: 2.3105 Epoch 408/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1265 - val_loss: 2.3011 Epoch 409/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1261 - val_loss: 2.2917 Epoch 410/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1256 - val_loss: 2.2824 Epoch 411/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1252 - val_loss: 2.2731 Epoch 412/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1248 - val_loss: 2.2642 Epoch 413/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1244 - val_loss: 2.2553 Epoch 414/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1240 - val_loss: 2.2461 Epoch 415/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1236 - val_loss: 2.2373 Epoch 416/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1232 - val_loss: 2.2284 Epoch 417/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1228 - val_loss: 2.2194 Epoch 418/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1224 - val_loss: 2.2105 Epoch 419/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1220 - val_loss: 2.2017 Epoch 420/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1216 - val_loss: 2.1927 Epoch 421/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1212 - val_loss: 2.1841 Epoch 422/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1208 - val_loss: 2.1749 Epoch 423/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1205 - val_loss: 2.1657 Epoch 424/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1200 - val_loss: 2.1567 Epoch 425/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1196 - val_loss: 2.1476 Epoch 426/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1192 - val_loss: 2.1388 Epoch 427/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1189 - val_loss: 2.1294 Epoch 428/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1185 - val_loss: 2.1209 Epoch 429/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1181 - val_loss: 2.1120 Epoch 430/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1177 - val_loss: 2.1027 Epoch 431/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1173 - val_loss: 2.0937 Epoch 432/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1170 - val_loss: 2.0851 Epoch 433/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1166 - val_loss: 2.0769 Epoch 434/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1162 - val_loss: 2.0680 Epoch 435/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1158 - val_loss: 2.0597 Epoch 436/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1154 - val_loss: 2.0511 Epoch 437/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1151 - val_loss: 2.0421 Epoch 438/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1147 - val_loss: 2.0333 Epoch 439/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1143 - val_loss: 2.0251 Epoch 440/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1140 - val_loss: 2.0163 Epoch 441/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1136 - val_loss: 2.0074 Epoch 442/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1133 - val_loss: 1.9985 Epoch 443/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1129 - val_loss: 1.9900 Epoch 444/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1125 - val_loss: 1.9811 Epoch 445/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1121 - val_loss: 1.9727 Epoch 446/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1118 - val_loss: 1.9641 Epoch 447/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1114 - val_loss: 1.9554 Epoch 448/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1111 - val_loss: 1.9468 Epoch 449/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1107 - val_loss: 1.9385 Epoch 450/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1104 - val_loss: 1.9299 Epoch 451/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1100 - val_loss: 1.9216 Epoch 452/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1097 - val_loss: 1.9130 Epoch 453/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1093 - val_loss: 1.9047 Epoch 454/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1090 - val_loss: 1.8963 Epoch 455/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1086 - val_loss: 1.8880 Epoch 456/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1083 - val_loss: 1.8797 Epoch 457/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1079 - val_loss: 1.8712 Epoch 458/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1076 - val_loss: 1.8626 Epoch 459/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1072 - val_loss: 1.8541 Epoch 460/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1068 - val_loss: 1.8456 Epoch 461/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1065 - val_loss: 1.8371 Epoch 462/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1062 - val_loss: 1.8291 Epoch 463/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1058 - val_loss: 1.8207 Epoch 464/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1055 - val_loss: 1.8123 Epoch 465/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1051 - val_loss: 1.8044 Epoch 466/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1048 - val_loss: 1.7963 Epoch 467/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1044 - val_loss: 1.7885 Epoch 468/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1041 - val_loss: 1.7803 Epoch 469/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1038 - val_loss: 1.7722 Epoch 470/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1034 - val_loss: 1.7646 Epoch 471/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1031 - val_loss: 1.7565 Epoch 472/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1028 - val_loss: 1.7484 Epoch 473/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1025 - val_loss: 1.7402 Epoch 474/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1021 - val_loss: 1.7330 Epoch 475/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1018 - val_loss: 1.7250 Epoch 476/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1015 - val_loss: 1.7168 Epoch 477/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1012 - val_loss: 1.7089 Epoch 478/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1008 - val_loss: 1.7008 Epoch 479/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.1005 - val_loss: 1.6927 Epoch 480/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.1002 - val_loss: 1.6846 Epoch 481/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0999 - val_loss: 1.6768 Epoch 482/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0996 - val_loss: 1.6686 Epoch 483/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0992 - val_loss: 1.6610 Epoch 484/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0989 - val_loss: 1.6530 Epoch 485/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0986 - val_loss: 1.6448 Epoch 486/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0983 - val_loss: 1.6373 Epoch 487/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0980 - val_loss: 1.6292 Epoch 488/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0976 - val_loss: 1.6219 Epoch 489/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0973 - val_loss: 1.6139 Epoch 490/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0970 - val_loss: 1.6062 Epoch 491/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0967 - val_loss: 1.5984 Epoch 492/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0964 - val_loss: 1.5904 Epoch 493/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0961 - val_loss: 1.5827 Epoch 494/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0957 - val_loss: 1.5750 Epoch 495/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0954 - val_loss: 1.5672 Epoch 496/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0951 - val_loss: 1.5593 Epoch 497/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0948 - val_loss: 1.5515 Epoch 498/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0945 - val_loss: 1.5438 Epoch 499/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0942 - val_loss: 1.5357 Epoch 500/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0938 - val_loss: 1.5281 Epoch 501/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0935 - val_loss: 1.5207 Epoch 502/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0932 - val_loss: 1.5133 Epoch 503/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0929 - val_loss: 1.5063 Epoch 504/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0926 - val_loss: 1.4992 Epoch 505/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0923 - val_loss: 1.4920 Epoch 506/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0920 - val_loss: 1.4839 Epoch 507/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0917 - val_loss: 1.4763 Epoch 508/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0914 - val_loss: 1.4686 Epoch 509/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0911 - val_loss: 1.4608 Epoch 510/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0908 - val_loss: 1.4534 Epoch 511/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0905 - val_loss: 1.4456 Epoch 512/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0902 - val_loss: 1.4380 Epoch 513/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0899 - val_loss: 1.4307 Epoch 514/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0896 - val_loss: 1.4231 Epoch 515/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0893 - val_loss: 1.4158 Epoch 516/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0890 - val_loss: 1.4086 Epoch 517/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0887 - val_loss: 1.4011 Epoch 518/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0884 - val_loss: 1.3936 Epoch 519/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0881 - val_loss: 1.3862 Epoch 520/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0878 - val_loss: 1.3786 Epoch 521/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0875 - val_loss: 1.3714 Epoch 522/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0872 - val_loss: 1.3644 Epoch 523/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0869 - val_loss: 1.3574 Epoch 524/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0866 - val_loss: 1.3505 Epoch 525/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0864 - val_loss: 1.3437 Epoch 526/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0861 - val_loss: 1.3364 Epoch 527/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0858 - val_loss: 1.3296 Epoch 528/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0855 - val_loss: 1.3227 Epoch 529/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0852 - val_loss: 1.3158 Epoch 530/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0849 - val_loss: 1.3094 Epoch 531/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0847 - val_loss: 1.3025 Epoch 532/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0844 - val_loss: 1.2952 Epoch 533/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0841 - val_loss: 1.2883 Epoch 534/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0838 - val_loss: 1.2809 Epoch 535/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0835 - val_loss: 1.2734 Epoch 536/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0832 - val_loss: 1.2665 Epoch 537/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0830 - val_loss: 1.2597 Epoch 538/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0827 - val_loss: 1.2527 Epoch 539/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0824 - val_loss: 1.2453 Epoch 540/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0821 - val_loss: 1.2380 Epoch 541/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0818 - val_loss: 1.2310 Epoch 542/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0815 - val_loss: 1.2237 Epoch 543/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0812 - val_loss: 1.2171 Epoch 544/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0810 - val_loss: 1.2103 Epoch 545/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0807 - val_loss: 1.2032 Epoch 546/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0804 - val_loss: 1.1967 Epoch 547/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0801 - val_loss: 1.1904 Epoch 548/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0798 - val_loss: 1.1842 Epoch 549/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0796 - val_loss: 1.1775 Epoch 550/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0793 - val_loss: 1.1708 Epoch 551/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0790 - val_loss: 1.1644 Epoch 552/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0788 - val_loss: 1.1575 Epoch 553/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0785 - val_loss: 1.1506 Epoch 554/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0782 - val_loss: 1.1441 Epoch 555/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0779 - val_loss: 1.1371 Epoch 556/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0777 - val_loss: 1.1302 Epoch 557/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0774 - val_loss: 1.1237 Epoch 558/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0771 - val_loss: 1.1172 Epoch 559/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0769 - val_loss: 1.1104 Epoch 560/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0766 - val_loss: 1.1040 Epoch 561/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0763 - val_loss: 1.0975 Epoch 562/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0761 - val_loss: 1.0910 Epoch 563/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0758 - val_loss: 1.0850 Epoch 564/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0756 - val_loss: 1.0784 Epoch 565/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0753 - val_loss: 1.0720 Epoch 566/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0751 - val_loss: 1.0654 Epoch 567/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0748 - val_loss: 1.0591 Epoch 568/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0746 - val_loss: 1.0525 Epoch 569/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0743 - val_loss: 1.0462 Epoch 570/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0741 - val_loss: 1.0400 Epoch 571/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0738 - val_loss: 1.0336 Epoch 572/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0736 - val_loss: 1.0274 Epoch 573/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0733 - val_loss: 1.0215 Epoch 574/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0731 - val_loss: 1.0155 Epoch 575/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0728 - val_loss: 1.0095 Epoch 576/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0726 - val_loss: 1.0033 Epoch 577/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0723 - val_loss: 0.9969 Epoch 578/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0721 - val_loss: 0.9911 Epoch 579/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0719 - val_loss: 0.9855 Epoch 580/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0716 - val_loss: 0.9800 Epoch 581/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0714 - val_loss: 0.9744 Epoch 582/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0711 - val_loss: 0.9686 Epoch 583/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0709 - val_loss: 0.9626 Epoch 584/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0707 - val_loss: 0.9567 Epoch 585/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0704 - val_loss: 0.9504 Epoch 586/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0702 - val_loss: 0.9447 Epoch 587/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0699 - val_loss: 0.9390 Epoch 588/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0697 - val_loss: 0.9330 Epoch 589/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0695 - val_loss: 0.9273 Epoch 590/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0692 - val_loss: 0.9217 Epoch 591/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0690 - val_loss: 0.9159 Epoch 592/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0688 - val_loss: 0.9097 Epoch 593/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0685 - val_loss: 0.9039 Epoch 594/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0683 - val_loss: 0.8983 Epoch 595/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0681 - val_loss: 0.8930 Epoch 596/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0678 - val_loss: 0.8872 Epoch 597/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0676 - val_loss: 0.8821 Epoch 598/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0674 - val_loss: 0.8764 Epoch 599/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.0672 - val_loss: 0.8709 Epoch 600/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0669 - val_loss: 0.8655 Epoch 601/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0667 - val_loss: 0.8599 Epoch 602/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0665 - val_loss: 0.8545 Epoch 603/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0662 - val_loss: 0.8489 Epoch 604/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0660 - val_loss: 0.8432 Epoch 605/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0658 - val_loss: 0.8379 Epoch 606/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0656 - val_loss: 0.8323 Epoch 607/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0654 - val_loss: 0.8270 Epoch 608/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0651 - val_loss: 0.8218 Epoch 609/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0649 - val_loss: 0.8166 Epoch 610/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0647 - val_loss: 0.8109 Epoch 611/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0645 - val_loss: 0.8056 Epoch 612/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0643 - val_loss: 0.8000 Epoch 613/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0641 - val_loss: 0.7947 Epoch 614/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0639 - val_loss: 0.7896 Epoch 615/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0637 - val_loss: 0.7842 Epoch 616/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0635 - val_loss: 0.7790 Epoch 617/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0632 - val_loss: 0.7744 Epoch 618/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0630 - val_loss: 0.7692 Epoch 619/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0628 - val_loss: 0.7644 Epoch 620/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0626 - val_loss: 0.7597 Epoch 621/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0624 - val_loss: 0.7546 Epoch 622/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0622 - val_loss: 0.7497 Epoch 623/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0620 - val_loss: 0.7450 Epoch 624/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0618 - val_loss: 0.7403 Epoch 625/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0616 - val_loss: 0.7351 Epoch 626/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0614 - val_loss: 0.7301 Epoch 627/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0612 - val_loss: 0.7254 Epoch 628/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0610 - val_loss: 0.7203 Epoch 629/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0608 - val_loss: 0.7157 Epoch 630/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0607 - val_loss: 0.7109 Epoch 631/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0605 - val_loss: 0.7062 Epoch 632/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0603 - val_loss: 0.7016 Epoch 633/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0601 - val_loss: 0.6969 Epoch 634/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0599 - val_loss: 0.6921 Epoch 635/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0597 - val_loss: 0.6874 Epoch 636/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0595 - val_loss: 0.6825 Epoch 637/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0593 - val_loss: 0.6778 Epoch 638/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0591 - val_loss: 0.6729 Epoch 639/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0590 - val_loss: 0.6679 Epoch 640/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0588 - val_loss: 0.6632 Epoch 641/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0586 - val_loss: 0.6585 Epoch 642/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0584 - val_loss: 0.6537 Epoch 643/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0582 - val_loss: 0.6489 Epoch 644/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0580 - val_loss: 0.6440 Epoch 645/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0578 - val_loss: 0.6392 Epoch 646/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0576 - val_loss: 0.6343 Epoch 647/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0574 - val_loss: 0.6300 Epoch 648/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0573 - val_loss: 0.6258 Epoch 649/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0571 - val_loss: 0.6210 Epoch 650/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0569 - val_loss: 0.6163 Epoch 651/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0567 - val_loss: 0.6116 Epoch 652/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0565 - val_loss: 0.6071 Epoch 653/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0564 - val_loss: 0.6025 Epoch 654/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0562 - val_loss: 0.5979 Epoch 655/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0560 - val_loss: 0.5934 Epoch 656/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0558 - val_loss: 0.5893 Epoch 657/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0557 - val_loss: 0.5852 Epoch 658/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0555 - val_loss: 0.5812 Epoch 659/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0553 - val_loss: 0.5772 Epoch 660/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0552 - val_loss: 0.5731 Epoch 661/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0550 - val_loss: 0.5690 Epoch 662/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0548 - val_loss: 0.5651 Epoch 663/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0547 - val_loss: 0.5608 Epoch 664/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0545 - val_loss: 0.5567 Epoch 665/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0543 - val_loss: 0.5525 Epoch 666/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0542 - val_loss: 0.5482 Epoch 667/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0540 - val_loss: 0.5440 Epoch 668/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0538 - val_loss: 0.5399 Epoch 669/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0537 - val_loss: 0.5359 Epoch 670/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0535 - val_loss: 0.5321 Epoch 671/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0533 - val_loss: 0.5282 Epoch 672/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0532 - val_loss: 0.5240 Epoch 673/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0530 - val_loss: 0.5203 Epoch 674/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0529 - val_loss: 0.5162 Epoch 675/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0527 - val_loss: 0.5126 Epoch 676/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0526 - val_loss: 0.5090 Epoch 677/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0524 - val_loss: 0.5054 Epoch 678/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0523 - val_loss: 0.5018 Epoch 679/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0521 - val_loss: 0.4978 Epoch 680/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0519 - val_loss: 0.4943 Epoch 681/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0518 - val_loss: 0.4907 Epoch 682/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0517 - val_loss: 0.4869 Epoch 683/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0515 - val_loss: 0.4836 Epoch 684/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0513 - val_loss: 0.4795 Epoch 685/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0512 - val_loss: 0.4756 Epoch 686/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0510 - val_loss: 0.4716 Epoch 687/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0509 - val_loss: 0.4680 Epoch 688/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0507 - val_loss: 0.4640 Epoch 689/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0506 - val_loss: 0.4605 Epoch 690/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0504 - val_loss: 0.4566 Epoch 691/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0503 - val_loss: 0.4532 Epoch 692/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0501 - val_loss: 0.4497 Epoch 693/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0500 - val_loss: 0.4460 Epoch 694/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0499 - val_loss: 0.4424 Epoch 695/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0497 - val_loss: 0.4388 Epoch 696/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0496 - val_loss: 0.4354 Epoch 697/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0494 - val_loss: 0.4322 Epoch 698/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0493 - val_loss: 0.4287 Epoch 699/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0492 - val_loss: 0.4255 Epoch 700/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0490 - val_loss: 0.4220 Epoch 701/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0489 - val_loss: 0.4185 Epoch 702/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0488 - val_loss: 0.4151 Epoch 703/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0486 - val_loss: 0.4117 Epoch 704/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0485 - val_loss: 0.4084 Epoch 705/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0484 - val_loss: 0.4050 Epoch 706/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0482 - val_loss: 0.4019 Epoch 707/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0481 - val_loss: 0.3990 Epoch 708/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0480 - val_loss: 0.3958 Epoch 709/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0479 - val_loss: 0.3925 Epoch 710/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0477 - val_loss: 0.3891 Epoch 711/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0476 - val_loss: 0.3859 Epoch 712/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0475 - val_loss: 0.3827 Epoch 713/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0474 - val_loss: 0.3799 Epoch 714/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0472 - val_loss: 0.3766 Epoch 715/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0471 - val_loss: 0.3737 Epoch 716/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0470 - val_loss: 0.3704 Epoch 717/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0469 - val_loss: 0.3674 Epoch 718/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0467 - val_loss: 0.3644 Epoch 719/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0466 - val_loss: 0.3615 Epoch 720/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0465 - val_loss: 0.3587 Epoch 721/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0464 - val_loss: 0.3559 Epoch 722/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0463 - val_loss: 0.3527 Epoch 723/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0462 - val_loss: 0.3499 Epoch 724/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0460 - val_loss: 0.3468 Epoch 725/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0459 - val_loss: 0.3439 Epoch 726/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0458 - val_loss: 0.3409 Epoch 727/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0457 - val_loss: 0.3379 Epoch 728/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0456 - val_loss: 0.3352 Epoch 729/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0455 - val_loss: 0.3323 Epoch 730/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0454 - val_loss: 0.3293 Epoch 731/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0452 - val_loss: 0.3264 Epoch 732/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0451 - val_loss: 0.3236 Epoch 733/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0450 - val_loss: 0.3209 Epoch 734/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0449 - val_loss: 0.3183 Epoch 735/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0448 - val_loss: 0.3156 Epoch 736/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0447 - val_loss: 0.3129 Epoch 737/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0446 - val_loss: 0.3105 Epoch 738/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0445 - val_loss: 0.3079 Epoch 739/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.3053 Epoch 740/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0443 - val_loss: 0.3026 Epoch 741/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0442 - val_loss: 0.3001 Epoch 742/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0441 - val_loss: 0.2975 Epoch 743/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0440 - val_loss: 0.2950 Epoch 744/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0439 - val_loss: 0.2923 Epoch 745/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0438 - val_loss: 0.2897 Epoch 746/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0437 - val_loss: 0.2873 Epoch 747/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0436 - val_loss: 0.2851 Epoch 748/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0435 - val_loss: 0.2827 Epoch 749/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0434 - val_loss: 0.2804 Epoch 750/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0433 - val_loss: 0.2780 Epoch 751/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0432 - val_loss: 0.2757 Epoch 752/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0431 - val_loss: 0.2732 Epoch 753/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0430 - val_loss: 0.2710 Epoch 754/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0429 - val_loss: 0.2687 Epoch 755/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.2662 Epoch 756/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0427 - val_loss: 0.2639 Epoch 757/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0426 - val_loss: 0.2617 Epoch 758/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0425 - val_loss: 0.2595 Epoch 759/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0425 - val_loss: 0.2574 Epoch 760/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0424 - val_loss: 0.2552 Epoch 761/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0423 - val_loss: 0.2530 Epoch 762/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0422 - val_loss: 0.2508 Epoch 763/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0421 - val_loss: 0.2487 Epoch 764/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2465 Epoch 765/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0420 - val_loss: 0.2444 Epoch 766/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0419 - val_loss: 0.2422 Epoch 767/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0418 - val_loss: 0.2400 Epoch 768/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0417 - val_loss: 0.2380 Epoch 769/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0416 - val_loss: 0.2356 Epoch 770/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0415 - val_loss: 0.2336 Epoch 771/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0415 - val_loss: 0.2313 Epoch 772/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0414 - val_loss: 0.2293 Epoch 773/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0413 - val_loss: 0.2273 Epoch 774/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0412 - val_loss: 0.2255 Epoch 775/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0411 - val_loss: 0.2236 Epoch 776/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0411 - val_loss: 0.2217 Epoch 777/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0410 - val_loss: 0.2200 Epoch 778/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0409 - val_loss: 0.2178 Epoch 779/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2158 Epoch 780/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0408 - val_loss: 0.2139 Epoch 781/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0407 - val_loss: 0.2120 Epoch 782/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0406 - val_loss: 0.2101 Epoch 783/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2082 Epoch 784/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0405 - val_loss: 0.2063 Epoch 785/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0404 - val_loss: 0.2045 Epoch 786/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2026 Epoch 787/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0403 - val_loss: 0.2007 Epoch 788/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.0402 - val_loss: 0.1989 Epoch 789/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0401 - val_loss: 0.1971 Epoch 790/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1955 Epoch 791/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0400 - val_loss: 0.1938 Epoch 792/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0399 - val_loss: 0.1920 Epoch 793/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1903 Epoch 794/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0398 - val_loss: 0.1883 Epoch 795/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0397 - val_loss: 0.1866 Epoch 796/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1848 Epoch 797/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0396 - val_loss: 0.1831 Epoch 798/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0395 - val_loss: 0.1816 Epoch 799/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1799 Epoch 800/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0394 - val_loss: 0.1782 Epoch 801/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0393 - val_loss: 0.1765 Epoch 802/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1750 Epoch 803/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0392 - val_loss: 0.1736 Epoch 804/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1721 Epoch 805/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0391 - val_loss: 0.1708 Epoch 806/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0390 - val_loss: 0.1693 Epoch 807/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1678 Epoch 808/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0389 - val_loss: 0.1663 Epoch 809/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1648 Epoch 810/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0388 - val_loss: 0.1632 Epoch 811/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1617 Epoch 812/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0387 - val_loss: 0.1603 Epoch 813/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1589 Epoch 814/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0386 - val_loss: 0.1575 Epoch 815/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.0385 - val_loss: 0.1562 Epoch 816/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1547 Epoch 817/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0384 - val_loss: 0.1534 Epoch 818/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1521 Epoch 819/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0383 - val_loss: 0.1507 Epoch 820/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1493 Epoch 821/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0382 - val_loss: 0.1479 Epoch 822/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1466 Epoch 823/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1453 Epoch 824/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0381 - val_loss: 0.1441 Epoch 825/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0380 - val_loss: 0.1429 Epoch 826/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1416 Epoch 827/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0379 - val_loss: 0.1404 Epoch 828/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1392 Epoch 829/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0378 - val_loss: 0.1378 Epoch 830/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1365 Epoch 831/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0377 - val_loss: 0.1353 Epoch 832/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0377 - val_loss: 0.1342 Epoch 833/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1330 Epoch 834/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0376 - val_loss: 0.1319 Epoch 835/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1309 Epoch 836/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0375 - val_loss: 0.1297 Epoch 837/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1287 Epoch 838/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0374 - val_loss: 0.1276 Epoch 839/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.0373 - val_loss: 0.1265 Epoch 840/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1253 Epoch 841/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0373 - val_loss: 0.1241 Epoch 842/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1229 Epoch 843/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0372 - val_loss: 0.1218 Epoch 844/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1209 Epoch 845/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0371 - val_loss: 0.1198 Epoch 846/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1189 Epoch 847/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1178 Epoch 848/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0370 - val_loss: 0.1168 Epoch 849/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1159 Epoch 850/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1149 Epoch 851/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0369 - val_loss: 0.1139 Epoch 852/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1129 Epoch 853/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0368 - val_loss: 0.1120 Epoch 854/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1111 Epoch 855/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1102 Epoch 856/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0367 - val_loss: 0.1093 Epoch 857/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1083 Epoch 858/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1074 Epoch 859/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0366 - val_loss: 0.1066 Epoch 860/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1057 Epoch 861/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1047 Epoch 862/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0365 - val_loss: 0.1038 Epoch 863/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1029 Epoch 864/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0364 - val_loss: 0.1019 Epoch 865/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0364 - val_loss: 0.1009 Epoch 866/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.1000 Epoch 867/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0992 Epoch 868/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0363 - val_loss: 0.0984 Epoch 869/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0976 Epoch 870/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0968 Epoch 871/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0362 - val_loss: 0.0962 Epoch 872/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0362 - val_loss: 0.0954 Epoch 873/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0946 Epoch 874/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0939 Epoch 875/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0361 - val_loss: 0.0931 Epoch 876/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0924 Epoch 877/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0916 Epoch 878/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0908 Epoch 879/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0360 - val_loss: 0.0902 Epoch 880/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0895 Epoch 881/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0889 Epoch 882/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0881 Epoch 883/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0359 - val_loss: 0.0875 Epoch 884/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0868 Epoch 885/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0861 Epoch 886/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0853 Epoch 887/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0358 - val_loss: 0.0846 Epoch 888/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0839 Epoch 889/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0833 Epoch 890/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0357 - val_loss: 0.0827 Epoch 891/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0357 - val_loss: 0.0821 Epoch 892/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0815 Epoch 893/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0810 Epoch 894/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0804 Epoch 895/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0799 Epoch 896/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0794 Epoch 897/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0789 Epoch 898/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0783 Epoch 899/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0778 Epoch 900/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0355 - val_loss: 0.0773 Epoch 901/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0355 - val_loss: 0.0768 Epoch 902/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0762 Epoch 903/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0756 Epoch 904/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0751 Epoch 905/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0746 Epoch 906/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0354 - val_loss: 0.0740 Epoch 907/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0735 Epoch 908/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0731 Epoch 909/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0726 Epoch 910/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0353 - val_loss: 0.0723 Epoch 911/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0719 Epoch 912/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0353 - val_loss: 0.0714 Epoch 913/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0352 - val_loss: 0.0709 Epoch 914/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0703 Epoch 915/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0699 Epoch 916/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0694 Epoch 917/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0690 Epoch 918/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0352 - val_loss: 0.0684 Epoch 919/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0680 Epoch 920/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0675 Epoch 921/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0671 Epoch 922/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0666 Epoch 923/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0663 Epoch 924/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0351 - val_loss: 0.0659 Epoch 925/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0654 Epoch 926/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0649 Epoch 927/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0644 Epoch 928/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0639 Epoch 929/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0634 Epoch 930/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0631 Epoch 931/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0628 Epoch 932/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0350 - val_loss: 0.0624 Epoch 933/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0622 Epoch 934/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0618 Epoch 935/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0614 Epoch 936/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0611 Epoch 937/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0608 Epoch 938/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0349 - val_loss: 0.0606 Epoch 939/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0349 - val_loss: 0.0603 Epoch 940/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0599 Epoch 941/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0597 Epoch 942/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0594 Epoch 943/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0591 Epoch 944/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0588 Epoch 945/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.0348 - val_loss: 0.0584 Epoch 946/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0580 Epoch 947/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0577 Epoch 948/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0575 Epoch 949/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0348 - val_loss: 0.0571 Epoch 950/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0569 Epoch 951/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0565 Epoch 952/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0563 Epoch 953/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0560 Epoch 954/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0557 Epoch 955/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0554 Epoch 956/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0552 Epoch 957/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0549 Epoch 958/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0347 - val_loss: 0.0547 Epoch 959/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0347 - val_loss: 0.0544 Epoch 960/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0542 Epoch 961/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0540 Epoch 962/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0346 - val_loss: 0.0537 Epoch 963/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0534 Epoch 964/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0532 Epoch 965/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0529 Epoch 966/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0527 Epoch 967/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0524 Epoch 968/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0522 Epoch 969/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0520 Epoch 970/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0519 Epoch 971/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0517 Epoch 972/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0346 - val_loss: 0.0515 Epoch 973/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0513 Epoch 974/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0511 Epoch 975/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0509 Epoch 976/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0507 Epoch 977/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0505 Epoch 978/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0503 Epoch 979/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0501 Epoch 980/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0499 Epoch 981/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0497 Epoch 982/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0495 Epoch 983/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0494 Epoch 984/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0492 Epoch 985/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0490 Epoch 986/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0488 Epoch 987/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0486 Epoch 988/2000 6/6 [==============================] - 0s 2ms/step - loss: 0.0344 - val_loss: 0.0484 Epoch 989/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0345 - val_loss: 0.0482 Epoch 990/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0481 Epoch 991/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0479 Epoch 992/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0478 Epoch 993/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0476 Epoch 994/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0475 Epoch 995/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0473 Epoch 996/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0472 Epoch 997/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0470 Epoch 998/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0469 Epoch 999/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0467 Epoch 1000/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0466 Epoch 1001/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0344 - val_loss: 0.0464 Epoch 1002/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0463 Epoch 1003/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0462 Epoch 1004/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0461 Epoch 1005/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0459 Epoch 1006/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458 Epoch 1007/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0458 Epoch 1008/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0457 Epoch 1009/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0457 Epoch 1010/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0456 Epoch 1011/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0455 Epoch 1012/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454 Epoch 1013/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0344 - val_loss: 0.0454 Epoch 1014/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0452 Epoch 1015/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0452 Epoch 1016/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0450 Epoch 1017/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0448 Epoch 1018/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0446 Epoch 1019/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0445 Epoch 1020/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0444 Epoch 1021/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0443 Epoch 1022/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0442 Epoch 1023/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441 Epoch 1024/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0441 Epoch 1025/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0440 Epoch 1026/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0439 Epoch 1027/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0438 Epoch 1028/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0437 Epoch 1029/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0436 Epoch 1030/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434 Epoch 1031/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0434 Epoch 1032/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0433 Epoch 1033/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0432 Epoch 1034/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431 Epoch 1035/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0431 Epoch 1036/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0343 - val_loss: 0.0430 Epoch 1037/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0429 Epoch 1038/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0428 Epoch 1039/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427 Epoch 1040/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0427 Epoch 1041/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0426 Epoch 1042/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0425 Epoch 1043/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0424 Epoch 1044/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423 Epoch 1045/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0423 Epoch 1046/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0422 Epoch 1047/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421 Epoch 1048/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0421 Epoch 1049/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0420 Epoch 1050/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0419 Epoch 1051/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0418 Epoch 1052/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0418 Epoch 1053/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0417 Epoch 1054/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417 Epoch 1055/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0417 Epoch 1056/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0343 - val_loss: 0.0416 Epoch 1057/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416 Epoch 1058/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0416 Epoch 1059/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415 Epoch 1060/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0415 Epoch 1061/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414 Epoch 1062/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0414 Epoch 1063/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0413 Epoch 1064/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412 Epoch 1065/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0412 Epoch 1066/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0411 Epoch 1067/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410 Epoch 1068/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0410 Epoch 1069/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0409 Epoch 1070/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409 Epoch 1071/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409 Epoch 1072/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0409 Epoch 1073/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408 Epoch 1074/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0408 Epoch 1075/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407 Epoch 1076/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407 Epoch 1077/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0407 Epoch 1078/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0407 Epoch 1079/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406 Epoch 1080/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406 Epoch 1081/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0406 Epoch 1082/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405 Epoch 1083/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405 Epoch 1084/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0405 Epoch 1085/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404 Epoch 1086/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404 Epoch 1087/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0404 Epoch 1088/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1089/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1090/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1091/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1092/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0403 Epoch 1093/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1094/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1095/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0402 Epoch 1096/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1097/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1098/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1099/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1100/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1101/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0401 Epoch 1102/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400 Epoch 1103/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400 Epoch 1104/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0400 Epoch 1105/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1106/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1107/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1108/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0399 Epoch 1109/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1110/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1111/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1112/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1113/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1114/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0398 Epoch 1115/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1116/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1117/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0397 Epoch 1118/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1119/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1120/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1121/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0396 Epoch 1122/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1123/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1124/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1125/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1126/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1127/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1128/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1129/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1130/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1131/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1132/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1133/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0395 Epoch 1134/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1135/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1136/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1137/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1138/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1139/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1140/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1141/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1142/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1143/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0394 Epoch 1144/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1145/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1146/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1147/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1148/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1149/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1150/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1151/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1152/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1153/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1154/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1155/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0393 Epoch 1156/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1157/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1158/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1159/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1160/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1161/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1162/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0392 Epoch 1163/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1164/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1165/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1166/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1167/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1168/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1169/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1170/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1171/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1172/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1173/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1174/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0391 Epoch 1175/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1176/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1177/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1178/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1179/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1180/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1181/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1182/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1183/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1184/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1185/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1186/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1187/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1188/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1189/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1190/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1191/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1192/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1193/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1194/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1195/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1196/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1197/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1198/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1199/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1200/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0390 Epoch 1201/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1202/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1203/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1204/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1205/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1206/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1207/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1208/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1209/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1210/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1211/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1212/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1213/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1214/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1215/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1216/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1217/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1218/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1219/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1220/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0389 Epoch 1221/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1222/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1223/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1224/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1225/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1226/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1227/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1228/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1229/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1230/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1231/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1232/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1233/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1234/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1235/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1236/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1237/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1238/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1239/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1240/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1241/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1242/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1243/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1244/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1245/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1246/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1247/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1248/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1249/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1250/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1251/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1252/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1253/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1254/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1255/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1256/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1257/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1258/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0388 Epoch 1259/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1260/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1261/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1262/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1263/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1264/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1265/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1266/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1267/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1268/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1269/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1270/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1271/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1272/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1273/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1274/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1275/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1276/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1277/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1278/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1279/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1280/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1281/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1282/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1283/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1284/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1285/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1286/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1287/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1288/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1289/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1290/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1291/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1292/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1293/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1294/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1295/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1296/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1297/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1298/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1299/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1300/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1301/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1302/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1303/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1304/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1305/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1306/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1307/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1308/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1309/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1310/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1311/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1312/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1313/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1314/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1315/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1316/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1317/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1318/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1319/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1320/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1321/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1322/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1323/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1324/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1325/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1326/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1327/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1328/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1329/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1330/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1331/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1332/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1333/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1334/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1335/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1336/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1337/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1338/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1339/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1340/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1341/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1342/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1343/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1344/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1345/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1346/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1347/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1348/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1349/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1350/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1351/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1352/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1353/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1354/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1355/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1356/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1357/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1358/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1359/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1360/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1361/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1362/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1363/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1364/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1365/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1366/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1367/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1368/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1369/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1370/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1371/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1372/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1373/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1374/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1375/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1376/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1377/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1378/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1379/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1380/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1381/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1382/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1383/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1384/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1385/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1386/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1387/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1388/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1389/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1390/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1391/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1392/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1393/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1394/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1395/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1396/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1397/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1398/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1399/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1400/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1401/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1402/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1403/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1404/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1405/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1406/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1407/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1408/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1409/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1410/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1411/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1412/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1413/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1414/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1415/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1416/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1417/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1418/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1419/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1420/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1421/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1422/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1423/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1424/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1425/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1426/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1427/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1428/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1429/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1430/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1431/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1432/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1433/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1434/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1435/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1436/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1437/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1438/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1439/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1440/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1441/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1442/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1443/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1444/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1445/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1446/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1447/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1448/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1449/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1450/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1451/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1452/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1453/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1454/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1455/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1456/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1457/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1458/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1459/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1460/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1461/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1462/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1463/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1464/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1465/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1466/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1467/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1468/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1469/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1470/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1471/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1472/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1473/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1474/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1475/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1476/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1477/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1478/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1479/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1480/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1481/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1482/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1483/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1484/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1485/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1486/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1487/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1488/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1489/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1490/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1491/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1492/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1493/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1494/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1495/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1496/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1497/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1498/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1499/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1500/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1501/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1502/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1503/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1504/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1505/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1506/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1507/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1508/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1509/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1510/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1511/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1512/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1513/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1514/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1515/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1516/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1517/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1518/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1519/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1520/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1521/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1522/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1523/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1524/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1525/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1526/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1527/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1528/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1529/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1530/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1531/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1532/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1533/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1534/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1535/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1536/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1537/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1538/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1539/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1540/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1541/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1542/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1543/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1544/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1545/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1546/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1547/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1548/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1549/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1550/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1551/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1552/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1553/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1554/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1555/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1556/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1557/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1558/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1559/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1560/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1561/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1562/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1563/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1564/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1565/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1566/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1567/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1568/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1569/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1570/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1571/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1572/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1573/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1574/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1575/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1576/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1577/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1578/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1579/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1580/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1581/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1582/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1583/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1584/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1585/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1586/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1587/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1588/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1589/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1590/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1591/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1592/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1593/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1594/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1595/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1596/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1597/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1598/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1599/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1600/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1601/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1602/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1603/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1604/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1605/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1606/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1607/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1608/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1609/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1610/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1611/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1612/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1613/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1614/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1615/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1616/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1617/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1618/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1619/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1620/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1621/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1622/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1623/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1624/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1625/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1626/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1627/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1628/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1629/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1630/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1631/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1632/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1633/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1634/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1635/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1636/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1637/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1638/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1639/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1640/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1641/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1642/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1643/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1644/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1645/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1646/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1647/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1648/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1649/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1650/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1651/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1652/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1653/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1654/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1655/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1656/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1657/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1658/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1659/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1660/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1661/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1662/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1663/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1664/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1665/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1666/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1667/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1668/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1669/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1670/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1671/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1672/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1673/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1674/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1675/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1676/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1677/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1678/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1679/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1680/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1681/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1682/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1683/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1684/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1685/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1686/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1687/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1688/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1689/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1690/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1691/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1692/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1693/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1694/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1695/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1696/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1697/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1698/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1699/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1700/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1701/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1702/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1703/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1704/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1705/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1706/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1707/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1708/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1709/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1710/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1711/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1712/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1713/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1714/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1715/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1716/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1717/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1718/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1719/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1720/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1721/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1722/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1723/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1724/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1725/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1726/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1727/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1728/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1729/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1730/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1731/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1732/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1733/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1734/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1735/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1736/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1737/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1738/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1739/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1740/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1741/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1742/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1743/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1744/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1745/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1746/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1747/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1748/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1749/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1750/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1751/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1752/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1753/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1754/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1755/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1756/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1757/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1758/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1759/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1760/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1761/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1762/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1763/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1764/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1765/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1766/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1767/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1768/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1769/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1770/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1771/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1772/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1773/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1774/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1775/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1776/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1777/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1778/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1779/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1780/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1781/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1782/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1783/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1784/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1785/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1786/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1787/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1788/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1789/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1790/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1791/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1792/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1793/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1794/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1795/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1796/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1797/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1798/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1799/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1800/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1801/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1802/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1803/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1804/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1805/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1806/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1807/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1808/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1809/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1810/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1811/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1812/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1813/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1814/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1815/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1816/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1817/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1818/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1819/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1820/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1821/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1822/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1823/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1824/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1825/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1826/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1827/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1828/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1829/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1830/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1831/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1832/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1833/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1834/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1835/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1836/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1837/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1838/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1839/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1840/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1841/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1842/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1843/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1844/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1845/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1846/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1847/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1848/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1849/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1850/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1851/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1852/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1853/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1854/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1855/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1856/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1857/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1858/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1859/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1860/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1861/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0387 Epoch 1862/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1863/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1864/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1865/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1866/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1867/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1868/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1869/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1870/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1871/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1872/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1873/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1874/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1875/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1876/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1877/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1878/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1879/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1880/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1881/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1882/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1883/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1884/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1885/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1886/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1887/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1888/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1889/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1890/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1891/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1892/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1893/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1894/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1895/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1896/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1897/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1898/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1899/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1900/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1901/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1902/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1903/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1904/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1905/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1906/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1907/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1908/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1909/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1910/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1911/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1912/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1913/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1914/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1915/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1916/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1917/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1918/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1919/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1920/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1921/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1922/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1923/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1924/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1925/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1926/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1927/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1928/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1929/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1930/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1931/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1932/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1933/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1934/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1935/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1936/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1937/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1938/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1939/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1940/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1941/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1942/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1943/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1944/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1945/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1946/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1947/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1948/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1949/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1950/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1951/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1952/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1953/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1954/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1955/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1956/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1957/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1958/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1959/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1960/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1961/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1962/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1963/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1964/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1965/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1966/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1967/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1968/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1969/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1970/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1971/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1972/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1973/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0385 Epoch 1974/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1975/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1976/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1977/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1978/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1979/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1980/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1981/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1982/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1983/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1984/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1985/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1986/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1987/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1988/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1989/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1990/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1991/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1992/2000 6/6 [==============================] - 0s 4ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1993/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1994/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1995/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1996/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1997/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1998/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 1999/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 Epoch 2000/2000 6/6 [==============================] - 0s 3ms/step - loss: 0.0342 - val_loss: 0.0386 . &lt;keras.callbacks.History at 0x7f67c81a26b0&gt; . . #%tensorboard --logdir logs --host 0.0.0.0 . &#53584;&#49436;&#48372;&#46300;: &#49324;&#50857;&#51088;&#51648;&#51221;&#44536;&#47548; &#50640;&#54253;&#48324;&#47196; &#49884;&#44033;&#54868; (2) . - 중간층의 출력결과를 시각화하고 싶다. . 4. Piecewise-linear regression (15점) . 아래의 모형을 고려하자. . model: $y_i= begin{cases} x_i +0.3 epsilon_i &amp; x leq 0 3.5x_i +0.3 epsilon_i &amp; x&gt;0 end{cases}$ . 아래는 위의 모형에서 생성한 샘플이다. . np.random.seed(43052) N=100 x= np.linspace(-1,1,N).reshape(N,1) y= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1) . (풀이) . tf.random.set_seed(43055) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(2)) net.add(tf.keras.layers.Activation(tf.nn.relu)) net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=&#39;sgd&#39;,loss=&#39;mse&#39;) net.fit(x,y,epochs=1,batch_size=100) . 1/1 [==============================] - 0s 79ms/step - loss: 2.1414 . &lt;keras.callbacks.History at 0x7f68849e9420&gt; . l1,a1,l2 =net.layers . fig, (ax1,ax2,ax3) = plt.subplots(1,3) fig.set_figwidth(9) ax1.plot(x,y,&#39;.&#39;,alpha=0.2); ax1.plot(x,l1(x),&#39;--&#39;); ax2.plot(x,y,&#39;.&#39;,alpha=0.2); ax2.plot(x,a1(l1(x)),&#39;--&#39;); ax3.plot(x,y,&#39;.&#39;,alpha=0.2); ax3.plot(x,l2(a1(l1(x))),&#39;--r&#39;); . 이런 그림이 100에폭마다 그려졌으면 좋겠다. | . - 새로운 클래스를 만들자. . class PlotMidlayer(tf.keras.callbacks.Callback): def on_epoch_begin(self,epoch,logs): # 입력은 무조건 self, epoch, logs를 써야합니다 --&gt; 이 함수안에 에폭이 끝날때마다 할 동작을 정의한다. if epoch % 100 ==0: fig, (ax1,ax2,ax3) = plt.subplots(1,3) fig.set_figwidth(9) ax1.plot(x,y,&#39;.&#39;,alpha=0.2); ax1.plot(x,l1(x),&#39;--&#39;); ax2.plot(x,y,&#39;.&#39;,alpha=0.2); ax2.plot(x,a1(l1(x)),&#39;--&#39;); ax3.plot(x,y,&#39;.&#39;,alpha=0.2); ax3.plot(x,l2(a1(l1(x))),&#39;--r&#39;); with tf.summary.create_file_writer(logdir).as_default(): tf.summary.image(&quot;적합결과시각화&quot;+str(epoch), plot_to_image(fig), step=0) . !rm -rf logs cb1= tf.keras.callbacks.TensorBoard(update_freq=&#39;epoch&#39;,histogram_freq=100) cb2= PlotMidlayer() net.fit(x,y,epochs=1000, batch_size=100,verbose=0 ,callbacks=[cb1,cb2]) . &lt;keras.callbacks.History at 0x7f6ef421c280&gt; . #%tensorboard --logdir logs --host 0.0.0.0 .",
            "url": "https://guebin.github.io/STBDA2022/2022/05/30/(13%EC%A3%BC%EC%B0%A8)-5%EC%9B%9430%EC%9D%BC.html",
            "relUrl": "/2022/05/30/(13%EC%A3%BC%EC%B0%A8)-5%EC%9B%9430%EC%9D%BC.html",
            "date": " • May 30, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "(12주차) 5월23일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import matplotlib.pyplot as plt import numpy as np . CNN . CONV&#51032; &#50669;&#54624; . - 데이터생성 (그냥 흑백대비 데이터) . _X1 = tnp.ones([50,25])*10 _X1 . &lt;tf.Tensor: shape=(50, 25), dtype=float64, numpy= array([[10., 10., 10., ..., 10., 10., 10.], [10., 10., 10., ..., 10., 10., 10.], [10., 10., 10., ..., 10., 10., 10.], ..., [10., 10., 10., ..., 10., 10., 10.], [10., 10., 10., ..., 10., 10., 10.], [10., 10., 10., ..., 10., 10., 10.]])&gt; . _X2 = tnp.zeros([50,25])*10 _X2 . &lt;tf.Tensor: shape=(50, 25), dtype=float64, numpy= array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]])&gt; . tf.concat([_X1,_X2],axis=1) . &lt;tf.Tensor: shape=(50, 50), dtype=float64, numpy= array([[10., 10., 10., ..., 0., 0., 0.], [10., 10., 10., ..., 0., 0., 0.], [10., 10., 10., ..., 0., 0., 0.], ..., [10., 10., 10., ..., 0., 0., 0.], [10., 10., 10., ..., 0., 0., 0.], [10., 10., 10., ..., 0., 0., 0.]])&gt; . _noise = tnp.random.randn(50*50).reshape(50,50) _noise . &lt;tf.Tensor: shape=(50, 50), dtype=float64, numpy= array([[-0.30380244, 0.06484819, 0.60069937, ..., -0.49237769, 1.72552047, 0.32319886], [-0.1442766 , 0.32071132, 0.27135225, ..., 0.12584098, 1.77500838, 0.30678486], [-0.98493241, 0.70428041, -0.10798709, ..., -0.07145503, 0.11185082, 1.4473293 ], ..., [ 0.41430467, -0.67483518, -0.46844066, ..., 0.76154689, -1.60328529, -0.37098601], [-1.65297477, -1.45893833, -1.7887122 , ..., -0.81344932, -0.21032504, -0.53206832], [-0.2352507 , -0.77675024, -2.01329394, ..., -1.41071477, -1.20259288, 0.07060629]])&gt; . XXX = tf.concat([_X1,_X2],axis=1) + _noise . XXX=XXX.reshape(1,50,50,1) . plt.imshow(XXX.reshape(50,50),cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f6769d77f10&gt; . - conv layer 생성 . conv = tf.keras.layers.Conv2D(2,(2,2)) . conv.weights # 처음에는 가중치가 없음 . [] . conv(XXX) # 가중치를 만들기 위해서 XXX를 conv에 한번 통과시킴 conv.weights # 이제 가중치가 생김 . [&lt;tf.Variable &#39;conv2d_1/kernel:0&#39; shape=(2, 2, 1, 2) dtype=float32, numpy= array([[[[ 0.06554878, 0.39761645]], [[-0.4267348 , -0.376472 ]]], [[[ 0.2653011 , 0.42274743]], [[ 0.4461723 , -0.6650867 ]]]], dtype=float32)&gt;, &lt;tf.Variable &#39;conv2d_1/bias:0&#39; shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;] . - 가중치의 값을 확인해보자. . conv.weights[0] # kernel에 해당하는것 . &lt;tf.Variable &#39;conv2d_1/kernel:0&#39; shape=(2, 2, 1, 2) dtype=float32, numpy= array([[[[ 0.06554878, 0.39761645]], [[-0.4267348 , -0.376472 ]]], [[[ 0.2653011 , 0.42274743]], [[ 0.4461723 , -0.6650867 ]]]], dtype=float32)&gt; . conv.weights[1] # bias에 해당하는것 . &lt;tf.Variable &#39;conv2d_1/bias:0&#39; shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt; . - 필터값을 원하는 것으로 변경해보자. . w0 = [[0.25,0.25],[0.25,0.25]] # 잡티를 제거하는 효과를 준다. w1 = [[-1.0,1.0],[-1.0,1.0]] # 경계를 찾기 좋아보이는 필터이다. (엣지검출) . w=np.concatenate([np.array(w0).reshape(2,2,1,1),np.array(w1).reshape(2,2,1,1)],axis=-1) w . array([[[[ 0.25, -1. ]], [[ 0.25, 1. ]]], [[[ 0.25, -1. ]], [[ 0.25, 1. ]]]]) . b= np.array([0.0,0.0]) b . array([0., 0.]) . conv.set_weights([w,b]) conv.get_weights() . [array([[[[ 0.25, -1. ]], [[ 0.25, 1. ]]], [[[ 0.25, -1. ]], [[ 0.25, 1. ]]]], dtype=float32), array([0., 0.], dtype=float32)] . 첫번째는 평균을 구하는 필터, | 두번째는 엣지를 검출하는 필터 | . - 필터를 넣은 결과를 확인 . XXX0=conv(XXX)[...,0] # 채널0 XXX0 . &lt;tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy= array([[[ 9.984369 , 10.314403 , 10.114662 , ..., -0.2716803 , 0.78349805, 1.032628 ], [ 9.973946 , 10.29709 , 10.011451 , ..., -0.78137755, 0.4853113 , 0.91024333], [ 9.694317 , 10.180944 , 10.165418 , ..., -1.1441237 , -0.10771888, 0.0131253 ], ..., [ 9.950029 , 9.197831 , 9.421099 , ..., 0.2848997 , -0.24674678, -0.35682005], [ 9.156889 , 8.902268 , 9.352164 , ..., 0.01892059, -0.46637818, -0.67916614], [ 8.969021 , 8.490577 , 9.140195 , ..., -0.2541374 , -0.9092705 , -0.46859497]]], dtype=float32)&gt; . XXX1=conv(XXX)[...,1] # 채널1 XXX1 . &lt;tf.Tensor: shape=(1, 49, 49), dtype=float32, numpy= array([[[ 0.8336382 , 0.48649216, -1.2854509 , ..., 0.35364777, 3.8670654 , -2.8705451 ], [ 2.1542006 , -0.8616276 , -0.28092575, ..., 3.2342823 , 1.8324732 , -0.13274503], [ 2.0953035 , -0.14879417, 0.08668804, ..., 5.0843253 , -0.93870586, 1.4220824 ], ..., [-1.498992 , -1.5097971 , 2.4028683 , ..., 1.3495452 , -3.4761312 , 3.0358381 ], [-0.89510345, -0.12337971, 1.9229631 , ..., -0.17948717, -1.7617078 , 0.910556 ], [-0.3474636 , -1.5663171 , 4.1647916 , ..., -3.4317784 , 0.81124616, 0.9514559 ]]], dtype=float32)&gt; . - 각 채널을 시각화 . fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2) . ax1.imshow(XXX.reshape(50,50),cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f67684b5720&gt; . ax3.imshow(XXX0.reshape(49,49),cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f67684b47c0&gt; . ax4.imshow(XXX1.reshape(49,49),cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f67684b72e0&gt; . fig . 2사분면: 원래이미지 | 3사분면: 원래이미지 -&gt; 평균을 의미하는 conv적용 | 4사분면: 원래이미지 -&gt; 엣지를 검출하는 conv적용 | . - conv(XXX)의 각 채널에 한번더 conv를 통과시켜보자 . conv(XXX0.reshape(1,49,49,1))[...,0] ### XXX0 -&gt; 평균필터 &lt;=&gt; XXX -&gt; 평균필터 -&gt; 평균필터 conv(XXX0.reshape(1,49,49,1))[...,1] ### XXX0 -&gt; 엣지필터 &lt;=&gt; XXX -&gt; 평균필터 -&gt; 엣지필터 conv(XXX1.reshape(1,49,49,1))[...,0] ### XXX1 -&gt; 평균필터 &lt;=&gt; XXX -&gt; 엣지필터 -&gt; 평균필터 conv(XXX1.reshape(1,49,49,1))[...,1] ### XXX1 -&gt; 엣지필터 &lt;=&gt; XXX -&gt; 엣지필터 -&gt; 엣지필터 . &lt;tf.Tensor: shape=(1, 48, 48), dtype=float32, numpy= array([[[ 1.01424513e+01, 1.01844015e+01, 9.86733055e+00, ..., -4.99692082e-01, 5.39378747e-02, 8.02920163e-01], [ 1.00365734e+01, 1.01637259e+01, 1.02149420e+01, ..., -7.43912578e-01, -3.86977196e-01, 3.25240284e-01], [ 9.74410343e+00, 1.01362820e+01, 1.04401426e+01, ..., -8.01947534e-01, -6.61381423e-01, -1.04143508e-01], ..., [ 9.49301243e+00, 9.22852516e+00, 9.76124573e+00, ..., -3.70009184e-01, -2.82902658e-01, -3.20988595e-01], [ 9.30175495e+00, 9.21834087e+00, 9.71927547e+00, ..., -1.98229820e-01, -1.02326170e-01, -4.37277794e-01], [ 8.87968922e+00, 8.97130108e+00, 9.59087849e+00, ..., -5.38469851e-03, -4.02716398e-01, -6.30852461e-01]]], dtype=float32)&gt; . fig,ax =plt.subplots(3,4) . ax[0][0].imshow(XXX.reshape(50,50),cmap=&#39;gray&#39;) # 원래이미지 . &lt;matplotlib.image.AxesImage at 0x7f61a4265540&gt; . ax[1][0].imshow(XXX0.reshape(49,49),cmap=&#39;gray&#39;) # 원래이미지 -&gt; 평균필터 ax[1][2].imshow(XXX1.reshape(49,49),cmap=&#39;gray&#39;) # 원래이미지 -&gt; 엣지필터 . &lt;matplotlib.image.AxesImage at 0x7f61a429b880&gt; . ax[2][0].imshow(conv(XXX0.reshape(1,49,49,1))[...,0].reshape(48,48),cmap=&#39;gray&#39;) # 원래이미지 -&gt; 평균필터 ax[2][1].imshow(conv(XXX0.reshape(1,49,49,1))[...,1].reshape(48,48),cmap=&#39;gray&#39;) # 원래이미지 -&gt; 엣지필터 ax[2][2].imshow(conv(XXX1.reshape(1,49,49,1))[...,0].reshape(48,48),cmap=&#39;gray&#39;) # 원래이미지 -&gt; 평균필터 ax[2][3].imshow(conv(XXX1.reshape(1,49,49,1))[...,1].reshape(48,48),cmap=&#39;gray&#39;) # 원래이미지 -&gt; 엣지필터 . &lt;matplotlib.image.AxesImage at 0x7f61a415e380&gt; . fig.set_figheight(8) fig.set_figwidth(16) fig.tight_layout() fig . - 요약 . conv의 weight에 따라서 엣지를 검출하는 필터가 만들어지기도 하고 스무딩의 역할을 하는 필터가 만들어지기도 한다. 그리고 우리는 의미를 알 수 없지만 어떠한 역할을 하는 필터가 만들어질 것이다. | 이것들을 조합하다보면 우연히 이미지를 분류하기에 유리한 특징을 뽑아내는 weight가 맞춰질 수도 있겠다. | 채널수를 많이 만들고 다양한 웨이트조합을 실험하다보면 보다 복잡한 이미지의 특징을 추출할 수도 있을 것이다? | 컨볼루션 레이어의 역할 = 이미지의 특징을 추출하는 역할 | . - 참고: 스트라이드, 패딩 . 스트라이드: 윈도우가 1칸씩 이동하는 것이 아니라 2~3칸씩 이동함 | 패딩: 이미지의 가장자리에 정당한 값을 넣어서 (예를들어 0) 컨볼루션을 수행. 따라서 컨볼루션 연산 이후에도 이미지의 크기가 줄어들지 않도록 방지한다. | . MAXPOOL . - 기본적역할: 이미지의 크기를 줄이는 것 . 이미지의의 크기를 줄여야하는 이유? 어차피 최종적으로 10차원으로 줄어야하므로 | 이미지의 크기를 줄이면서도 동시에 아주 크리티컬한 특징은 손실없이 유지하고 싶다~ | . - 점점 작은 이미지가 되면서 중요한 특징들은 살아남지만 그렇지 않으면 죽는다. (캐리커쳐 느낌) . - 평균이 아니라 max를 쓴 이유는? 그냥 평균보다 나을것이라고 생각했음.. . 그런데 사실은 꼭 그렇지만은 않아서 최근에는 꼭 맥스풀링을 고집하진 않는 추세 (평균풀링도 많이씀) | . CNN &#50500;&#53412;&#53581;&#52376;&#51032; &#54364;&#54788;&#48169;&#48277; . - 아래와 같이 아키텍처의 다이어그램형태로 표현하고 굳이 노드별로 이미지를 그리진 않음 . . - 물론 아래와 같이 그리는 경우도 있음 . . Discusstion about CNN . - 격자형태로 배열된 자료를 처리하는데 특화된 신경망이다. . 시계열 (1차원격자), 이미지 (2차원격자) | . - 실제응용에서 엄청난 성공을 거두었다. . - 이름의 유래는 컨볼루션이라는 수학적 연산을 사용했기 때문 . 컨볼루션은 조금 특별한 선형변환이다. | . - 신경과학의 원리가 심층학습에 영향을 미친 사례이다. . CNN&#51032; &#47784;&#54000;&#48652; . - 희소성 + 매개변수의 공유 . 다소 철학적인 모티브임 | 희소성: 이미지를 분석하여 특징을 뽑아낼때 부분부분의 특징만 뽑으면 된다는 의미 | 매개변수의 공유: 한 채널에는 하나의 역할을 하는 커널을 설계하면 된다는 의미 (스무딩이든 엣징이든). 즉 어떤지역은 스무딩, 어떤지역은 엣징을 할 필요가 없이 한채널에서는 엣징만, 다른채널에서는 스무딩만 수행한뒤 여러채널을 조합해서 이해하면 된다. | . - 매개변수 공유효과로 인해서 파라메터가 확 줄어든다. . (예시) (1,6,6,1) -&gt; (1,5,5,2) . MLP방식이면 (36,50) 의 차원을 가진 매트릭스가 필요함 =&gt; 1800개의 매개변수 필요 | CNN은 8개의 매개변수 필요 | . CNN &#49888;&#44221;&#47581;&#51032; &#44592;&#48376;&#44396;&#51312; . - 기본유닛 . conv - activation - pooling | conv - conv - activation - pooling | . &#47784;&#54805;&#51032; &#49457;&#45733;&#51012; &#50732;&#47532;&#44592; &#50948;&#54620; &#45432;&#47141;&#46308; . dropout . - 아래의 예제를 복습하자. . np.random.seed(43052) x = np.linspace(0,1,100).reshape(100,1) y = np.random.normal(loc=0,scale=0.01,size=(100,1)) plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x7f1b9d3df0d0&gt;] . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(2048,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) net.fit(x,y,epochs=5000,verbose=0,batch_size=100) . 2022-05-23 19:33:01.211991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . &lt;keras.callbacks.History at 0x7f1b9528feb0&gt; . plt.plot(x,y) plt.plot(x,net(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f1b881bc9d0&gt;] . - train/test로 나누어서 생각해보자. . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(2048,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) net.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80) . &lt;keras.callbacks.History at 0x7f1b881f9840&gt; . plt.plot(x,y) plt.plot(x[:80],net(x[:80]),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f1b881faa40&gt;] . plt.plot(x,y) plt.plot(x[:80],net(x[:80]),&#39;--&#39;) plt.plot(x[80:],net(x[80:]),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f1b881686d0&gt;] . train에서 추세를 따라가는게 좋은게 아니다 $ to$ 그냥 직선으로 핏하는거 이외에는 다 오버핏이다. | . - 매 에폭마다 적당히 80%의 노드들을 빼고 학습하자 $ to$ 너무 잘 학습되는 문제는 생기지 않을 것이다 (과적합이 방지될것이다?) . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(2048,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dropout(0.8)) net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;adam&#39;) net.fit(x[:80],y[:80],epochs=5000,verbose=0,batch_size=80) . &lt;keras.callbacks.History at 0x7f1b80381a50&gt; . plt.plot(x,y) plt.plot(x[:80],net(x[:80]),&#39;--&#39;) plt.plot(x[80:],net(x[80:]),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f1b8039db10&gt;] . - 드랍아웃에 대한 summary . 직관: 특정노드를 랜덤으로 off시키면 학습이 방해되어 오히려 과적합이 방지되는 효과가 있다 (그렇지만 진짜 중요한 특징이라면 랜덤으로 off 되더라도 어느정도는 학습될 듯) | note: 드랍아웃을 쓰면 오버핏이 줄어드는건 맞지만 완전히 없어지는건 아니다. | note: 오버핏을 줄이는 유일한 방법이 드랍아웃만 있는것도 아니며, 드랍아웃이 오버핏을 줄이는 가장 효과적인 방법도 아니다 (최근에는 dropout보다 batch nomalization을 사용하는 추세임) | . train / val / test . - data . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() . X= x_train.reshape(-1,28,28,1)/255 ## 입력이 0~255 -&gt; 0~1로 표준화 시키는 효과 + float으로 자료형이 바뀜 y = tf.keras.utils.to_categorical(y_train) XX = x_test.reshape(-1,28,28,1)/255 yy = tf.keras.utils.to_categorical(y_test) . net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(50,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.categorical_crossentropy,metrics=&#39;accuracy&#39;) . cb1 = tf.keras.callbacks.TensorBoard() net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb1,verbose=1) . Epoch 1/200 240/240 [==============================] - 0s 1ms/step - loss: 0.7013 - accuracy: 0.7666 - val_loss: 0.4976 - val_accuracy: 0.8320 Epoch 2/200 240/240 [==============================] - 0s 887us/step - loss: 0.4703 - accuracy: 0.8400 - val_loss: 0.4822 - val_accuracy: 0.8320 Epoch 3/200 240/240 [==============================] - 0s 915us/step - loss: 0.4287 - accuracy: 0.8518 - val_loss: 0.4339 - val_accuracy: 0.8535 Epoch 4/200 240/240 [==============================] - 0s 949us/step - loss: 0.4061 - accuracy: 0.8592 - val_loss: 0.4077 - val_accuracy: 0.8568 Epoch 5/200 240/240 [==============================] - 0s 937us/step - loss: 0.3851 - accuracy: 0.8661 - val_loss: 0.3948 - val_accuracy: 0.8619 Epoch 6/200 240/240 [==============================] - 0s 951us/step - loss: 0.3703 - accuracy: 0.8699 - val_loss: 0.3900 - val_accuracy: 0.8617 Epoch 7/200 240/240 [==============================] - 0s 879us/step - loss: 0.3587 - accuracy: 0.8746 - val_loss: 0.3846 - val_accuracy: 0.8678 Epoch 8/200 240/240 [==============================] - 0s 897us/step - loss: 0.3468 - accuracy: 0.8768 - val_loss: 0.3684 - val_accuracy: 0.8716 Epoch 9/200 240/240 [==============================] - 0s 905us/step - loss: 0.3397 - accuracy: 0.8788 - val_loss: 0.3678 - val_accuracy: 0.8708 Epoch 10/200 240/240 [==============================] - 0s 918us/step - loss: 0.3307 - accuracy: 0.8806 - val_loss: 0.3619 - val_accuracy: 0.8726 Epoch 11/200 240/240 [==============================] - 0s 961us/step - loss: 0.3208 - accuracy: 0.8843 - val_loss: 0.3619 - val_accuracy: 0.8711 Epoch 12/200 240/240 [==============================] - 0s 896us/step - loss: 0.3160 - accuracy: 0.8857 - val_loss: 0.3572 - val_accuracy: 0.8734 Epoch 13/200 240/240 [==============================] - 0s 879us/step - loss: 0.3098 - accuracy: 0.8889 - val_loss: 0.3567 - val_accuracy: 0.8723 Epoch 14/200 240/240 [==============================] - 0s 914us/step - loss: 0.3023 - accuracy: 0.8915 - val_loss: 0.3545 - val_accuracy: 0.8751 Epoch 15/200 240/240 [==============================] - 0s 972us/step - loss: 0.2983 - accuracy: 0.8937 - val_loss: 0.3514 - val_accuracy: 0.8763 Epoch 16/200 240/240 [==============================] - 0s 963us/step - loss: 0.2945 - accuracy: 0.8939 - val_loss: 0.3745 - val_accuracy: 0.8639 Epoch 17/200 240/240 [==============================] - 0s 884us/step - loss: 0.2901 - accuracy: 0.8956 - val_loss: 0.3427 - val_accuracy: 0.8786 Epoch 18/200 240/240 [==============================] - 0s 876us/step - loss: 0.2835 - accuracy: 0.8972 - val_loss: 0.3470 - val_accuracy: 0.8770 Epoch 19/200 240/240 [==============================] - 0s 916us/step - loss: 0.2787 - accuracy: 0.8997 - val_loss: 0.3457 - val_accuracy: 0.8803 Epoch 20/200 240/240 [==============================] - 0s 943us/step - loss: 0.2741 - accuracy: 0.9007 - val_loss: 0.3373 - val_accuracy: 0.8814 Epoch 21/200 240/240 [==============================] - 0s 883us/step - loss: 0.2699 - accuracy: 0.9016 - val_loss: 0.3352 - val_accuracy: 0.8832 Epoch 22/200 240/240 [==============================] - 0s 880us/step - loss: 0.2644 - accuracy: 0.9036 - val_loss: 0.3320 - val_accuracy: 0.8817 Epoch 23/200 240/240 [==============================] - 0s 948us/step - loss: 0.2610 - accuracy: 0.9059 - val_loss: 0.3384 - val_accuracy: 0.8768 Epoch 24/200 240/240 [==============================] - 0s 946us/step - loss: 0.2575 - accuracy: 0.9076 - val_loss: 0.3446 - val_accuracy: 0.8785 Epoch 25/200 240/240 [==============================] - 0s 939us/step - loss: 0.2532 - accuracy: 0.9084 - val_loss: 0.3312 - val_accuracy: 0.8820 Epoch 26/200 240/240 [==============================] - 0s 967us/step - loss: 0.2509 - accuracy: 0.9094 - val_loss: 0.3383 - val_accuracy: 0.8833 Epoch 27/200 240/240 [==============================] - 0s 929us/step - loss: 0.2487 - accuracy: 0.9106 - val_loss: 0.3365 - val_accuracy: 0.8827 Epoch 28/200 240/240 [==============================] - 0s 943us/step - loss: 0.2450 - accuracy: 0.9123 - val_loss: 0.3376 - val_accuracy: 0.8827 Epoch 29/200 240/240 [==============================] - 0s 922us/step - loss: 0.2424 - accuracy: 0.9122 - val_loss: 0.3346 - val_accuracy: 0.8823 Epoch 30/200 240/240 [==============================] - 0s 903us/step - loss: 0.2407 - accuracy: 0.9137 - val_loss: 0.3367 - val_accuracy: 0.8813 Epoch 31/200 240/240 [==============================] - 0s 906us/step - loss: 0.2380 - accuracy: 0.9147 - val_loss: 0.3376 - val_accuracy: 0.8813 Epoch 32/200 240/240 [==============================] - 0s 885us/step - loss: 0.2349 - accuracy: 0.9155 - val_loss: 0.3372 - val_accuracy: 0.8856 Epoch 33/200 240/240 [==============================] - 0s 891us/step - loss: 0.2324 - accuracy: 0.9167 - val_loss: 0.3362 - val_accuracy: 0.8833 Epoch 34/200 240/240 [==============================] - 0s 900us/step - loss: 0.2285 - accuracy: 0.9177 - val_loss: 0.3486 - val_accuracy: 0.8810 Epoch 35/200 240/240 [==============================] - 0s 858us/step - loss: 0.2270 - accuracy: 0.9188 - val_loss: 0.3364 - val_accuracy: 0.8817 Epoch 36/200 240/240 [==============================] - 0s 893us/step - loss: 0.2241 - accuracy: 0.9204 - val_loss: 0.3401 - val_accuracy: 0.8852 Epoch 37/200 240/240 [==============================] - 0s 907us/step - loss: 0.2258 - accuracy: 0.9178 - val_loss: 0.3451 - val_accuracy: 0.8811 Epoch 38/200 240/240 [==============================] - 0s 899us/step - loss: 0.2249 - accuracy: 0.9196 - val_loss: 0.3377 - val_accuracy: 0.8836 Epoch 39/200 240/240 [==============================] - 0s 876us/step - loss: 0.2187 - accuracy: 0.9215 - val_loss: 0.3298 - val_accuracy: 0.8867 Epoch 40/200 240/240 [==============================] - 0s 905us/step - loss: 0.2148 - accuracy: 0.9229 - val_loss: 0.3342 - val_accuracy: 0.8853 Epoch 41/200 240/240 [==============================] - 0s 954us/step - loss: 0.2118 - accuracy: 0.9240 - val_loss: 0.3378 - val_accuracy: 0.8840 Epoch 42/200 240/240 [==============================] - 0s 916us/step - loss: 0.2135 - accuracy: 0.9243 - val_loss: 0.3348 - val_accuracy: 0.8857 Epoch 43/200 240/240 [==============================] - 0s 943us/step - loss: 0.2101 - accuracy: 0.9247 - val_loss: 0.3369 - val_accuracy: 0.8857 Epoch 44/200 240/240 [==============================] - 0s 877us/step - loss: 0.2069 - accuracy: 0.9261 - val_loss: 0.3400 - val_accuracy: 0.8832 Epoch 45/200 240/240 [==============================] - 0s 900us/step - loss: 0.2040 - accuracy: 0.9267 - val_loss: 0.3358 - val_accuracy: 0.8854 Epoch 46/200 240/240 [==============================] - 0s 926us/step - loss: 0.2029 - accuracy: 0.9271 - val_loss: 0.3562 - val_accuracy: 0.8802 Epoch 47/200 240/240 [==============================] - 0s 901us/step - loss: 0.2052 - accuracy: 0.9263 - val_loss: 0.3509 - val_accuracy: 0.8841 Epoch 48/200 240/240 [==============================] - 0s 927us/step - loss: 0.1977 - accuracy: 0.9296 - val_loss: 0.3434 - val_accuracy: 0.8852 Epoch 49/200 240/240 [==============================] - 0s 870us/step - loss: 0.1980 - accuracy: 0.9281 - val_loss: 0.3587 - val_accuracy: 0.8810 Epoch 50/200 240/240 [==============================] - 0s 874us/step - loss: 0.1972 - accuracy: 0.9292 - val_loss: 0.3483 - val_accuracy: 0.8828 Epoch 51/200 240/240 [==============================] - 0s 924us/step - loss: 0.1973 - accuracy: 0.9286 - val_loss: 0.3518 - val_accuracy: 0.8834 Epoch 52/200 240/240 [==============================] - 0s 886us/step - loss: 0.1920 - accuracy: 0.9309 - val_loss: 0.3647 - val_accuracy: 0.8796 Epoch 53/200 240/240 [==============================] - 0s 901us/step - loss: 0.1935 - accuracy: 0.9306 - val_loss: 0.3571 - val_accuracy: 0.8831 Epoch 54/200 240/240 [==============================] - 0s 946us/step - loss: 0.1918 - accuracy: 0.9311 - val_loss: 0.3545 - val_accuracy: 0.8834 Epoch 55/200 240/240 [==============================] - 0s 950us/step - loss: 0.1921 - accuracy: 0.9306 - val_loss: 0.3643 - val_accuracy: 0.8823 Epoch 56/200 240/240 [==============================] - 0s 913us/step - loss: 0.1870 - accuracy: 0.9335 - val_loss: 0.3665 - val_accuracy: 0.8798 Epoch 57/200 240/240 [==============================] - 0s 908us/step - loss: 0.1857 - accuracy: 0.9328 - val_loss: 0.3539 - val_accuracy: 0.8833 Epoch 58/200 240/240 [==============================] - 0s 919us/step - loss: 0.1815 - accuracy: 0.9353 - val_loss: 0.3569 - val_accuracy: 0.8833 Epoch 59/200 240/240 [==============================] - 0s 863us/step - loss: 0.1832 - accuracy: 0.9337 - val_loss: 0.3603 - val_accuracy: 0.8833 Epoch 60/200 240/240 [==============================] - 0s 912us/step - loss: 0.1804 - accuracy: 0.9361 - val_loss: 0.3690 - val_accuracy: 0.8812 Epoch 61/200 240/240 [==============================] - 0s 858us/step - loss: 0.1769 - accuracy: 0.9368 - val_loss: 0.3624 - val_accuracy: 0.8840 Epoch 62/200 240/240 [==============================] - 0s 885us/step - loss: 0.1756 - accuracy: 0.9366 - val_loss: 0.3637 - val_accuracy: 0.8829 Epoch 63/200 240/240 [==============================] - 0s 903us/step - loss: 0.1766 - accuracy: 0.9363 - val_loss: 0.3663 - val_accuracy: 0.8824 Epoch 64/200 240/240 [==============================] - 0s 878us/step - loss: 0.1767 - accuracy: 0.9363 - val_loss: 0.3694 - val_accuracy: 0.8825 Epoch 65/200 240/240 [==============================] - 0s 866us/step - loss: 0.1733 - accuracy: 0.9377 - val_loss: 0.3820 - val_accuracy: 0.8838 Epoch 66/200 240/240 [==============================] - 0s 949us/step - loss: 0.1742 - accuracy: 0.9370 - val_loss: 0.3721 - val_accuracy: 0.8825 Epoch 67/200 240/240 [==============================] - 0s 887us/step - loss: 0.1698 - accuracy: 0.9386 - val_loss: 0.3717 - val_accuracy: 0.8838 Epoch 68/200 240/240 [==============================] - 0s 942us/step - loss: 0.1683 - accuracy: 0.9399 - val_loss: 0.3823 - val_accuracy: 0.8821 Epoch 69/200 240/240 [==============================] - 0s 951us/step - loss: 0.1680 - accuracy: 0.9406 - val_loss: 0.3739 - val_accuracy: 0.8865 Epoch 70/200 240/240 [==============================] - 0s 900us/step - loss: 0.1673 - accuracy: 0.9395 - val_loss: 0.3789 - val_accuracy: 0.8821 Epoch 71/200 240/240 [==============================] - 0s 904us/step - loss: 0.1671 - accuracy: 0.9396 - val_loss: 0.3881 - val_accuracy: 0.8808 Epoch 72/200 240/240 [==============================] - 0s 915us/step - loss: 0.1664 - accuracy: 0.9396 - val_loss: 0.3821 - val_accuracy: 0.8824 Epoch 73/200 240/240 [==============================] - 0s 899us/step - loss: 0.1603 - accuracy: 0.9433 - val_loss: 0.3864 - val_accuracy: 0.8822 Epoch 74/200 240/240 [==============================] - 0s 926us/step - loss: 0.1621 - accuracy: 0.9411 - val_loss: 0.3850 - val_accuracy: 0.8820 Epoch 75/200 240/240 [==============================] - 0s 902us/step - loss: 0.1578 - accuracy: 0.9439 - val_loss: 0.3827 - val_accuracy: 0.8838 Epoch 76/200 240/240 [==============================] - 0s 899us/step - loss: 0.1589 - accuracy: 0.9431 - val_loss: 0.4160 - val_accuracy: 0.8751 Epoch 77/200 240/240 [==============================] - 0s 909us/step - loss: 0.1597 - accuracy: 0.9426 - val_loss: 0.3934 - val_accuracy: 0.8810 Epoch 78/200 240/240 [==============================] - 0s 894us/step - loss: 0.1582 - accuracy: 0.9420 - val_loss: 0.4076 - val_accuracy: 0.8777 Epoch 79/200 240/240 [==============================] - 0s 897us/step - loss: 0.1573 - accuracy: 0.9439 - val_loss: 0.3890 - val_accuracy: 0.8832 Epoch 80/200 240/240 [==============================] - 0s 917us/step - loss: 0.1567 - accuracy: 0.9445 - val_loss: 0.4039 - val_accuracy: 0.8805 Epoch 81/200 240/240 [==============================] - 0s 921us/step - loss: 0.1529 - accuracy: 0.9455 - val_loss: 0.3967 - val_accuracy: 0.8818 Epoch 82/200 240/240 [==============================] - 0s 919us/step - loss: 0.1522 - accuracy: 0.9456 - val_loss: 0.4028 - val_accuracy: 0.8796 Epoch 83/200 240/240 [==============================] - 0s 984us/step - loss: 0.1501 - accuracy: 0.9462 - val_loss: 0.4147 - val_accuracy: 0.8802 Epoch 84/200 240/240 [==============================] - 0s 873us/step - loss: 0.1493 - accuracy: 0.9466 - val_loss: 0.3956 - val_accuracy: 0.8818 Epoch 85/200 240/240 [==============================] - 0s 882us/step - loss: 0.1484 - accuracy: 0.9470 - val_loss: 0.4121 - val_accuracy: 0.8807 Epoch 86/200 240/240 [==============================] - 0s 894us/step - loss: 0.1504 - accuracy: 0.9449 - val_loss: 0.4089 - val_accuracy: 0.8790 Epoch 87/200 240/240 [==============================] - 0s 924us/step - loss: 0.1437 - accuracy: 0.9493 - val_loss: 0.4243 - val_accuracy: 0.8776 Epoch 88/200 240/240 [==============================] - 0s 905us/step - loss: 0.1462 - accuracy: 0.9478 - val_loss: 0.4123 - val_accuracy: 0.8799 Epoch 89/200 240/240 [==============================] - 0s 950us/step - loss: 0.1444 - accuracy: 0.9479 - val_loss: 0.4105 - val_accuracy: 0.8823 Epoch 90/200 240/240 [==============================] - 0s 908us/step - loss: 0.1460 - accuracy: 0.9482 - val_loss: 0.4103 - val_accuracy: 0.8827 Epoch 91/200 240/240 [==============================] - 0s 889us/step - loss: 0.1425 - accuracy: 0.9489 - val_loss: 0.4112 - val_accuracy: 0.8819 Epoch 92/200 240/240 [==============================] - 0s 869us/step - loss: 0.1455 - accuracy: 0.9483 - val_loss: 0.4115 - val_accuracy: 0.8818 Epoch 93/200 240/240 [==============================] - 0s 917us/step - loss: 0.1412 - accuracy: 0.9491 - val_loss: 0.4177 - val_accuracy: 0.8805 Epoch 94/200 240/240 [==============================] - 0s 865us/step - loss: 0.1398 - accuracy: 0.9500 - val_loss: 0.4177 - val_accuracy: 0.8813 Epoch 95/200 240/240 [==============================] - 0s 910us/step - loss: 0.1417 - accuracy: 0.9504 - val_loss: 0.4248 - val_accuracy: 0.8796 Epoch 96/200 240/240 [==============================] - 0s 917us/step - loss: 0.1392 - accuracy: 0.9499 - val_loss: 0.4207 - val_accuracy: 0.8840 Epoch 97/200 240/240 [==============================] - 0s 872us/step - loss: 0.1366 - accuracy: 0.9514 - val_loss: 0.4218 - val_accuracy: 0.8810 Epoch 98/200 240/240 [==============================] - 0s 896us/step - loss: 0.1344 - accuracy: 0.9519 - val_loss: 0.4281 - val_accuracy: 0.8794 Epoch 99/200 240/240 [==============================] - 0s 928us/step - loss: 0.1346 - accuracy: 0.9521 - val_loss: 0.4304 - val_accuracy: 0.8803 Epoch 100/200 240/240 [==============================] - 0s 849us/step - loss: 0.1368 - accuracy: 0.9511 - val_loss: 0.4335 - val_accuracy: 0.8800 Epoch 101/200 240/240 [==============================] - 0s 930us/step - loss: 0.1317 - accuracy: 0.9540 - val_loss: 0.4345 - val_accuracy: 0.8799 Epoch 102/200 240/240 [==============================] - 0s 919us/step - loss: 0.1318 - accuracy: 0.9530 - val_loss: 0.4430 - val_accuracy: 0.8774 Epoch 103/200 240/240 [==============================] - 0s 905us/step - loss: 0.1306 - accuracy: 0.9538 - val_loss: 0.4427 - val_accuracy: 0.8783 Epoch 104/200 240/240 [==============================] - 0s 892us/step - loss: 0.1322 - accuracy: 0.9532 - val_loss: 0.4409 - val_accuracy: 0.8797 Epoch 105/200 240/240 [==============================] - 0s 853us/step - loss: 0.1322 - accuracy: 0.9524 - val_loss: 0.4631 - val_accuracy: 0.8759 Epoch 106/200 240/240 [==============================] - 0s 901us/step - loss: 0.1295 - accuracy: 0.9540 - val_loss: 0.4451 - val_accuracy: 0.8811 Epoch 107/200 240/240 [==============================] - 0s 910us/step - loss: 0.1287 - accuracy: 0.9539 - val_loss: 0.4393 - val_accuracy: 0.8795 Epoch 108/200 240/240 [==============================] - 0s 927us/step - loss: 0.1265 - accuracy: 0.9549 - val_loss: 0.4547 - val_accuracy: 0.8783 Epoch 109/200 240/240 [==============================] - 0s 899us/step - loss: 0.1257 - accuracy: 0.9553 - val_loss: 0.4467 - val_accuracy: 0.8798 Epoch 110/200 240/240 [==============================] - 0s 911us/step - loss: 0.1264 - accuracy: 0.9558 - val_loss: 0.4494 - val_accuracy: 0.8775 Epoch 111/200 240/240 [==============================] - 0s 911us/step - loss: 0.1256 - accuracy: 0.9550 - val_loss: 0.4600 - val_accuracy: 0.8777 Epoch 112/200 240/240 [==============================] - 0s 897us/step - loss: 0.1241 - accuracy: 0.9561 - val_loss: 0.4468 - val_accuracy: 0.8785 Epoch 113/200 240/240 [==============================] - 0s 904us/step - loss: 0.1242 - accuracy: 0.9556 - val_loss: 0.4592 - val_accuracy: 0.8788 Epoch 114/200 240/240 [==============================] - 0s 913us/step - loss: 0.1224 - accuracy: 0.9574 - val_loss: 0.4640 - val_accuracy: 0.8778 Epoch 115/200 240/240 [==============================] - 0s 912us/step - loss: 0.1235 - accuracy: 0.9563 - val_loss: 0.4590 - val_accuracy: 0.8777 Epoch 116/200 240/240 [==============================] - 0s 907us/step - loss: 0.1222 - accuracy: 0.9568 - val_loss: 0.4762 - val_accuracy: 0.8781 Epoch 117/200 240/240 [==============================] - 0s 913us/step - loss: 0.1178 - accuracy: 0.9583 - val_loss: 0.4585 - val_accuracy: 0.8818 Epoch 118/200 240/240 [==============================] - 0s 901us/step - loss: 0.1207 - accuracy: 0.9571 - val_loss: 0.4796 - val_accuracy: 0.8770 Epoch 119/200 240/240 [==============================] - 0s 883us/step - loss: 0.1194 - accuracy: 0.9574 - val_loss: 0.4711 - val_accuracy: 0.8790 Epoch 120/200 240/240 [==============================] - 0s 953us/step - loss: 0.1204 - accuracy: 0.9565 - val_loss: 0.4612 - val_accuracy: 0.8796 Epoch 121/200 240/240 [==============================] - 0s 887us/step - loss: 0.1138 - accuracy: 0.9596 - val_loss: 0.4792 - val_accuracy: 0.8770 Epoch 122/200 240/240 [==============================] - 0s 874us/step - loss: 0.1142 - accuracy: 0.9602 - val_loss: 0.4784 - val_accuracy: 0.8762 Epoch 123/200 240/240 [==============================] - 0s 914us/step - loss: 0.1184 - accuracy: 0.9574 - val_loss: 0.4791 - val_accuracy: 0.8787 Epoch 124/200 240/240 [==============================] - 0s 922us/step - loss: 0.1161 - accuracy: 0.9593 - val_loss: 0.4876 - val_accuracy: 0.8763 Epoch 125/200 240/240 [==============================] - 0s 882us/step - loss: 0.1159 - accuracy: 0.9584 - val_loss: 0.4888 - val_accuracy: 0.8745 Epoch 126/200 240/240 [==============================] - 0s 951us/step - loss: 0.1140 - accuracy: 0.9597 - val_loss: 0.5025 - val_accuracy: 0.8754 Epoch 127/200 240/240 [==============================] - 0s 900us/step - loss: 0.1151 - accuracy: 0.9593 - val_loss: 0.4892 - val_accuracy: 0.8747 Epoch 128/200 240/240 [==============================] - 0s 890us/step - loss: 0.1100 - accuracy: 0.9611 - val_loss: 0.4833 - val_accuracy: 0.8777 Epoch 129/200 240/240 [==============================] - 0s 890us/step - loss: 0.1121 - accuracy: 0.9606 - val_loss: 0.4996 - val_accuracy: 0.8720 Epoch 130/200 240/240 [==============================] - 0s 892us/step - loss: 0.1097 - accuracy: 0.9614 - val_loss: 0.4904 - val_accuracy: 0.8779 Epoch 131/200 240/240 [==============================] - 0s 888us/step - loss: 0.1084 - accuracy: 0.9620 - val_loss: 0.4944 - val_accuracy: 0.8748 Epoch 132/200 240/240 [==============================] - 0s 931us/step - loss: 0.1123 - accuracy: 0.9604 - val_loss: 0.4892 - val_accuracy: 0.8778 Epoch 133/200 240/240 [==============================] - 0s 896us/step - loss: 0.1097 - accuracy: 0.9621 - val_loss: 0.5165 - val_accuracy: 0.8733 Epoch 134/200 240/240 [==============================] - 0s 897us/step - loss: 0.1050 - accuracy: 0.9631 - val_loss: 0.5124 - val_accuracy: 0.8731 Epoch 135/200 240/240 [==============================] - 0s 915us/step - loss: 0.1093 - accuracy: 0.9618 - val_loss: 0.5165 - val_accuracy: 0.8733 Epoch 136/200 240/240 [==============================] - 0s 909us/step - loss: 0.1045 - accuracy: 0.9640 - val_loss: 0.5045 - val_accuracy: 0.8781 Epoch 137/200 240/240 [==============================] - 0s 931us/step - loss: 0.1056 - accuracy: 0.9627 - val_loss: 0.5124 - val_accuracy: 0.8773 Epoch 138/200 240/240 [==============================] - 0s 905us/step - loss: 0.1089 - accuracy: 0.9610 - val_loss: 0.5152 - val_accuracy: 0.8751 Epoch 139/200 240/240 [==============================] - 0s 922us/step - loss: 0.1059 - accuracy: 0.9628 - val_loss: 0.5150 - val_accuracy: 0.8744 Epoch 140/200 240/240 [==============================] - 0s 923us/step - loss: 0.1031 - accuracy: 0.9638 - val_loss: 0.5156 - val_accuracy: 0.8740 Epoch 141/200 240/240 [==============================] - 0s 870us/step - loss: 0.1063 - accuracy: 0.9620 - val_loss: 0.5207 - val_accuracy: 0.8719 Epoch 142/200 240/240 [==============================] - 0s 904us/step - loss: 0.1054 - accuracy: 0.9621 - val_loss: 0.5220 - val_accuracy: 0.8740 Epoch 143/200 240/240 [==============================] - 0s 929us/step - loss: 0.1043 - accuracy: 0.9623 - val_loss: 0.5356 - val_accuracy: 0.8736 Epoch 144/200 240/240 [==============================] - 0s 869us/step - loss: 0.1039 - accuracy: 0.9636 - val_loss: 0.5403 - val_accuracy: 0.8737 Epoch 145/200 240/240 [==============================] - 0s 872us/step - loss: 0.1014 - accuracy: 0.9649 - val_loss: 0.5294 - val_accuracy: 0.8751 Epoch 146/200 240/240 [==============================] - 0s 903us/step - loss: 0.1027 - accuracy: 0.9636 - val_loss: 0.5321 - val_accuracy: 0.8770 Epoch 147/200 240/240 [==============================] - 0s 906us/step - loss: 0.1012 - accuracy: 0.9646 - val_loss: 0.5329 - val_accuracy: 0.8748 Epoch 148/200 240/240 [==============================] - 0s 909us/step - loss: 0.1006 - accuracy: 0.9645 - val_loss: 0.5368 - val_accuracy: 0.8743 Epoch 149/200 240/240 [==============================] - 0s 872us/step - loss: 0.0975 - accuracy: 0.9656 - val_loss: 0.5320 - val_accuracy: 0.8759 Epoch 150/200 240/240 [==============================] - 0s 895us/step - loss: 0.0985 - accuracy: 0.9660 - val_loss: 0.5357 - val_accuracy: 0.8745 Epoch 151/200 240/240 [==============================] - 0s 896us/step - loss: 0.0962 - accuracy: 0.9668 - val_loss: 0.5353 - val_accuracy: 0.8748 Epoch 152/200 240/240 [==============================] - 0s 936us/step - loss: 0.0955 - accuracy: 0.9674 - val_loss: 0.5318 - val_accuracy: 0.8763 Epoch 153/200 240/240 [==============================] - 0s 878us/step - loss: 0.0970 - accuracy: 0.9660 - val_loss: 0.5866 - val_accuracy: 0.8702 Epoch 154/200 240/240 [==============================] - 0s 940us/step - loss: 0.0992 - accuracy: 0.9646 - val_loss: 0.5421 - val_accuracy: 0.8750 Epoch 155/200 240/240 [==============================] - 0s 931us/step - loss: 0.0965 - accuracy: 0.9661 - val_loss: 0.5436 - val_accuracy: 0.8739 Epoch 156/200 240/240 [==============================] - 0s 939us/step - loss: 0.0959 - accuracy: 0.9666 - val_loss: 0.5542 - val_accuracy: 0.8745 Epoch 157/200 240/240 [==============================] - 0s 898us/step - loss: 0.0980 - accuracy: 0.9647 - val_loss: 0.5441 - val_accuracy: 0.8747 Epoch 158/200 240/240 [==============================] - 0s 905us/step - loss: 0.0925 - accuracy: 0.9676 - val_loss: 0.5507 - val_accuracy: 0.8746 Epoch 159/200 240/240 [==============================] - 0s 905us/step - loss: 0.0950 - accuracy: 0.9663 - val_loss: 0.5700 - val_accuracy: 0.8712 Epoch 160/200 240/240 [==============================] - 0s 927us/step - loss: 0.0908 - accuracy: 0.9679 - val_loss: 0.5641 - val_accuracy: 0.8727 Epoch 161/200 240/240 [==============================] - 0s 937us/step - loss: 0.0936 - accuracy: 0.9668 - val_loss: 0.5689 - val_accuracy: 0.8750 Epoch 162/200 240/240 [==============================] - 0s 899us/step - loss: 0.0888 - accuracy: 0.9694 - val_loss: 0.5641 - val_accuracy: 0.8772 Epoch 163/200 240/240 [==============================] - 0s 907us/step - loss: 0.0903 - accuracy: 0.9685 - val_loss: 0.5628 - val_accuracy: 0.8737 Epoch 164/200 240/240 [==============================] - 0s 908us/step - loss: 0.0923 - accuracy: 0.9675 - val_loss: 0.5623 - val_accuracy: 0.8752 Epoch 165/200 240/240 [==============================] - 0s 949us/step - loss: 0.0880 - accuracy: 0.9696 - val_loss: 0.5775 - val_accuracy: 0.8713 Epoch 166/200 240/240 [==============================] - 0s 930us/step - loss: 0.0919 - accuracy: 0.9672 - val_loss: 0.5924 - val_accuracy: 0.8719 Epoch 167/200 240/240 [==============================] - 0s 932us/step - loss: 0.0868 - accuracy: 0.9697 - val_loss: 0.5952 - val_accuracy: 0.8719 Epoch 168/200 240/240 [==============================] - 0s 861us/step - loss: 0.0910 - accuracy: 0.9679 - val_loss: 0.5820 - val_accuracy: 0.8733 Epoch 169/200 240/240 [==============================] - 0s 938us/step - loss: 0.0871 - accuracy: 0.9700 - val_loss: 0.5781 - val_accuracy: 0.8734 Epoch 170/200 240/240 [==============================] - 0s 912us/step - loss: 0.0885 - accuracy: 0.9691 - val_loss: 0.5683 - val_accuracy: 0.8742 Epoch 171/200 240/240 [==============================] - 0s 939us/step - loss: 0.0894 - accuracy: 0.9680 - val_loss: 0.5945 - val_accuracy: 0.8721 Epoch 172/200 240/240 [==============================] - 0s 888us/step - loss: 0.0878 - accuracy: 0.9684 - val_loss: 0.5798 - val_accuracy: 0.8744 Epoch 173/200 240/240 [==============================] - 0s 917us/step - loss: 0.0852 - accuracy: 0.9710 - val_loss: 0.6017 - val_accuracy: 0.8690 Epoch 174/200 240/240 [==============================] - 0s 872us/step - loss: 0.0838 - accuracy: 0.9714 - val_loss: 0.5840 - val_accuracy: 0.8753 Epoch 175/200 240/240 [==============================] - 0s 937us/step - loss: 0.0863 - accuracy: 0.9697 - val_loss: 0.5770 - val_accuracy: 0.8726 Epoch 176/200 240/240 [==============================] - 0s 892us/step - loss: 0.0854 - accuracy: 0.9697 - val_loss: 0.5971 - val_accuracy: 0.8741 Epoch 177/200 240/240 [==============================] - 0s 924us/step - loss: 0.0896 - accuracy: 0.9674 - val_loss: 0.5859 - val_accuracy: 0.8743 Epoch 178/200 240/240 [==============================] - 0s 904us/step - loss: 0.0845 - accuracy: 0.9699 - val_loss: 0.6103 - val_accuracy: 0.8723 Epoch 179/200 240/240 [==============================] - 0s 889us/step - loss: 0.0863 - accuracy: 0.9688 - val_loss: 0.6157 - val_accuracy: 0.8708 Epoch 180/200 240/240 [==============================] - 0s 902us/step - loss: 0.0850 - accuracy: 0.9697 - val_loss: 0.5974 - val_accuracy: 0.8752 Epoch 181/200 240/240 [==============================] - 0s 899us/step - loss: 0.0825 - accuracy: 0.9710 - val_loss: 0.6102 - val_accuracy: 0.8736 Epoch 182/200 240/240 [==============================] - 0s 895us/step - loss: 0.0833 - accuracy: 0.9709 - val_loss: 0.6265 - val_accuracy: 0.8690 Epoch 183/200 240/240 [==============================] - 0s 899us/step - loss: 0.0821 - accuracy: 0.9715 - val_loss: 0.6197 - val_accuracy: 0.8718 Epoch 184/200 240/240 [==============================] - 0s 940us/step - loss: 0.0791 - accuracy: 0.9727 - val_loss: 0.6139 - val_accuracy: 0.8727 Epoch 185/200 240/240 [==============================] - 0s 880us/step - loss: 0.0815 - accuracy: 0.9716 - val_loss: 0.6055 - val_accuracy: 0.8718 Epoch 186/200 240/240 [==============================] - 0s 878us/step - loss: 0.0860 - accuracy: 0.9696 - val_loss: 0.6192 - val_accuracy: 0.8713 Epoch 187/200 240/240 [==============================] - 0s 925us/step - loss: 0.0905 - accuracy: 0.9680 - val_loss: 0.6147 - val_accuracy: 0.8722 Epoch 188/200 240/240 [==============================] - 0s 884us/step - loss: 0.0826 - accuracy: 0.9707 - val_loss: 0.6203 - val_accuracy: 0.8737 Epoch 189/200 240/240 [==============================] - 0s 934us/step - loss: 0.0810 - accuracy: 0.9722 - val_loss: 0.6316 - val_accuracy: 0.8701 Epoch 190/200 240/240 [==============================] - 0s 899us/step - loss: 0.0763 - accuracy: 0.9735 - val_loss: 0.6246 - val_accuracy: 0.8758 Epoch 191/200 240/240 [==============================] - 0s 888us/step - loss: 0.0767 - accuracy: 0.9736 - val_loss: 0.6358 - val_accuracy: 0.8712 Epoch 192/200 240/240 [==============================] - 0s 867us/step - loss: 0.0792 - accuracy: 0.9728 - val_loss: 0.6281 - val_accuracy: 0.8749 Epoch 193/200 240/240 [==============================] - 0s 932us/step - loss: 0.0764 - accuracy: 0.9741 - val_loss: 0.6248 - val_accuracy: 0.8737 Epoch 194/200 240/240 [==============================] - 0s 919us/step - loss: 0.0765 - accuracy: 0.9733 - val_loss: 0.6223 - val_accuracy: 0.8734 Epoch 195/200 240/240 [==============================] - 0s 870us/step - loss: 0.0796 - accuracy: 0.9724 - val_loss: 0.6413 - val_accuracy: 0.8707 Epoch 196/200 240/240 [==============================] - 0s 903us/step - loss: 0.0750 - accuracy: 0.9742 - val_loss: 0.6315 - val_accuracy: 0.8737 Epoch 197/200 240/240 [==============================] - 0s 901us/step - loss: 0.0789 - accuracy: 0.9724 - val_loss: 0.6602 - val_accuracy: 0.8684 Epoch 198/200 240/240 [==============================] - 0s 915us/step - loss: 0.0793 - accuracy: 0.9720 - val_loss: 0.6615 - val_accuracy: 0.8698 Epoch 199/200 240/240 [==============================] - 0s 892us/step - loss: 0.0727 - accuracy: 0.9753 - val_loss: 0.6486 - val_accuracy: 0.8692 Epoch 200/200 240/240 [==============================] - 0s 937us/step - loss: 0.0736 - accuracy: 0.9747 - val_loss: 0.6474 - val_accuracy: 0.8723 . &lt;keras.callbacks.History at 0x7f1b802e05b0&gt; . . - 텐서보드 여는 방법1 . %load_ext tensorboard # 주피터노트북 (혹은 주피터랩)에서 텐서보드를 임베딩하여 넣을 수 있도록 도와주는 매직펑션 . # !rm -rf logs # !kill 313799 . # %tensorboard --logdir logs --host 0.0.0.0 # %tensorboard --logdir logs # &lt;-- 실습에서는 이렇게 하면됩니다. . (참고사항) 파이썬 3.10의 경우 아래의 수정이 필요 . ?/python3.10/site-packages/tensorboard/_vendor/html5lib/_trie/_base.py 을 열고 . from collections import Mapping ### 수정전 from collections.abc import Mapping ### 수정후 . 와 같이 수정한다. . 왜냐하면 파이썬 3.10부터 from collections import Mapping 가 동작하지 않고 from collections.abc import Mapping 가 동작하도록 문법이 바뀜 | . - 텐서보드를 실행하는 방법2 . # !tensorboard --logdir logs --host 0.0.0.0 # !tensorboard --logdir logs # &lt;-- 실습에서는 이렇게 하면됩니다. . &#51312;&#44592;&#51333;&#47308; . - 텐서보드를 살펴보니 특정에폭 이후에는 오히려 과적합이 진행되는 듯 하다 (학습할수록 손해인듯 하다) $ to$ 그 특정에폭까지만 학습해보자 . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(5000,activation=&#39;relu&#39;)) ## 과적합좀 시키려고 net.add(tf.keras.layers.Dense(5000,activation=&#39;relu&#39;)) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가 net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.categorical_crossentropy,metrics=&#39;accuracy&#39;) . #cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) . Epoch 1/200 240/240 [==============================] - 1s 4ms/step - loss: 0.5483 - accuracy: 0.8134 - val_loss: 0.4027 - val_accuracy: 0.8546 Epoch 2/200 240/240 [==============================] - 1s 3ms/step - loss: 0.3568 - accuracy: 0.8671 - val_loss: 0.3531 - val_accuracy: 0.8712 Epoch 3/200 240/240 [==============================] - 1s 3ms/step - loss: 0.3210 - accuracy: 0.8799 - val_loss: 0.3477 - val_accuracy: 0.8733 Epoch 4/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2971 - accuracy: 0.8876 - val_loss: 0.3502 - val_accuracy: 0.8776 . &lt;keras.callbacks.History at 0x7f1b80086650&gt; . #cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) . Epoch 1/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2791 - accuracy: 0.8935 - val_loss: 0.3224 - val_accuracy: 0.8820 Epoch 2/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2619 - accuracy: 0.8999 - val_loss: 0.3498 - val_accuracy: 0.8779 . &lt;keras.callbacks.History at 0x7f1b24290a90&gt; . #cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) . Epoch 1/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2491 - accuracy: 0.9043 - val_loss: 0.3641 - val_accuracy: 0.8711 Epoch 2/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2328 - accuracy: 0.9110 - val_loss: 0.3282 - val_accuracy: 0.8848 Epoch 3/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2254 - accuracy: 0.9151 - val_loss: 0.3280 - val_accuracy: 0.8843 Epoch 4/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2144 - accuracy: 0.9177 - val_loss: 0.3191 - val_accuracy: 0.8925 Epoch 5/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2074 - accuracy: 0.9223 - val_loss: 0.3152 - val_accuracy: 0.8949 Epoch 6/200 240/240 [==============================] - 1s 3ms/step - loss: 0.1952 - accuracy: 0.9250 - val_loss: 0.3322 - val_accuracy: 0.8863 . &lt;keras.callbacks.History at 0x7f1b242c1660&gt; . #cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) . Epoch 1/200 240/240 [==============================] - 1s 3ms/step - loss: 0.1908 - accuracy: 0.9257 - val_loss: 0.3513 - val_accuracy: 0.8836 Epoch 2/200 240/240 [==============================] - 1s 3ms/step - loss: 0.1799 - accuracy: 0.9304 - val_loss: 0.3376 - val_accuracy: 0.8901 Epoch 3/200 240/240 [==============================] - 1s 3ms/step - loss: 0.1712 - accuracy: 0.9346 - val_loss: 0.3568 - val_accuracy: 0.8894 . &lt;keras.callbacks.History at 0x7f1b24302230&gt; . #cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=1) # val-loss가 1회증가하면 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) . Epoch 1/200 240/240 [==============================] - 1s 3ms/step - loss: 0.1591 - accuracy: 0.9367 - val_loss: 0.3995 - val_accuracy: 0.8780 Epoch 2/200 240/240 [==============================] - 1s 3ms/step - loss: 0.1552 - accuracy: 0.9398 - val_loss: 0.3469 - val_accuracy: 0.8917 Epoch 3/200 240/240 [==============================] - 1s 3ms/step - loss: 0.1481 - accuracy: 0.9423 - val_loss: 0.3726 - val_accuracy: 0.8853 . &lt;keras.callbacks.History at 0x7f1b24136e00&gt; . - 몇 번 좀 참았다가 멈추면 좋겠다. . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(5000,activation=&#39;relu&#39;)) ## 과적합좀 시키려고 net.add(tf.keras.layers.Dense(5000,activation=&#39;relu&#39;)) ## 레이어를 2장만듬 + 레이어하나당 노드수도 증가 net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.categorical_crossentropy,metrics=&#39;accuracy&#39;) . #cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=5) # 좀더 참다가 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=cb2,verbose=1) . Epoch 1/200 240/240 [==============================] - 1s 4ms/step - loss: 0.5475 - accuracy: 0.8139 - val_loss: 0.4219 - val_accuracy: 0.8453 Epoch 2/200 240/240 [==============================] - 1s 3ms/step - loss: 0.3575 - accuracy: 0.8676 - val_loss: 0.3647 - val_accuracy: 0.8712 Epoch 3/200 240/240 [==============================] - 1s 3ms/step - loss: 0.3219 - accuracy: 0.8792 - val_loss: 0.3559 - val_accuracy: 0.8710 Epoch 4/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2990 - accuracy: 0.8883 - val_loss: 0.3448 - val_accuracy: 0.8808 Epoch 5/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2759 - accuracy: 0.8966 - val_loss: 0.3337 - val_accuracy: 0.8792 Epoch 6/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2621 - accuracy: 0.9004 - val_loss: 0.3220 - val_accuracy: 0.8841 Epoch 7/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2478 - accuracy: 0.9074 - val_loss: 0.3302 - val_accuracy: 0.8858 Epoch 8/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2342 - accuracy: 0.9110 - val_loss: 0.3150 - val_accuracy: 0.8904 Epoch 9/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2261 - accuracy: 0.9144 - val_loss: 0.3117 - val_accuracy: 0.8932 Epoch 10/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2116 - accuracy: 0.9200 - val_loss: 0.3345 - val_accuracy: 0.8888 Epoch 11/200 240/240 [==============================] - 1s 3ms/step - loss: 0.2081 - accuracy: 0.9207 - val_loss: 0.3344 - val_accuracy: 0.8867 Epoch 12/200 240/240 [==============================] - 1s 3ms/step - loss: 0.1956 - accuracy: 0.9255 - val_loss: 0.3158 - val_accuracy: 0.8975 Epoch 13/200 240/240 [==============================] - 1s 3ms/step - loss: 0.1863 - accuracy: 0.9275 - val_loss: 0.3302 - val_accuracy: 0.8934 Epoch 14/200 240/240 [==============================] - 1s 3ms/step - loss: 0.1764 - accuracy: 0.9324 - val_loss: 0.3717 - val_accuracy: 0.8859 . &lt;keras.callbacks.History at 0x7f1b24301960&gt; . - 텐서보드로 그려보자? . # %tensorboard --logdir logs --host 0.0.0.0 # 아무것도 안나온다 -&gt; 왜? cb1을 써야 텐서보드가 나옴 . - 조기종료와 텐서보드를 같이 쓰려면? . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(50,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.categorical_crossentropy,metrics=&#39;accuracy&#39;) . cb1 = tf.keras.callbacks.TensorBoard() cb2 = tf.keras.callbacks.EarlyStopping(patience=7) # 좀더 참다가 멈추어라 net.fit(X,y,epochs=200,batch_size=200,validation_split=0.2,callbacks=[cb1,cb2]) . Epoch 1/200 240/240 [==============================] - 0s 1ms/step - loss: 0.7184 - accuracy: 0.7581 - val_loss: 0.5077 - val_accuracy: 0.8276 Epoch 2/200 240/240 [==============================] - 0s 890us/step - loss: 0.4752 - accuracy: 0.8386 - val_loss: 0.4793 - val_accuracy: 0.8342 Epoch 3/200 240/240 [==============================] - 0s 899us/step - loss: 0.4304 - accuracy: 0.8517 - val_loss: 0.4386 - val_accuracy: 0.8497 Epoch 4/200 240/240 [==============================] - 0s 880us/step - loss: 0.4048 - accuracy: 0.8582 - val_loss: 0.4029 - val_accuracy: 0.8603 Epoch 5/200 240/240 [==============================] - 0s 923us/step - loss: 0.3832 - accuracy: 0.8669 - val_loss: 0.3932 - val_accuracy: 0.8619 Epoch 6/200 240/240 [==============================] - 0s 934us/step - loss: 0.3697 - accuracy: 0.8705 - val_loss: 0.3842 - val_accuracy: 0.8657 Epoch 7/200 240/240 [==============================] - 0s 900us/step - loss: 0.3569 - accuracy: 0.8759 - val_loss: 0.3844 - val_accuracy: 0.8668 Epoch 8/200 240/240 [==============================] - 0s 889us/step - loss: 0.3482 - accuracy: 0.8774 - val_loss: 0.3679 - val_accuracy: 0.8708 Epoch 9/200 240/240 [==============================] - 0s 912us/step - loss: 0.3387 - accuracy: 0.8799 - val_loss: 0.3602 - val_accuracy: 0.8719 Epoch 10/200 240/240 [==============================] - 0s 923us/step - loss: 0.3299 - accuracy: 0.8820 - val_loss: 0.3610 - val_accuracy: 0.8748 Epoch 11/200 240/240 [==============================] - 0s 853us/step - loss: 0.3229 - accuracy: 0.8858 - val_loss: 0.3574 - val_accuracy: 0.8717 Epoch 12/200 240/240 [==============================] - 0s 904us/step - loss: 0.3157 - accuracy: 0.8873 - val_loss: 0.3572 - val_accuracy: 0.8743 Epoch 13/200 240/240 [==============================] - 0s 890us/step - loss: 0.3106 - accuracy: 0.8899 - val_loss: 0.3545 - val_accuracy: 0.8761 Epoch 14/200 240/240 [==============================] - 0s 911us/step - loss: 0.3046 - accuracy: 0.8914 - val_loss: 0.3493 - val_accuracy: 0.8759 Epoch 15/200 240/240 [==============================] - 0s 921us/step - loss: 0.3011 - accuracy: 0.8928 - val_loss: 0.3483 - val_accuracy: 0.8776 Epoch 16/200 240/240 [==============================] - 0s 937us/step - loss: 0.2988 - accuracy: 0.8935 - val_loss: 0.3733 - val_accuracy: 0.8716 Epoch 17/200 240/240 [==============================] - 0s 892us/step - loss: 0.2925 - accuracy: 0.8947 - val_loss: 0.3481 - val_accuracy: 0.8768 Epoch 18/200 240/240 [==============================] - 0s 933us/step - loss: 0.2880 - accuracy: 0.8951 - val_loss: 0.3396 - val_accuracy: 0.8801 Epoch 19/200 240/240 [==============================] - 0s 957us/step - loss: 0.2827 - accuracy: 0.8982 - val_loss: 0.3439 - val_accuracy: 0.8798 Epoch 20/200 240/240 [==============================] - 0s 881us/step - loss: 0.2791 - accuracy: 0.8986 - val_loss: 0.3489 - val_accuracy: 0.8779 Epoch 21/200 240/240 [==============================] - 0s 886us/step - loss: 0.2765 - accuracy: 0.9007 - val_loss: 0.3350 - val_accuracy: 0.8823 Epoch 22/200 240/240 [==============================] - 0s 912us/step - loss: 0.2709 - accuracy: 0.9016 - val_loss: 0.3350 - val_accuracy: 0.8812 Epoch 23/200 240/240 [==============================] - 0s 908us/step - loss: 0.2688 - accuracy: 0.9029 - val_loss: 0.3374 - val_accuracy: 0.8820 Epoch 24/200 240/240 [==============================] - 0s 930us/step - loss: 0.2658 - accuracy: 0.9041 - val_loss: 0.3445 - val_accuracy: 0.8805 Epoch 25/200 240/240 [==============================] - 0s 872us/step - loss: 0.2607 - accuracy: 0.9058 - val_loss: 0.3383 - val_accuracy: 0.8822 Epoch 26/200 240/240 [==============================] - 0s 928us/step - loss: 0.2607 - accuracy: 0.9056 - val_loss: 0.3415 - val_accuracy: 0.8811 Epoch 27/200 240/240 [==============================] - 0s 927us/step - loss: 0.2576 - accuracy: 0.9068 - val_loss: 0.3402 - val_accuracy: 0.8814 Epoch 28/200 240/240 [==============================] - 0s 905us/step - loss: 0.2525 - accuracy: 0.9098 - val_loss: 0.3469 - val_accuracy: 0.8802 . &lt;keras.callbacks.History at 0x7f1b24217a00&gt; . # 조기종료가 구현된 그림이 출력 # %tensorboard --logdir logs --host 0.0.0.0 . &#54616;&#51060;&#54140;&#54028;&#46972;&#47700;&#53552; &#49440;&#53469; . - 하이퍼파라메터 설정 . from tensorboard.plugins.hparams import api as hp . a=net.evaluate(XX,yy) . 313/313 [==============================] - 0s 859us/step - loss: 0.3803 - accuracy: 0.8704 . !rm -rf logs for u in [50,5000]: for d in [0.0,0.5]: for o in [&#39;adam&#39;,&#39;sgd&#39;]: logdir = &#39;logs/hpguebin_{}_{}_{}&#39;.format(u,d,o) with tf.summary.create_file_writer(logdir).as_default(): net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(u,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dropout(d)) net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=o,loss=tf.losses.categorical_crossentropy,metrics=[&#39;accuracy&#39;,&#39;Recall&#39;]) cb3 = hp.KerasCallback(logdir, {&#39;유닛수&#39;:u, &#39;드랍아웃비율&#39;:d, &#39;옵티마이저&#39;:o}) net.fit(X,y,epochs=3,callbacks=cb3) _rslt=net.evaluate(XX,yy) _mymetric=_rslt[1]*0.8 + _rslt[2]*0.2 tf.summary.scalar(&#39;애큐러시와리컬의가중평균(테스트셋)&#39;, _mymetric, step=1) . Epoch 1/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.5255 - accuracy: 0.8180 - recall: 0.7546 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3993 - accuracy: 0.8588 - recall: 0.8294 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3648 - accuracy: 0.8698 - recall: 0.8443 313/313 [==============================] - 0s 830us/step - loss: 0.4063 - accuracy: 0.8545 - recall: 0.8286 Epoch 1/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.7744 - accuracy: 0.7503 - recall: 0.5797 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.5204 - accuracy: 0.8223 - recall: 0.7565 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4742 - accuracy: 0.8369 - recall: 0.7859 313/313 [==============================] - 0s 828us/step - loss: 0.4899 - accuracy: 0.8304 - recall: 0.7831 Epoch 1/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.7502 - accuracy: 0.7356 - recall: 0.6115 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.5738 - accuracy: 0.7923 - recall: 0.7133 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.5473 - accuracy: 0.8037 - recall: 0.7321 313/313 [==============================] - 0s 865us/step - loss: 0.4319 - accuracy: 0.8448 - recall: 0.7919 Epoch 1/3 1875/1875 [==============================] - 2s 1ms/step - loss: 1.0932 - accuracy: 0.6228 - recall: 0.3971 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.7616 - accuracy: 0.7388 - recall: 0.5956 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.6828 - accuracy: 0.7684 - recall: 0.6478 313/313 [==============================] - 0s 894us/step - loss: 0.5265 - accuracy: 0.8180 - recall: 0.7353 Epoch 1/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4777 - accuracy: 0.8292 - recall: 0.7890 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3603 - accuracy: 0.8682 - recall: 0.8427 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3197 - accuracy: 0.8817 - recall: 0.8605 313/313 [==============================] - 0s 846us/step - loss: 0.3803 - accuracy: 0.8628 - recall: 0.8428 Epoch 1/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.6685 - accuracy: 0.7883 - recall: 0.6444 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4815 - accuracy: 0.8372 - recall: 0.7781 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4408 - accuracy: 0.8498 - recall: 0.8021 313/313 [==============================] - 0s 859us/step - loss: 0.4634 - accuracy: 0.8390 - recall: 0.7962 Epoch 1/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.5708 - accuracy: 0.7991 - recall: 0.7556 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4418 - accuracy: 0.8393 - recall: 0.8057 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4091 - accuracy: 0.8514 - recall: 0.8211 313/313 [==============================] - 0s 850us/step - loss: 0.3937 - accuracy: 0.8587 - recall: 0.8238 Epoch 1/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.6930 - accuracy: 0.7752 - recall: 0.6338 Epoch 2/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.5048 - accuracy: 0.8274 - recall: 0.7651 Epoch 3/3 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4608 - accuracy: 0.8417 - recall: 0.7910 313/313 [==============================] - 0s 854us/step - loss: 0.4625 - accuracy: 0.8396 - recall: 0.7957 . #%tensorboard --logdir logs --host 0.0.0.0 . &#49689;&#51228; . - 아래의 네트워크에서 옵티마이저를 adam, sgd를 선택하여 각각 적합시켜보고 testset의 loss를 성능비교를 하라. epoch은 5정도로 설정하라. . net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(50,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(50,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(optimizer=???,loss=tf.losses.categorical_crossentropy,metrics=[&#39;accuracy&#39;,&#39;Recall&#39;]) .",
            "url": "https://guebin.github.io/STBDA2022/2022/05/23/(12%EC%A3%BC%EC%B0%A8)-5%EC%9B%9423%EC%9D%BC.html",
            "relUrl": "/2022/05/23/(12%EC%A3%BC%EC%B0%A8)-5%EC%9B%9423%EC%9D%BC.html",
            "date": " • May 23, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "(11주차) 5월16일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import tensorflow.experimental.numpy as tnp import numpy as np import matplotlib.pyplot as plt . tnp.experimental_enable_numpy_behavior() . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() . X = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64) y = tf.keras.utils.to_categorical(y_train) XX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64) yy = tf.keras.utils.to_categorical(y_test) . - 첫시도 . net1 = tf.keras.Sequential() net1.add(tf.keras.layers.Flatten()) net1.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net1.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net1.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net1.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net1.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net1.compile(optimizer=&#39;adam&#39;, loss=tf.losses.categorical_crossentropy,metrics=&#39;accuracy&#39;) net1.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 2s 1ms/step - loss: 1.2243 - accuracy: 0.7894 Epoch 2/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4534 - accuracy: 0.8388 Epoch 3/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4161 - accuracy: 0.8502 Epoch 4/5 1875/1875 [==============================] - 2s 907us/step - loss: 0.4019 - accuracy: 0.8569 Epoch 5/5 1875/1875 [==============================] - 2s 973us/step - loss: 0.3873 - accuracy: 0.8622 . &lt;keras.callbacks.History at 0x7f38111302b0&gt; . net1.evaluate(XX,yy) . 313/313 [==============================] - 0s 835us/step - loss: 0.4163 - accuracy: 0.8504 . [0.41634100675582886, 0.8503999710083008] . net1.summary() . Model: &#34;sequential_6&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_6 (Flatten) (None, 784) 0 dense_22 (Dense) (None, 500) 392500 dense_23 (Dense) (None, 500) 250500 dense_24 (Dense) (None, 500) 250500 dense_25 (Dense) (None, 500) 250500 dense_26 (Dense) (None, 10) 5010 ================================================================= Total params: 1,149,010 Trainable params: 1,149,010 Non-trainable params: 0 _________________________________________________________________ . - 두번째 시도 . net2 = tf.keras.Sequential() net2.add(tf.keras.layers.Conv2D(30,(2,2),activation=&#39;relu&#39;)) net2.add(tf.keras.layers.MaxPool2D()) net2.add(tf.keras.layers.Conv2D(30,(2,2),activation=&#39;relu&#39;)) net2.add(tf.keras.layers.MaxPool2D()) net2.add(tf.keras.layers.Flatten()) #net2.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net2.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net2.compile(optimizer=&#39;adam&#39;, loss=tf.losses.categorical_crossentropy,metrics=&#39;accuracy&#39;) net2.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.8787 - accuracy: 0.8044 Epoch 2/5 1875/1875 [==============================] - 2s 994us/step - loss: 0.3843 - accuracy: 0.8618 Epoch 3/5 1875/1875 [==============================] - 2s 994us/step - loss: 0.3454 - accuracy: 0.8755 Epoch 4/5 1875/1875 [==============================] - 2s 989us/step - loss: 0.3217 - accuracy: 0.8838 Epoch 5/5 1875/1875 [==============================] - 2s 901us/step - loss: 0.3053 - accuracy: 0.8878 . &lt;keras.callbacks.History at 0x7f3234110fd0&gt; . net2.evaluate(XX,yy) . 313/313 [==============================] - 0s 837us/step - loss: 0.3438 - accuracy: 0.8736 . [0.3437994122505188, 0.8736000061035156] . net2.summary() . Model: &#34;sequential_8&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_2 (Conv2D) (None, 27, 27, 30) 150 max_pooling2d_2 (MaxPooling (None, 13, 13, 30) 0 2D) conv2d_3 (Conv2D) (None, 12, 12, 30) 3630 max_pooling2d_3 (MaxPooling (None, 6, 6, 30) 0 2D) flatten_8 (Flatten) (None, 1080) 0 dense_28 (Dense) (None, 10) 10810 ================================================================= Total params: 14,590 Trainable params: 14,590 Non-trainable params: 0 _________________________________________________________________ . 14590/ 1149010 . 0.012697887746842934 . c1, m1, c2, m2, flttn, dns = net2.layers . print(X.shape) # 입력이미지 = 2D print(c1(X).shape) #2D print(m1(c1(X)).shape) #2D print(c2(m1(c1(X))).shape) #2D print(m2(c2(m1(c1(X)))).shape) #2D print(flttn(m2(c2(m1(c1(X))))).shape)# 1D print(dns(flttn(m2(c2(m1(c1(X)))))).shape)# 1D . (60000, 28, 28, 1) (60000, 27, 27, 30) (60000, 13, 13, 30) (60000, 12, 12, 30) (60000, 6, 6, 30) (60000, 1080) (60000, 10) . MaxPool2D . &#53580;&#49828;&#53944;1 . - 레이어생성 . m=tf.keras.layers.MaxPool2D() . - 입력데이터 . XXX = tnp.arange(1*4*4*1).reshape(1,4,4,1) XXX.reshape(1,4,4) . &lt;tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]])&gt; . - 입력데이터가 레이어를 통과한 모습 . m(XXX).reshape(1,2,2) . &lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy= array([[[ 5, 7], [13, 15]]])&gt; . - MaxPool2D layer의 역할: (2,2)윈도우를 만들고 (2,2)윈도우에서 max를 뽑아 값을 기록, 윈도우를 움직이면서 반복 . &#53580;&#49828;&#53944;2 . XXX = tnp.arange(1*6*6*1).reshape(1,6,6,1) XXX.reshape(1,6,6) . &lt;tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy= array([[[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]]])&gt; . m(XXX).reshape(1,3,3) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy= array([[[ 7, 9, 11], [19, 21, 23], [31, 33, 35]]])&gt; . &#53580;&#49828;&#53944;3 . m=tf.keras.layers.MaxPool2D(pool_size=(3, 3)) . XXX = tnp.arange(1*6*6*1).reshape(1,6,6,1) XXX.reshape(1,6,6) . &lt;tf.Tensor: shape=(1, 6, 6), dtype=int64, numpy= array([[[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]]])&gt; . m(XXX).reshape(1,2,2) . &lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy= array([[[14, 17], [32, 35]]])&gt; . &#53580;&#49828;&#53944;4 . m=tf.keras.layers.MaxPool2D(pool_size=(2, 2)) . XXX = tnp.arange(1*5*5*1).reshape(1,5,5,1) XXX.reshape(1,5,5) . &lt;tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy= array([[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]])&gt; . m(XXX).reshape(1,2,2) . &lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy= array([[[ 6, 8], [16, 18]]])&gt; . m=tf.keras.layers.MaxPool2D(pool_size=(2, 2),padding=&quot;same&quot;) . XXX = tnp.arange(1*5*5*1).reshape(1,5,5,1) XXX.reshape(1,5,5) . &lt;tf.Tensor: shape=(1, 5, 5), dtype=int64, numpy= array([[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]])&gt; . m(XXX).reshape(1,3,3) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=int64, numpy= array([[[ 6, 8, 9], [16, 18, 19], [21, 23, 24]]])&gt; . &#53580;&#49828;&#53944;5 . XXX = tnp.arange(2*4*4*1).reshape(2,4,4,1) XXX.reshape(2,4,4) . &lt;tf.Tensor: shape=(2, 4, 4), dtype=int64, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]], [[16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]]])&gt; . m(XXX).reshape(2,2,2) . &lt;tf.Tensor: shape=(2, 2, 2), dtype=int64, numpy= array([[[ 5, 7], [13, 15]], [[21, 23], [29, 31]]])&gt; . &#53580;&#49828;&#53944;6 . XXX = tnp.arange(1*4*4*3).reshape(1,4,4,3) . XXX[...,0] . &lt;tf.Tensor: shape=(1, 4, 4), dtype=int64, numpy= array([[[ 0, 3, 6, 9], [12, 15, 18, 21], [24, 27, 30, 33], [36, 39, 42, 45]]])&gt; . m(XXX)[...,0] . &lt;tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy= array([[[15, 21], [39, 45]]])&gt; . Conv2D . &#53580;&#49828;&#53944;1 . - 레이어생성 . cnv = tf.keras.layers.Conv2D(1,(2,2)) . - XXX생성 . XXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1) XXX.reshape(1,4,4) . &lt;tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy= array([[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]])&gt; . cnv(XXX).reshape(1,3,3) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy= array([[[ 4.0450797, 5.7349434, 7.4248075], [10.804535 , 12.494399 , 14.184262 ], [17.56399 , 19.253855 , 20.943718 ]]], dtype=float32)&gt; . XXX에서 cnv(XXX)로 가는 맵핑을 찾는건 쉽지 않아보인다. | 심지어 랜덤으로 결정되는 부분도 있어보임 | . - 코드정리 + 시드통일 . tf.random.set_seed(43052) cnv = tf.keras.layers.Conv2D(1,(2,2)) XXX = tnp.arange(1*4*4*1,dtype=tf.float64).reshape(1,4,4,1) . - conv의 입출력 . print(XXX.reshape(1,4,4)) print(cnv(XXX).reshape(1,3,3)) . tf.Tensor( [[[ 0. 1. 2. 3.] [ 4. 5. 6. 7.] [ 8. 9. 10. 11.] [12. 13. 14. 15.]]], shape=(1, 4, 4), dtype=float64) tf.Tensor( [[[ -4.125754 -5.312817 -6.4998803] [ -8.874006 -10.0610695 -11.248133 ] [-13.622259 -14.809322 -15.996386 ]]], shape=(1, 3, 3), dtype=float32) . - conv연산 추론 . tf.reshape(cnv.weights[0],(2,2)) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[-0.13014299, -0.23927206], [-0.20175874, -0.6158894 ]], dtype=float32)&gt; . 0 * -0.13014299 + 1 * -0.23927206 + 4 * -0.20175874 + 5 * -0.6158894 + 0 . -4.1257540200000005 . - 내가 정의한 weights를 대입하여 conv 연산 확인 . cnv.get_weights()[0].shape . (2, 2, 1, 1) . w = np.array([1/4,1/4,1/4,1/4],dtype=np.float32).reshape(2, 2, 1, 1) b = np.array([3],dtype=np.float32) . cnv.set_weights([w,b]) . XXX.reshape(1,4,4) . &lt;tf.Tensor: shape=(1, 4, 4), dtype=float64, numpy= array([[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]]])&gt; . cnv(XXX).reshape(1,3,3) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy= array([[[ 5.5, 6.5, 7.5], [ 9.5, 10.5, 11.5], [13.5, 14.5, 15.5]]], dtype=float32)&gt; . np.mean([0,1,4,5])+3, np.mean([1,2,5,6])+3, np.mean([2,3,6,7])+3 . (5.5, 6.5, 7.5) . tf.keras.layers.Conv2D(1,kernel_size=(2,2)) &#50836;&#50557; . - 요약 . (1) size=(2,2)인 윈도우를 만듬. . (2) XXX에 윈도우를 통과시켜서 (2,2)크기의 sub XXX 를 얻음. sub XXX의 각 원소에 conv2d.weights[0]의 각 원소를 element-wise하게 곱한다. . (3) (2)의 결과를 모두 더한다. 그리고 그 결과에 다시 conv2d.weights[1]을 수행 . (4) 윈도우를 이동시키면서 반복! . &#53580;&#49828;&#53944;2 . - 레이어와 XXX생성 . tf.random.set_seed(43052) cnv = tf.keras.layers.Conv2D(1,(3,3)) XXX = tnp.arange(1*5*5*1,dtype=tf.float64).reshape(1,5,5,1) . XXX.reshape(1,5,5) ## 입력: XXX . &lt;tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy= array([[[ 0., 1., 2., 3., 4.], [ 5., 6., 7., 8., 9.], [10., 11., 12., 13., 14.], [15., 16., 17., 18., 19.], [20., 21., 22., 23., 24.]]])&gt; . tf.reshape(cnv.weights[0],(3,3)) ## 커널의 가중치 . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[-0.08676198, -0.1595147 , -0.13450584], [-0.4105929 , -0.38366908, 0.07744962], [-0.09255642, 0.4915564 , 0.20828158]], dtype=float32)&gt; . cnv(XXX).reshape(1,3,3) ## 출력: conv(XXX) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy= array([[[ 2.7395768 , 2.2492635 , 1.7589504 ], [ 0.28801066, -0.20230258, -0.6926158 ], [-2.1635566 , -2.6538715 , -3.1441827 ]]], dtype=float32)&gt; . tf.reduce_sum(XXX.reshape(1,5,5)[0,:3,:3] * tf.reshape(cnv.weights[0],(3,3))) . &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.739577144384384&gt; . &#53580;&#49828;&#53944;3 . . XXX = tf.constant([[3,3,2,1,0],[0,0,1,3,1],[3,1,2,2,3],[2,0,0,2,2],[2,0,0,0,1]],dtype=tf.float64).reshape(1,5,5,1) XXX.reshape(1,5,5) . &lt;tf.Tensor: shape=(1, 5, 5), dtype=float64, numpy= array([[[3., 3., 2., 1., 0.], [0., 0., 1., 3., 1.], [3., 1., 2., 2., 3.], [2., 0., 0., 2., 2.], [2., 0., 0., 0., 1.]]])&gt; . cnv = tf.keras.layers.Conv2D(1,(3,3)) . cnv.weights . [] . cnv(XXX).reshape(1,3,3) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy= array([[[1.7157198, 2.9689512, 2.7728844], [2.4162836, 1.8230928, 2.9890852], [1.9408667, 1.2231059, 2.2712555]]], dtype=float32)&gt; . cnv.weights[0] . &lt;tf.Variable &#39;conv2d_13/kernel:0&#39; shape=(3, 3, 1, 1) dtype=float32, numpy= array([[[[ 0.28270614]], [[-0.13318631]], [[ 0.21818542]]], [[[ 0.23769057]], [[ 0.40044254]], [[ 0.38520074]]], [[[ 0.15709132]], [[ 0.48156905]], [[-0.25362712]]]], dtype=float32)&gt; . _w = tf.constant([[0,1,2],[2,2,0],[0,1,2]],dtype=tf.float64).reshape(3,3,1,1) _b = tf.constant([0],dtype=tf.float64) . cnv.set_weights([_w,_b]) . cnv(XXX).reshape(1,3,3) . &lt;tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy= array([[[12., 12., 17.], [10., 17., 19.], [ 9., 6., 14.]]], dtype=float32)&gt; . &#53580;&#49828;&#53944;4 . tf.random.set_seed(43052) cnv = tf.keras.layers.Conv2D(1,(2,2)) XXX = tnp.arange(2*5*5*1,dtype=tf.float64).reshape(2,5,5,1) . print(XXX.reshape(2,5,5)) cnv(XXX) # weights를 초기화 시키기 위해서 레이어를 1회 통과 cnv.set_weights([w,b]) print(cnv(XXX).reshape(2,4,4)) . tf.Tensor( [[[ 0. 1. 2. 3. 4.] [ 5. 6. 7. 8. 9.] [10. 11. 12. 13. 14.] [15. 16. 17. 18. 19.] [20. 21. 22. 23. 24.]] [[25. 26. 27. 28. 29.] [30. 31. 32. 33. 34.] [35. 36. 37. 38. 39.] [40. 41. 42. 43. 44.] [45. 46. 47. 48. 49.]]], shape=(2, 5, 5), dtype=float64) tf.Tensor( [[[ 6. 7. 8. 9.] [11. 12. 13. 14.] [16. 17. 18. 19.] [21. 22. 23. 24.]] [[31. 32. 33. 34.] [36. 37. 38. 39.] [41. 42. 43. 44.] [46. 47. 48. 49.]]], shape=(2, 4, 4), dtype=float32) . np.mean([0,1,5,6])+3,np.mean([25,26,30,31])+3, . (6.0, 31.0) . &#53580;&#49828;&#53944;5 . - . tf.random.set_seed(43052) cnv = tf.keras.layers.Conv2D(4,(2,2),activation=&#39;relu&#39;) XXX = tnp.arange(1*2*2*1,dtype=tf.float64).reshape(1,2,2,1) . print(XXX.reshape(1,2,2)) . tf.Tensor( [[[0. 1.] [2. 3.]]], shape=(1, 2, 2), dtype=float64) . cnv(XXX) . &lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy=array([[[[1.048703, 0. , 0. , 0. ]]]], dtype=float32)&gt; . cnv.weights[0] # (2,2) 커널의 크기 // 1은 XXX의 채널수 // 4는 conv(XXX)의 채널수 . &lt;tf.Variable &#39;conv2d_27/kernel:0&#39; shape=(2, 2, 1, 4) dtype=float32, numpy= array([[[[-0.08230966, -0.15132892, -0.12760344, -0.38952267]], [[-0.36398047, 0.07347518, -0.08780673, 0.46633136]]], [[[ 0.19759327, -0.46042526, -0.15406173, -0.34838456]], [[ 0.33916563, -0.08248386, 0.11705655, -0.49948823]]]], dtype=float32)&gt; . cnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫번째채널 출력을 얻기 위해 곱해지는 w . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[-0.08230966, -0.36398047], [ 0.19759327, 0.33916563]], dtype=float32)&gt; . tf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ### conv(XXX)의 첫번째 채널 출력결과 . &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.0487029552459717&gt; . - 계산결과를 확인하기 쉽게 하기 위한 약간의 트릭 . tf.random.set_seed(43052) cnv = tf.keras.layers.Conv2D(4,(2,2)) XXX = tnp.array([1]*1*2*2*1,dtype=tf.float64).reshape(1,2,2,1) . print(XXX.reshape(1,2,2)) . tf.Tensor( [[[1. 1.] [1. 1.]]], shape=(1, 2, 2), dtype=float64) . 이렇게 XXX를 설정하면 cnv(XXX)의 결과는 단지 cnv의 weight들의 sum이 된다. | . cnv(XXX) . &lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy= array([[[[ 0.09046876, -0.6207629 , -0.25241536, -0.7710641 ]]]], dtype=float32)&gt; . cnv.weights[0] # (2,2) 커널의 크기 // 1은 XXX의 채널수 // 4는 conv(XXX)의 채널수 . &lt;tf.Variable &#39;conv2d_24/kernel:0&#39; shape=(2, 2, 1, 4) dtype=float32, numpy= array([[[[-0.08230966, -0.15132892, -0.12760344, -0.38952267]], [[-0.36398047, 0.07347518, -0.08780673, 0.46633136]]], [[[ 0.19759327, -0.46042526, -0.15406173, -0.34838456]], [[ 0.33916563, -0.08248386, 0.11705655, -0.49948823]]]], dtype=float32)&gt; . cnv.weights[0][...,0].reshape(2,2) ## conv(XXX)의 첫번째채널 출력을 얻기 위해 곱해지는 w . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[-0.08230966, -0.36398047], [ 0.19759327, 0.33916563]], dtype=float32)&gt; . tf.reduce_sum(cnv.weights[0][...,0]) #tf.reduce_sum(XXX.reshape(1,2,2) * cnv.weights[0][...,0].reshape(2,2)) ### conv(XXX)의 첫번째 채널 출력결과 . &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.090468764&gt; . &#53580;&#49828;&#53944;6 . - 결과확인을 쉽게하기 위해서 XXX를 1로 통일 . tf.random.set_seed(43052) cnv = tf.keras.layers.Conv2D(4,(2,2)) XXX = tnp.array([1]*1*2*2*3,dtype=tf.float64).reshape(1,2,2,3) . cnv(XXX) . &lt;tf.Tensor: shape=(1, 1, 1, 4), dtype=float32, numpy= array([[[[ 0.3297621, -0.4498347, -1.0487393, -1.580095 ]]]], dtype=float32)&gt; . cnv.weights[0] ## (2,2)는 커널의 사이즈 // 3은 XXX의채널 // 4는 cnv(XXX)의 채널 . &lt;tf.Variable &#39;conv2d_33/kernel:0&#39; shape=(2, 2, 3, 4) dtype=float32, numpy= array([[[[-0.06956434, -0.12789628, -0.10784459, -0.32920673], [-0.30761963, 0.06209785, -0.07421023, 0.3941219 ], [ 0.16699678, -0.38913035, -0.13020593, -0.29443866]], [[ 0.28664726, -0.0697116 , 0.09893084, -0.4221446 ], [-0.23161241, -0.16410837, -0.36420006, 0.12424195], [-0.14245945, 0.36286396, -0.10751781, 0.1733647 ]]], [[[ 0.02764335, 0.15547717, -0.42024496, -0.31893867], [ 0.22414821, 0.3619454 , -0.00282967, -0.3503708 ], [ 0.4610079 , -0.17417148, 0.00401336, -0.29777044]], [[-0.1620284 , -0.42066965, -0.01578814, -0.4240524 ], [ 0.37925082, 0.24236053, 0.3949356 , -0.20996472], [-0.30264795, -0.28889188, -0.3237777 , 0.37506342]]]], dtype=float32)&gt; . cnv.weights[0][...,0] ## cnv(XXX)의 첫번째 채널결과를 얻기 위해서 사용하는 w . &lt;tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy= array([[[-0.06956434, -0.30761963, 0.16699678], [ 0.28664726, -0.23161241, -0.14245945]], [[ 0.02764335, 0.22414821, 0.4610079 ], [-0.1620284 , 0.37925082, -0.30264795]]], dtype=float32)&gt; . tf.reduce_sum(cnv.weights[0][...,0]) ### cnv(XXX)의 첫번째 채널의 결과 . &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.32976213&gt; . print(tf.reduce_sum(cnv.weights[0][...,0])) print(tf.reduce_sum(cnv.weights[0][...,1])) print(tf.reduce_sum(cnv.weights[0][...,2])) print(tf.reduce_sum(cnv.weights[0][...,3])) ### cnv(XXX)의 결과 . tf.Tensor(0.32976213, shape=(), dtype=float32) tf.Tensor(-0.44983464, shape=(), dtype=float32) tf.Tensor(-1.0487392, shape=(), dtype=float32) tf.Tensor(-1.5800952, shape=(), dtype=float32) . w_red = cnv.weights[0][...,0][...,0] w_green = cnv.weights[0][...,0][...,1] w_blue = cnv.weights[0][...,0][...,2] . tf.reduce_sum(XXX[...,0] * w_red + XXX[...,1] * w_green + XXX[...,2] * w_blue) ## cnv(XXX)의 첫채널 출력결과 . &lt;tf.Tensor: shape=(), dtype=float64, numpy=0.32976213097572327&gt; . hw . 아래와 같은 흑백이미지가 있다고 하자. . 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 . 위의 이미지에 아래와 같은 weight를 가진 필터를 적용하여 convolution한 결과를 계산하라. (bias는 0으로 가정한다) . -1 1 -1 1 .",
            "url": "https://guebin.github.io/STBDA2022/2022/05/16/(11%EC%A3%BC%EC%B0%A8)-5%EC%9B%9416%EC%9D%BC.html",
            "relUrl": "/2022/05/16/(11%EC%A3%BC%EC%B0%A8)-5%EC%9B%9416%EC%9D%BC.html",
            "date": " • May 16, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "(10주차) 5월9일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import matplotlib.pyplot as plt import numpy as np import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . tf.config.experimental.list_physical_devices() . 2022-05-22 18:47:04.943426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . [PhysicalDevice(name=&#39;/physical_device:CPU:0&#39;, device_type=&#39;CPU&#39;), PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . softmax function . &#47196;&#51648;&#49828;&#54001; &#47784;&#54805; (1): &#54876;&#49457;&#54868;&#54632;&#49688;&#47196; sigmoid &#49440;&#53469; . - 기본버전은 아래와 같다 . $$y_i approx text{sigmoid}(b + w_1 x_{1,i} + dots + w_{784}x_{784,i})= frac{ exp(b + w_1 x_{1,i} + dots + w_{784}x_{784,i})}{1+ exp(b + w_1 x_{1,i} + dots + w_{784}x_{784,i})}$$ . - 벡터버전은 아래와 같다. . $${ boldsymbol y} approx text{sigmoid}({ bf X}{ bf W} + b) = frac{ exp({ bf XW} +b)}{1+ exp({ bf XW} +b)}$$ . - 벡터버전에 익숙해지도록 하자. 벡터버전에 사용된 차원 및 연산을 정리하면 아래와 같다. . ${ bf X}$: (n,784) matrix . | ${ boldsymbol y}$: (n,1) matrix . | ${ bf W}$: (784,1) matrix . | $b$: (1,1) matrix . | +, exp 는 브로드캐스팅 . | . &#47196;&#51648;&#49828;&#54001; &#47784;&#54805; (2): &#54876;&#49457;&#54868;&#54632;&#49688;&#47196; softmax &#49440;&#53469; . - $y_i=0 text{ or } 1$ 대신에 $ boldsymbol{y}_i=[y_{i1},y_{i2}]= [1,0] text { or } [0,1]$와 같이 코딩하면 어떠할까? (즉 원핫인코딩을 한다면?) . - 활성화 함수를 취하기 전의 버전은 아래와 같이 볼 수 있다. . $$[{ boldsymbol y}_1 ~ { boldsymbol y}_2] propto [ { bf X}{ bf W}_1 ~ { bf X}{ bf W}_2] + [b_1 ~ b_2]= { bf X} [{ bf W}_1 { bf W}_2] + [b_1 ~ b_2]= { bf X}{ bf W} + { boldsymbol b}$$ . 여기에서 매트릭스 및 연산의 차원을 정리하면 아래와 같다. . ${ bf X}$: (n,784) matrix . | ${ boldsymbol y}_1,{ boldsymbol y}_2$: (n,1) matrix . | ${ boldsymbol y}:=[{ boldsymbol y}_1~ { boldsymbol y}_2]$: (n,2) matrix . | ${ bf W}_1$, ${ bf W}_2$: (784,1) matrix . | ${ bf W}:=[{ bf W}_1~ { bf W}_2]$: (784,2) matrix . | $b_1,b_2$: (1,1) matrix . | $ boldsymbol{b}:= [b_1 ~b_2] $: (1,2) matrix . | + 는 브로드캐스팅 . | . - 즉 로지스틱 모형 (1)의 형태를 겹쳐놓은 형태로 해석할 수 있음. 따라서 ${ bf X} { bf W}_1 + b_1$와 ${ bf X} { bf W}_2 + b_2$의 row값이 클수록 ${ boldsymbol y}_1$와 ${ boldsymbol y}_2$의 row값이 1이어야 함 . ${ boldsymbol y}_1 propto { bf X} { bf W}_1 + b_1$ $ to$ ${ bf X} { bf W}_1 + b_1$의 row값이 클수록 $ boldsymbol{y}_1$의 row 값이 1이라면 모형계수를 잘 추정한것 | ${ boldsymbol y}_2 propto { bf X} { bf W}_2 + b_2$ $ to$ ${ bf X} { bf W}_2 + b_2$의 row값이 클수록 $ boldsymbol{y}_2$의 row 값을 1이라면 모형계수를 잘 추정한것 | . - (문제) ${ bf X}{ bf W}_1 +b_1$의 값이 500, ${ bf X}{ bf W}_2 +b_2$의 값이 200 인 row가 있다고 하자. 대응하는 $ boldsymbol{y}_1, boldsymbol{y}_2$의 row값은 얼마로 적합되어야 하는가? . (1) $[0,0]$ . (2) $[0,1]$ . (3) $[1,0]$ &lt;-- 이게 답이다! . (4) $[1,1]$ . . Note: 둘다 0 혹은 둘다 1로 적합할수는 없으니까 (1), (4)는 제외한다. ${ bf X}{ bf W}_1 +b_1$의 값이 ${ bf X}{ bf W}_2 +b_2$의 값보다 크므로 (3)번이 합리적임 . - 목표: 위와 같은 문제의 답을 유도해주는 활성화함수를 설계하자. 즉 합리적인 $ hat{ boldsymbol{y}}_1, hat{ boldsymbol{y}}_2$를 구해주는 활성화 함수를 설계해보자. 이를 위해서는 아래의 사항들이 충족되어야 한다. . (1) $ hat{ boldsymbol{y}}_1$, $ hat{ boldsymbol{y}}_2$의 각 원소는 0보다 크고 1보다 작아야 한다. (확률을 의미해야 하니까) . (2) $ hat{ boldsymbol{y}}_1+ hat{ boldsymbol{y}}_2={ bf 1}$ 이어야 한다. (확률의 총합은 1이니까!) . (3) $ hat{ boldsymbol{y}}_1$와 $ hat{ boldsymbol{y}}_2$를 각각 따로해석하면 로지스틱처럼 되면 좋겠다. . - 아래와 같은 활성화 함수를 도입하면 어떨까? . $$ hat{ boldsymbol{y}}=[ hat{ boldsymbol y}_1 ~ hat{ boldsymbol y}_2] = big[ frac{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)}{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)+ exp({ bf X} hat{ bf W}_2+ hat{b}_2)} ~~ frac{ exp({ bf X} hat{ bf W}_2+ hat{b}_2)}{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)+ exp({ bf X} hat{ bf W}_2+ hat{b}_2)} big]$$ . - (1),(2)는 만족하는 듯 하다. (3)은 바로 이해되지는 않는다 . (1) $ hat{ boldsymbol{y}}_1$, $ hat{ boldsymbol{y}}_2$의 각 원소는 0보다 크고 1보다 작아야 한다. --&gt; OK! . (2) $ hat{ boldsymbol{y}}_1+ hat{ boldsymbol{y}}_2={ bf 1}$ 이어야 한다. --&gt; OK! . (3) $ hat{ boldsymbol{y}}_1$와 $ hat{ boldsymbol{y}}_2$를 각각 따로해석하면 로지스틱처럼 되면 좋겠다. --&gt; ??? . - 그런데 조금 따져보면 (3)도 만족된다는 것을 알 수 있다. (sigmoid, softmax Section 참고) . - 위와 같은 함수를 softmax라고 하자. 즉 아래와 같이 정의하자. . $$ hat{ boldsymbol y} = text{softmax}({ bf X} hat{ bf W} + { boldsymbol b}) = big[ frac{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)}{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)+ exp({ bf X} hat{ bf W}_2+ hat{b}_2)} ~~ frac{ exp({ bf X} hat{ bf W}_2+ hat{b}_2)}{ exp({ bf X} hat{ bf W}_1+ hat{b}_1)+ exp({ bf X} hat{ bf W}_2+ hat{b}_2)} big] $$ sigmoid, softmax . softmax&#45716; sigmoid&#51032; &#54869;&#51109;&#54805; . - 아래의 수식을 관찰하자. $$ frac{ exp( beta_0+ beta_1 x_i)}{1+ exp( beta_0+ beta_1x_i)}= frac{ exp( beta_0+ beta_1 x_i)}{e^0+ exp( beta_0+ beta_1x_i)}$$ . - 1을 $e^0$로 해석하면 모형2의 해석을 아래와 같이 모형1의 해석으로 적용할수 있다. . 모형2: ${ bf X} hat{ bf W}_1 + hat{b}_1$ 와 ${ bf X} hat{ bf W}_2 + hat{b}_2$ 의 크기를 비교하고 확률 결정 | 모형1: ${ bf X} hat{ bf W} + hat{b}$ 와 $0$의 크기를 비교하고 확률 결정 = ${ bf X} hat{ bf W} + hat{b}$의 row값이 양수이면 1로 예측하고 음수이면 0으로 예측 | . - 이항분포를 차원이 2인 다항분포로 해석가능한 것처럼 sigmoid는 차원이 2인 softmax로 해석가능하다. 즉 다항분포가 이항분포의 확장형으로 해석가능한 것처럼 softmax도 sigmoid의 확장형으로 해석가능하다. . &#53364;&#47000;&#49828;&#51032; &#49688;&#44032; 2&#51064; &#44221;&#50864; softmax vs sigmoid . - 언뜻 생각하면 클래스가 2인 경우에도 sigmoid 대신 softmax로 활성화함수를 이용해도 될 듯 하다. 즉 $y=0 text{ or } 1$와 같이 정리하지 않고 $y=[0,1] text{ or } [1,0]$ 와 같이 정리해도 무방할 듯 하다. . - 하지만 sigmoid가 좀 더 좋은 선택이다. 즉 $y= 0 text{ or } 1$로 데이터를 정리하는 것이 더 좋은 선택이다. 왜냐하면 sigmoid는 softmax와 비교하여 파라메터의 수가 적지만 표현력은 동등하기 때문이다. . - 표현력이 동등한 이유? 아래 수식을 관찰하자. . $$ big( frac{e^{300}}{e^{300}+e^{500}}, frac{e^{500}}{e^{300}+e^{500}} big) = big( frac{e^{0}}{e^{0}+e^{200}}, frac{e^{200}}{e^{0}+e^{200}} big)$$ . $ big( frac{e^{300}}{e^{300}+e^{500}}, frac{e^{500}}{e^{300}+e^{500}} big)$를 표현하기 위해서 300, 500 이라는 2개의 숫자가 필요한것이 아니고 따지고보면 200이라는 하나의 숫자만 필요하다. | $( hat{ boldsymbol{y}}_1, hat{ boldsymbol{y}}_2)$의 표현에서도 ${ bf X} hat{ bf W}_1 + hat{b}_1$ 와 ${ bf X} hat{ bf W}_2 + hat{b}_2$ 라는 숫자 각각이 필요한 것이 아니고 $({ bf X} hat{ bf W}_1 + hat{b}_1)-({ bf X} hat{ bf W}_2 + hat{b}_2)$의 값만 알면 된다. | . - 클래스의 수가 2개일 경우는 softmax가 sigmoid에 비하여 장점이 없다. 하지만 softmax는 클래스의 수가 3개 이상일 경우로 쉽게 확장할 수 있다는 점에서 매력적인 활성화 함수이다. . &#48516;&#47448;&#54624; &#53364;&#47000;&#49828;&#44032; 3&#44060; &#51060;&#49345;&#51068; &#44221;&#50864; &#49888;&#44221;&#47581; &#47784;&#54805;&#51032; &#49444;&#44228; . - y의 모양: [0 1 0 0 0 0 0 0 0 0] . - 활성화함수의 선택: softmax . - 손실함수의 선택: cross entropy . Fashion_MNIST &#50668;&#47084;&#53364;&#47000;&#49828;&#51032; &#48516;&#47448; (softmax&#51032; &#49892;&#49845;) . - 데이터정리 . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() . X= x_train.reshape(-1,784) y= tf.keras.utils.to_categorical(y_train) XX = x_test.reshape(-1,784) yy = tf.keras.utils.to_categorical(y_test) . - 시도1: 간단한 신경망 . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y10&quot; &quot;node2&quot; -&gt; &quot;y10&quot; &quot;...&quot; -&gt; &quot;y10&quot; &quot;node30&quot; -&gt; &quot;y10&quot; &quot;node1&quot; -&gt; &quot;y1&quot; &quot;node2&quot; -&gt; &quot;y1&quot; &quot;...&quot; -&gt; &quot;y1&quot; &quot;node30&quot; -&gt; &quot;y1&quot; &quot;node1&quot; -&gt; &quot;.&quot; &quot;node2&quot; -&gt; &quot;.&quot; &quot;...&quot; -&gt; &quot;.&quot; &quot;node30&quot; -&gt; &quot;.&quot; label = &quot;Layer 2: softmax&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: relu cluster_3 Layer 2: softmax x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node30 node30 x1&#45;&gt;node30 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node30 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node30 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node30 y10 y10 node1&#45;&gt;y10 y1 y1 node1&#45;&gt;y1 . . node1&#45;&gt;. node2&#45;&gt;y10 node2&#45;&gt;y1 node2&#45;&gt;. ...&#45;&gt;y10 ...&#45;&gt;y1 ...&#45;&gt;. node30&#45;&gt;y10 node30&#45;&gt;y1 node30&#45;&gt;. net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation = &#39;softmax&#39;)) net.compile(loss=tf.losses.categorical_crossentropy, optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;]) net.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 2s 639us/step - loss: 2.3870 - accuracy: 0.3897 Epoch 2/5 1875/1875 [==============================] - 1s 647us/step - loss: 1.2355 - accuracy: 0.5089 Epoch 3/5 1875/1875 [==============================] - 1s 625us/step - loss: 1.0652 - accuracy: 0.5585 Epoch 4/5 1875/1875 [==============================] - 1s 634us/step - loss: 0.9478 - accuracy: 0.6047 Epoch 5/5 1875/1875 [==============================] - 1s 631us/step - loss: 0.8989 - accuracy: 0.6225 . &lt;keras.callbacks.History at 0x7f1e583cb6d0&gt; . net.evaluate(XX,yy) . 313/313 [==============================] - 0s 673us/step - loss: 0.8989 - accuracy: 0.6276 . [0.89892578125, 0.6276000142097473] . net.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 30) 23550 dense_1 (Dense) (None, 10) 310 ================================================================= Total params: 23,860 Trainable params: 23,860 Non-trainable params: 0 _________________________________________________________________ . - 시도2: 더 깊은 신경망 . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node500&quot; &quot;x2&quot; -&gt; &quot;node500&quot; &quot;..&quot; -&gt; &quot;node500&quot; &quot;x784&quot; -&gt; &quot;node500&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;node1(2)&quot; &quot;node2&quot; -&gt; &quot;node1(2)&quot; &quot;...&quot; -&gt; &quot;node1(2)&quot; &quot;node500&quot; -&gt; &quot;node1(2)&quot; &quot;node1&quot; -&gt; &quot;node2(2)&quot; &quot;node2&quot; -&gt; &quot;node2(2)&quot; &quot;...&quot; -&gt; &quot;node2(2)&quot; &quot;node500&quot; -&gt; &quot;node2(2)&quot; &quot;node1&quot; -&gt; &quot;....&quot; &quot;node2&quot; -&gt; &quot;....&quot; &quot;...&quot; -&gt; &quot;....&quot; &quot;node500&quot; -&gt; &quot;....&quot; &quot;node1&quot; -&gt; &quot;node500(2)&quot; &quot;node2&quot; -&gt; &quot;node500(2)&quot; &quot;...&quot; -&gt; &quot;node500(2)&quot; &quot;node500&quot; -&gt; &quot;node500(2)&quot; label = &quot;Layer 2: relu&quot; } subgraph cluster_4{ style=filled; color=lightgrey; &quot;node1(2)&quot; -&gt; &quot;y10&quot; &quot;node2(2)&quot; -&gt; &quot;y10&quot; &quot;....&quot; -&gt; &quot;y10&quot; &quot;node500(2)&quot; -&gt; &quot;y10&quot; &quot;node1(2)&quot; -&gt; &quot;y1&quot; &quot;node2(2)&quot; -&gt; &quot;y1&quot; &quot;....&quot; -&gt; &quot;y1&quot; &quot;node500(2)&quot; -&gt; &quot;y1&quot; &quot;node1(2)&quot; -&gt; &quot;.&quot; &quot;node2(2)&quot; -&gt; &quot;.&quot; &quot;....&quot; -&gt; &quot;.&quot; &quot;node500(2)&quot; -&gt; &quot;.&quot; label = &quot;Layer 3: softmax&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: relu cluster_3 Layer 2: relu cluster_4 Layer 3: softmax x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node500 node500 x1&#45;&gt;node500 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node500 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node500 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node500 node1(2) node1(2) node1&#45;&gt;node1(2) node2(2) node2(2) node1&#45;&gt;node2(2) .... .... node1&#45;&gt;.... node500(2) node500(2) node1&#45;&gt;node500(2) node2&#45;&gt;node1(2) node2&#45;&gt;node2(2) node2&#45;&gt;.... node2&#45;&gt;node500(2) ...&#45;&gt;node1(2) ...&#45;&gt;node2(2) ...&#45;&gt;.... ...&#45;&gt;node500(2) node500&#45;&gt;node1(2) node500&#45;&gt;node2(2) node500&#45;&gt;.... node500&#45;&gt;node500(2) y10 y10 node1(2)&#45;&gt;y10 y1 y1 node1(2)&#45;&gt;y1 . . node1(2)&#45;&gt;. node2(2)&#45;&gt;y10 node2(2)&#45;&gt;y1 node2(2)&#45;&gt;. ....&#45;&gt;y10 ....&#45;&gt;y1 ....&#45;&gt;. node500(2)&#45;&gt;y10 node500(2)&#45;&gt;y1 node500(2)&#45;&gt;. net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation = &#39;softmax&#39;)) net.compile(loss=tf.losses.categorical_crossentropy, optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;]) net.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 1s 724us/step - loss: 1.9002 - accuracy: 0.7600 Epoch 2/5 1875/1875 [==============================] - 1s 756us/step - loss: 0.5813 - accuracy: 0.8076 Epoch 3/5 1875/1875 [==============================] - 1s 746us/step - loss: 0.4832 - accuracy: 0.8336 Epoch 4/5 1875/1875 [==============================] - 1s 728us/step - loss: 0.4307 - accuracy: 0.8474 Epoch 5/5 1875/1875 [==============================] - 1s 742us/step - loss: 0.4102 - accuracy: 0.8537 . &lt;keras.callbacks.History at 0x7f1e58138d00&gt; . net.evaluate(XX,yy) . 313/313 [==============================] - 0s 714us/step - loss: 0.4323 - accuracy: 0.8456 . [0.4322887063026428, 0.8456000089645386] . net.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_2 (Dense) (None, 500) 392500 dense_3 (Dense) (None, 500) 250500 dense_4 (Dense) (None, 10) 5010 ================================================================= Total params: 648,010 Trainable params: 648,010 Non-trainable params: 0 _________________________________________________________________ . &#54217;&#44032;&#51648;&#54364; . &#45796;&#50577;&#54620; &#54217;&#44032;&#51648;&#54364;&#46308; . - 의문: 왜 다양한 평가지표가 필요한가? (accuray면 끝나는거 아닌가? 더 이상 뭐가 필요해?) . - 여러가지 평가지표들: https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values . 이걸 다 암기하는건 불가능함. | 몇 개만 뽑아서 암기하고 왜 쓰는지만 생각해보고 넘어가자! | . confusion matrix&#51032; &#51060;&#54644; . - 표1 . 퇴사(예측) 안나감(예측) . 퇴사(실제) | TP | FN | . 안나감(실제) | FP | TN | . - 표2 (책에없음) . 퇴사(예측) 안나감(예측) . 퇴사(실제) | $(y, hat{y})= $ (O,O) | $(y, hat{y})= $(O,X) | . 안나감(실제) | $(y, hat{y})= $(X,O) | $(y, hat{y})= $(X,X) | . - 표3 (책에없음) . 퇴사(예측) 안나감(예측) . 퇴사(실제) | TP, $ # O/O$ | FN, $ #O/X$ | . 안나감(실제) | FP, $ #X/O$ | TN, $ #X/X$ | . 암기법, (1) 두번째 글자를 그대로 쓴다 (2) 첫글자가 T이면 분류를 제대로한것, 첫글자가 F이면 분류를 잘못한것 | . - 표4 (위키등에 있음) . 퇴사(예측) 안나감(예측) . 퇴사(실제) | TP, $ # O/O$ | FN, $ # O/X$ | Sensitivity(민감도)=Recall(재현율)=$ frac{TP}{TP+FN}$=$ frac{ #O/O}{ # O/O+ #O/X}$ | . 안나감(실제) | FP, $ # X/O$ | TN, $ # X/X$ | | . | Precision(프리시즌)=$ frac{TP}{TP+FP}$=$ frac{ # O/O}{ # O/O+ # X/O}$ | | Accuracy(애큐러시)=$ frac{TP+TN}{total}$=$ frac{ #O/O+ # X/X}{total}$ | . &#49345;&#54889;&#44537; . - 최규빈은 입사하여 &quot;퇴사자 예측시스템&quot;의 개발에 들어갔다. . - 자료의 특성상 대부분의 사람이 퇴사하지 않고 회사에 잘 다닌다. 즉 1000명이 있으면 10명정도 퇴사한다. . Accuracy . - 정의: Accuracy(애큐러시)=$ frac{TP+TN}{total}$=$ frac{ #O/O+ #X/X}{total}$ . 한국말로는 정확도, 정분류율이라고 한다. | 한국말이 헷갈리므로 그냥 영어를 외우는게 좋다. (어차피 Keras에서 옵션도 영어로 넣음) | . - (상확극 시점1) 왜 애큐러시는 불충분한가? . 회사: 퇴사자예측프로그램 개발해 | 최규빈: 귀찮은데 다 안나간다고 하자! -&gt; 99퍼의 accuracy | . 모델에 사용한 파라메터 = 0. 그런데 애큐러시 = 99! 이거 엄청 좋은 모형이다? . Sensitivity(&#48124;&#44048;&#46020;), Recall(&#51116;&#54788;&#50984;), True Positive Rate(TPR) . - 정의: Sensitivity(민감도)=Recall(재현율)=$ frac{TP}{TP+FN}$=$ frac{ # O/O}{ # O/O+ # O/X}$ . 분모: 실제 O인 관측치 수 | 분자: 실제 O를 O라고 예측한 관측치 수 | 뜻: 실제 O를 O라고 예측한 비율 | . - (상황극 시점2) recall을 봐야하는 이유 . 인사팀: 실제 퇴사자를 퇴사자로 예측해야 의미가 있음! 우리는 퇴사할것 같은 10명을 찍어달란 의미였어요! (그래야 면담을 하든 할거아냐!) | 최규빈: 가볍고(=파라메터 적고) 잘 맞추는 모형 만들어 달라면서요? | . 인사팀: (고민중..) 사실 생각해보니까 이 경우는 애큐러시는 의미가 없네. 실제 나간 사람 중 최규빈이 나간다고 한 사람이 몇인지 카운트 하는게 더 의미가 있겠다. 우리는 앞으로 리컬(혹은 민감도)를 보겠다! 예시1:실제로 퇴사한 10명중 최규빈이 나간다고 찍은 사람이 5명이면 리컬이 50% &gt; 예시2:최규빈이 아무도 나가지 않는다고 예측해버린다? 실제 10명중에서 최규빈이 나간다고 적중시킨사람은 0명이므로 이 경우 리컬은 0% . | . 결론: 우리가 필요한건 recall이니까 앞으로 recall을 가져와! accuracy는 큰 의미없어. (그래도 명색이 모델인데 accuracy가 90은 되면 좋겠다) | . Precision . - 정의: Precision(프리시즌)=$ frac{TP}{TP+FP}$=$ frac{ # O/O}{ # O/O+ # X/O}$ . 분모: O라고 예측한 관측치 | 분자: O라고 예측한 관측치중 진짜 O인 관측치 | 뜻: O라고 예측한 관측치중 진짜 O인 비율 | . - (상황극 시점3) recall 만으로 불충분한 이유 . 최규빈: 에휴.. 귀찮은데 그냥 좀만 수틀리면 다 나갈것 같다고 해야겠다. -&gt; 한 100명 나간다고 했음 -&gt; 실제로 최규빈이 찍은 100명중에 10명이 다 나감! | . 이 경우 애큐러시는 91%, 리컬은 100% (퇴사자 10명을 일단은 다 맞췄으므로). . 인사팀: (화가 많이 남) 멀쩡한 사람까지 다 퇴사할 것 같다고 하면 어떡해요? 최규빈 연구원이 나간다고 한 100명중에 실제로 10명만 나갔어요. . | 인사팀: 마치 총으로 과녁중앙에 맞춰 달라고 했더니 기관총을 가져와서 한번 긁은것이랑 뭐가 달라요? 맞추는게 문제가 아니고 precision이 너무 낮아요. . | . 최규빈: accuracy 90% 이상, recall은 높을수록 좋다는게 주문 아니었나요? | . 인사팀: (고민중..) 앞으로는 recall과 함께 precision도 같이 제출하세요. precision은 당신이 나간다고 한 사람중에 실제 나간사람의 비율을 의미해요. 이 경우는 $ frac{10}{100}$이니까 precision이 10%입니다. (속마음: recall 올리겠다고 무작정 너무 많이 예측하지 말란 말이야!) | . F1 score . - 정의: recall과 precision의 조화평균 . - (상황극 시점4) recall, precision을 모두 고려 . 최규빈: recall/precision을 같이 내는건 좋은데요, 둘은 trade off의 관계에 있습니다. 물론 둘다 올리는 모형이 있다면 좋지만 그게 쉽지는 않아요. 보통은 precision을 올리려면 recall이 희생되는 면이 있고요, recall을 올리려고 하면 precision이 다소 떨어집니다. . | 최규빈: 평가기준이 애매하다는 의미입니다. 모형1,2가 있는데 모형1은 모형2보다 precision이 약간 좋고 대신 recall이 떨어진다면 모형1이 좋은것입니까? 아니면 모형2가 좋은것입니까? . | . 인사팀: 그렇다면 둘을 평균내서 F1score를 계산해서 제출해주세요. | . Specificity(&#53945;&#51060;&#46020;), False Positive Rate(FPR) . - 정의: . (1) Specificity(특이도)=$ frac{TN}{FP+TN}$=$ frac{ # X/X}{ # X/O+ # X/X}$ . (2) False Positive Rate (FPR) = 1-Specificity(특이도) = $ frac{FP}{FP+TN}$=$ frac{ # X/O}{ # X/O+ # X/X}$ . - 의미: FPR = 오해해서 미안해, recall(=TPR)을 올리려고 보니 어쩔 수 없었어 ㅠㅠ . specificity는 안나간 사람을 안나갔다고 찾아낸 비율인데 별로 안중요하다. | FPR은 recall을 올리기 위해서 &quot;실제로는 회사 잘 다니고 있는 사람 중 최규빈이 나갈것 같다고 찍은 사람들&quot; 의 비율이다. | . 즉 생사람잡은 비율.. 오해해서 미안한 사람의 비율.. . ROC curve . - 정의: $x$축=FPR, $y$축=TPR 을 그린 커브 . - 의미: . 결국 &quot;오해해서 미안해 vs recall&quot;을 그린 곡선이 ROC커브이다. | 생각해보면 오해하는 사람이 많을수록 당연히 recall은 올라간다. 따라서 우상향하는 곡선이다. | 오해한 사람이 매우 적은데 recall이 우수하면 매우 좋은 모형이다. 그래서 초반부터 ROC값이 급격하게 올라가면 좋은 모형이다. | . Fashion MNIST &#45796;&#50577;&#54620; &#54217;&#44032;&#51648;&#54364;&#54876;&#50857; . - data . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() . X= x_train.reshape(-1,784) y= tf.keras.utils.to_categorical(y_train) XX = x_test.reshape(-1,784) yy = tf.keras.utils.to_categorical(y_test) . - 다양한 평가지표를 넣는 방법 (1) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation = &#39;softmax&#39;)) net.compile(loss=tf.losses.categorical_crossentropy, optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;,&#39;Recall&#39;]) net.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 2s 1ms/step - loss: 2.1429 - accuracy: 0.7392 - recall: 0.6980 Epoch 2/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.6420 - accuracy: 0.7838 - recall: 0.7256 Epoch 3/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.5696 - accuracy: 0.8013 - recall: 0.7390 Epoch 4/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4871 - accuracy: 0.8291 - recall: 0.7755 Epoch 5/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4436 - accuracy: 0.8435 - recall: 0.7994 . &lt;keras.callbacks.History at 0x7f1e581a76d0&gt; . net.evaluate(XX,yy) . 313/313 [==============================] - 0s 946us/step - loss: 0.4589 - accuracy: 0.8380 - recall: 0.7958 . [0.4589207172393799, 0.8379999995231628, 0.795799970626831] . - 다양한 평가지표를 넣는 방법 (2) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation = &#39;softmax&#39;)) net.compile(loss=tf.losses.categorical_crossentropy, optimizer=&#39;adam&#39;,metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()]) net.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 2s 1ms/step - loss: 2.3687 - categorical_accuracy: 0.7409 - recall: 0.6983 Epoch 2/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.6584 - categorical_accuracy: 0.7846 - recall: 0.7226 Epoch 3/5 1875/1875 [==============================] - 2s 995us/step - loss: 0.5752 - categorical_accuracy: 0.8088 - recall: 0.7578 Epoch 4/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4869 - categorical_accuracy: 0.8308 - recall: 0.7908 Epoch 5/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4575 - categorical_accuracy: 0.8406 - recall: 0.8007 . &lt;keras.callbacks.History at 0x7f1d89c517e0&gt; . net.evaluate(XX,yy) . 313/313 [==============================] - 0s 925us/step - loss: 0.4827 - categorical_accuracy: 0.8323 - recall: 0.7932 . [0.48269104957580566, 0.8323000073432922, 0.7932000160217285] . flatten layer . - 이미지 데이터를 분류하기 좋은 형태로 자료를 재정리하자. . X = tf.constant(x_train.reshape(-1,28,28,1),dtype=tf.float64) y = tf.keras.utils.to_categorical(y_train) XX = tf.constant(x_test.reshape(-1,28,28,1),dtype=tf.float64) yy = tf.keras.utils.to_categorical(y_test) . X.shape,XX.shape,y.shape,yy.shape . (TensorShape([60000, 28, 28, 1]), TensorShape([10000, 28, 28, 1]), (60000, 10), (10000, 10)) . - 일반적인 이미지 분석 모형을 적용하기 용이한 데이터 형태로 정리했다. -&gt; 그런데 모형에 넣고 돌릴려면 다시 차원을 펼쳐야 하지 않을까? . - 안펼치고 하고싶다. . flttn = tf.keras.layers.Flatten() . set(dir(flttn)) &amp; {&#39;__call__&#39;} . {&#39;__call__&#39;} . X.shape,flttn(X).shape, X.reshape(-1,784).shape . (TensorShape([60000, 28, 28, 1]), TensorShape([60000, 784]), TensorShape([60000, 784])) . - flttn . net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation = &#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation = &#39;softmax&#39;)) net.compile(loss=tf.losses.categorical_crossentropy, optimizer=&#39;adam&#39;,metrics=[tf.metrics.CategoricalAccuracy(),tf.metrics.Recall()]) net.fit(X,y,epochs=5) . Epoch 1/5 1875/1875 [==============================] - 2s 1ms/step - loss: 2.4544 - categorical_accuracy: 0.7473 - recall_2: 0.7128 Epoch 2/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.6754 - categorical_accuracy: 0.7847 - recall_2: 0.7243 Epoch 3/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.5578 - categorical_accuracy: 0.8135 - recall_2: 0.7598 Epoch 4/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4867 - categorical_accuracy: 0.8302 - recall_2: 0.7875 Epoch 5/5 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4228 - categorical_accuracy: 0.8474 - recall_2: 0.8080 . &lt;keras.callbacks.History at 0x7f1d89c50ca0&gt; . net.layers . [&lt;keras.layers.core.flatten.Flatten at 0x7f1d341ab310&gt;, &lt;keras.layers.core.dense.Dense at 0x7f1d341ab250&gt;, &lt;keras.layers.core.dense.Dense at 0x7f1d341bd4e0&gt;, &lt;keras.layers.core.dense.Dense at 0x7f1d34199fc0&gt;] . print(X.shape) print(net.layers[0](X).shape) print(net.layers[1](net.layers[0](X)).shape) print(net.layers[2](net.layers[1](net.layers[0](X))).shape) . (60000, 28, 28, 1) (60000, 784) (60000, 500) (60000, 500) . - 좀 더 복잡한 네트워크 -&gt; 하지만 한계가 보인다 -&gt; 좀 더 나은 아키텍처는 없을까 . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Flatten()) net.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(500,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(10,activation=&#39;softmax&#39;)) net.compile(loss=tf.losses.categorical_crossentropy, optimizer=&#39;adam&#39;,metrics=&#39;accuracy&#39;) net.fit(X,y,epochs=10) . Epoch 1/10 1875/1875 [==============================] - 2s 995us/step - loss: 1.1255 - accuracy: 0.7874 Epoch 2/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4725 - accuracy: 0.8325 Epoch 3/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.4147 - accuracy: 0.8532 Epoch 4/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3884 - accuracy: 0.8615 Epoch 5/10 1875/1875 [==============================] - 2s 973us/step - loss: 0.3738 - accuracy: 0.8675 Epoch 6/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3626 - accuracy: 0.8722 Epoch 7/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3514 - accuracy: 0.8774 Epoch 8/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3327 - accuracy: 0.8824 Epoch 9/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3282 - accuracy: 0.8831 Epoch 10/10 1875/1875 [==============================] - 2s 1ms/step - loss: 0.3164 - accuracy: 0.8862 . &lt;keras.callbacks.History at 0x7f17806f1db0&gt; . net.evaluate(XX,yy) . 313/313 [==============================] - 0s 852us/step - loss: 0.3776 - accuracy: 0.8676 . [0.37760305404663086, 0.8676000237464905] . - layer중에 우리는 끽해야 Dense정도 쓰고있었음. $ to$ flatten과 같은 다른 layer도 많음. $ to$ 이런것도 써보자 .",
            "url": "https://guebin.github.io/STBDA2022/2022/05/09/(10%EC%A3%BC%EC%B0%A8)-5%EC%9B%949%EC%9D%BC.html",
            "relUrl": "/2022/05/09/(10%EC%A3%BC%EC%B0%A8)-5%EC%9B%949%EC%9D%BC.html",
            "date": " • May 9, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "(9주차) 5월2일 (2)",
            "content": "&#44053;&#51032;&#50689;&#49345; . . import . import tensorflow as tf import matplotlib.pyplot as plt import numpy as np import tensorflow.experimental.numpy as tnp . tf.config.experimental.list_physical_devices() . [PhysicalDevice(name=&#39;/physical_device:CPU:0&#39;, device_type=&#39;CPU&#39;)] . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+ s + &#39;;}&#39;) . &#51473;&#44036;&#44256;&#49324; &#44288;&#47144; &#51105;&#45812; . &#51473;&#44036;&#44256;&#49324; 3&#48264;&#47928;&#51228; . - 특이한모형: 오버핏이 일어날 수 없는 모형이다. . 유의미한 coef: 상수항(bias), $ cos(t)$의 계수, $ cos(2t)$의 계수, $ cos(5t)$의 계수. | 유의미하지 않은 coef: $ cos(3t)$의 계수, $ cos(4t)$의 계수 | 유의미하지 않은 계수는 $n%$이 커질수록 0으로 추정된다 = $ cos(3t)$와 $ cos(5t)$는 사용자가 임의로 제외하지 않아도 결국 모형에서 알아서 제거된다 = overfit이 일어나지 않는다. 모형이 알아서 유의미한 변수만 뽑아서 fit하는 느낌 | . - 3번문제는 overfit이 일어나지 않는다. 이러한 신기한 일이 일어나는 이유는 모든 설명변수가 직교하기 때문임. . 이런 모형의 장점: overfit이 일어날 위험이 없으므로 train/test로 나누어 학습할 이유가 없다. (샘플만 버리는 꼴, test에 빼둔 observation까지 모아서 학습해 $ beta$를 좀 더 정확히 추론하는게 차라리 더 이득) | 이러한 모형에서 할일: 추정된 계수들이 0인지 아닌지만 test하면 된다. (이것을 유의성검정이라고 한다) | . - 직교기저의 예시 . 빨강과 파랑을 255,255만큼 섞으면 보라색이 된다. | 빨강과 파랑과 노랑을 각각 255,255,255만큼 섞으면 검은색이 된다. | 임의의 어떠한 색도 빨강,파랑,노랑의 조합으로 표현가능하다. 즉 $ text{color}= text{red}* beta_1 + text{blue}* beta_2 + text{yellow}* beta_3$ 이다. | (빨,파,노)는 색을 표현하는 basis이다. (적절한 $ beta_1, beta_2, beta_3$을 구하기만 하면 임의의 색도 표현가능) | (빨,보,노)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요할때 보라색-빨간색을 하면되니까) | (빨,보,검)역시 색을 표현하는 basis라 볼 수 있다. (파란색이 필요하면 보라색-빨간색을 하면되고, 노란색이 필요하면 검정색-보라색을 하면 되니까) | (빨,파,노)는 직교기저이다. | . - 3번에서 알아둘 것: (1) 직교기저의 개념 (추후 재설명) (2) 임의의 색을 표현하려면 3개의 basis가 필요함 . &#51473;&#44036;&#44256;&#49324; 1-(3)&#48264; &#47928;&#51228; . - 그림을 그려보자. . _x= tf.constant(np.arange(1,10001)/10000) _y= tnp.random.randn(10000) + (0.5 + 2*_x) plt.plot(_x,_y,&#39;.&#39;,alpha=0.1) . [&lt;matplotlib.lines.Line2D at 0x7f2ac00564c0&gt;] . - 저것 꼭 10000개 다 모아서 loss계산해야할까? . plt.plot(_x,_y,&#39;.&#39;,alpha=0.1) plt.plot(_x[::10],_y[::10],&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f2a906c30d0&gt;] . - 대충 이정도만 모아서 해도 비슷하지 않을까? $ to$ 해보자! . &#44221;&#49324;&#54616;&#44053;&#48277;&#44284; &#54869;&#47456;&#51201;&#44221;&#49324;&#54616;&#44053;&#48277; . ver1: &#47784;&#46304; &#49368;&#54540;&#51012; &#49324;&#50857;&#54616;&#50668; slope&#44228;&#49328; . - 단순회귀분석에서 샘플 10개 관측: $(x_1,y_1), dots,(x_{10},y_{10})$. . (epoch1) $loss= sum_{i=1}^{10}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ . (epoch2) $loss= sum_{i=1}^{10}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ . ... . ver2: &#54616;&#45208;&#51032; &#49368;&#54540;&#47564; &#49324;&#50857;&#54616;&#50668; slope&#44228;&#49328; . (epoch1) . $loss=(y_1- beta_0- beta_1x_1)^2 quad to quad slope quad to quad update$ | $loss=(y_2- beta_0- beta_1x_2)^2 quad to quad slope quad to quad update$ | ... | $loss=(y_{10}- beta_0- beta_1x_{10})^2 quad to quad slope quad to quad update$ | . (epoch2) . $loss=(y_1- beta_0- beta_1x_1)^2 quad to quad slope quad to quad update$ | $loss=(y_2- beta_0- beta_1x_2)^2 quad to quad slope quad to quad update$ | ... | $loss=(y_{10}- beta_0- beta_1x_{10})^2 quad to quad slope quad to quad update$ | . ... . ver3: $m( leq n)$&#44060;&#51032; &#49368;&#54540;&#47564; &#49324;&#50857;&#54616;&#50668; slope&#44228;&#49328; . $m=3$이라고 하자. . (epoch1) . $loss= sum_{i=1}^{3}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss= sum_{i=4}^{6}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss= sum_{i=7}^{9}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss=(y_{10}- beta_0- beta_1x_{10})^2 quad to quad slope quad to quad update$ | . (epoch2) . $loss= sum_{i=1}^{3}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss= sum_{i=4}^{6}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss= sum_{i=7}^{9}(y_i- beta_0- beta_1x_i)^2 quad to quad slope quad to quad update$ | $loss=(y_{10}- beta_0- beta_1x_{10})^2 quad to quad slope quad to quad update$ | . ... . &#50857;&#50612;&#51032; &#51221;&#47532; . &#50715;&#45216; (&#51328; &#45908; &#50628;&#48128;) . - ver1: gradient descent, batch gradient descent . - ver2: stochastic gradient descent . - ver3: mini-batch gradient descent, mini-batch stochastic gradient descent . &#50836;&#51608; . - ver1: gradient descent . - ver2: stochastic gradient descent with batch size = 1 . - ver3: stochastic gradient descent . https://www.deeplearningbook.org/contents/optimization.html, 알고리즘 8-1 참고. | . note: 이렇게 많이 쓰는 이유? ver1,2는 사실상 없는 방법이므로 . ver1,2,3 &#51060;&#50808;&#50640; &#51328; &#45908; &#51648;&#51200;&#48516;&#54620; &#44163;&#46308;&#51060; &#51080;&#45796;. . - ver2,3에서 샘플을 셔플할 수도 있다. . - ver3에서 일부 샘플이 학습에 참여 안하는 버전도 있다. . - 개인적 생각: 크게3개정도만 알면 괜찮고 나머지는 그렇게 유의미하지 않아보인다. . Discussion . - 핵심개념 . 메모리사용량: ver1 &gt; ver3 &gt; ver2 | 계산속도: ver1 &gt; ver3 &gt; ver2 | local-min에 갇힘: ver1 &gt; ver3 &gt; ver2 | . - 본질: GPU 메모리가 한정되어 있어서 ver1을 쓰지는 못한다. GPU 메모리를 가장 적게쓰는것은 ver2인데 이것은 너무 불안정하다. . - 틀리진 않지만 어색한 블로그 정리 내용들 . 경사하강법은 종종 국소최소점에 갇히는 문제가 있다. 이를 해결하기 위해서 등장한 방법이 확률적 경사하강법이다. --&gt; 영 틀린말은 아니지만 그걸 의도하고 만든건 아님 | 경사하강법은 계산시간이 오래걸린다. 계산을 빠르게 하기 위해서 등장한 방법이 확률적 경사하강법이다. --&gt; 1회 업데이트는 빠르게 계산함. 하지만 그것이 최적의 $ beta$를 빠르게 얻을 수 있다는 의미는 아님 | . fashion_mnist &#47784;&#46280; . tf.keras.datasets.fashion_mnist.load_data() . - tf.keras.datasets.fashion_mnist.load_data 의 리턴값 조사 . tf.keras.datasets.fashion_mnist.load_data?? . Signature: tf.keras.datasets.fashion_mnist.load_data() Source: @keras_export(&#39;keras.datasets.fashion_mnist.load_data&#39;) def load_data(): &#34;&#34;&#34;Loads the Fashion-MNIST dataset. This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The classes are: | Label | Description | |:--:|-| | 0 | T-shirt/top | | 1 | Trouser | | 2 | Pullover | | 3 | Dress | | 4 | Coat | | 5 | Sandal | | 6 | Shirt | | 7 | Sneaker | | 8 | Bag | | 9 | Ankle boot | Returns: Tuple of NumPy arrays: `(x_train, y_train), (x_test, y_test)`. **x_train**: uint8 NumPy array of grayscale image data with shapes `(60000, 28, 28)`, containing the training data. **y_train**: uint8 NumPy array of labels (integers in range 0-9) with shape `(60000,)` for the training data. **x_test**: uint8 NumPy array of grayscale image data with shapes (10000, 28, 28), containing the test data. **y_test**: uint8 NumPy array of labels (integers in range 0-9) with shape `(10000,)` for the test data. Example: python (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() assert x_train.shape == (60000, 28, 28) assert x_test.shape == (10000, 28, 28) assert y_train.shape == (60000,) assert y_test.shape == (10000,) License: The copyright for Fashion-MNIST is held by Zalando SE. Fashion-MNIST is licensed under the [MIT license]( https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE). &#34;&#34;&#34; dirname = os.path.join(&#39;datasets&#39;, &#39;fashion-mnist&#39;) base = &#39;https://storage.googleapis.com/tensorflow/tf-keras-datasets/&#39; files = [ &#39;train-labels-idx1-ubyte.gz&#39;, &#39;train-images-idx3-ubyte.gz&#39;, &#39;t10k-labels-idx1-ubyte.gz&#39;, &#39;t10k-images-idx3-ubyte.gz&#39; ] paths = [] for fname in files: paths.append(get_file(fname, origin=base + fname, cache_subdir=dirname)) with gzip.open(paths[0], &#39;rb&#39;) as lbpath: y_train = np.frombuffer(lbpath.read(), np.uint8, offset=8) with gzip.open(paths[1], &#39;rb&#39;) as imgpath: x_train = np.frombuffer( imgpath.read(), np.uint8, offset=16).reshape(len(y_train), 28, 28) with gzip.open(paths[2], &#39;rb&#39;) as lbpath: y_test = np.frombuffer(lbpath.read(), np.uint8, offset=8) with gzip.open(paths[3], &#39;rb&#39;) as imgpath: x_test = np.frombuffer( imgpath.read(), np.uint8, offset=16).reshape(len(y_test), 28, 28) return (x_train, y_train), (x_test, y_test) File: ~/anaconda3/envs/tfcpu/lib/python3.9/site-packages/keras/datasets/fashion_mnist.py Type: function . &#45936;&#51060;&#53552;&#49373;&#49457; &#48143; &#53456;&#49353; . - tf.keras.datasets.fashion_mnist.load_data()를 이용한 데이터 생성 . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data() . - 차원확인 . x_train.shape, y_train.shape, x_test.shape,y_test.shape . ((60000, 28, 28), (60000,), (10000, 28, 28), (10000,)) . 60000은 obs숫자인듯 | (28,28)은 28픽셀,28픽셀을 의미하는듯 | train/test는 6:1로 나눈것 같음 | . - 첫번째 obs . plt.imshow(x_train[0]) . &lt;matplotlib.image.AxesImage at 0x7f6520fd1e20&gt; . y_train[0] . 9 . 첫번쨰 obs에 대응하는 라벨 | . - 첫번째 obs와 동일한 라벨을 가지는 그림을 찾아보자. . np.where(y_train==9) . (array([ 0, 11, 15, ..., 59932, 59970, 59978]),) . y_train[11] . 9 . plt.imshow(x_train[11]) . &lt;matplotlib.image.AxesImage at 0x7f65219f9e80&gt; . &#45936;&#51060;&#53552;&#44396;&#51312; . - ${ bf X}$: (n,28,28) . - ${ bf y}$: (n,) , $y=0,1,2,3, dots,9$ . &#50696;&#51228;1 . &#45936;&#51060;&#53552; &#51221;&#47532; . - y=0,1에 대응하는 이미지만 정리하자. (우리가 배운건 로지스틱이니까) . y= y_train[(y_train==0) | (y_train==1)].reshape(-1,1) X= x_train[(y_train==0) | (y_train==1)].reshape(-1,784) yy= y_test[(y_test==0) | (y_test==1)].reshape(-1,1) XX= x_test[(y_test==0) | (y_test==1)].reshape(-1,784) . X.shape, y.shape, XX.shape, yy.shape . ((12000, 784), (12000, 1), (2000, 784), (2000, 1)) . &#54400;&#51060;1: &#51008;&#45769;&#52789;&#51012; &#54252;&#54632;&#54620; &#49888;&#44221;&#47581; // epochs=100 . gv(&#39;&#39;&#39; splines=line subgraph cluster_1{ style=filled; color=lightgrey; &quot;x1&quot; &quot;x2&quot; &quot;..&quot; &quot;x784&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x1&quot; -&gt; &quot;node1&quot; &quot;x2&quot; -&gt; &quot;node1&quot; &quot;..&quot; -&gt; &quot;node1&quot; &quot;x784&quot; -&gt; &quot;node1&quot; &quot;x1&quot; -&gt; &quot;node2&quot; &quot;x2&quot; -&gt; &quot;node2&quot; &quot;..&quot; -&gt; &quot;node2&quot; &quot;x784&quot; -&gt; &quot;node2&quot; &quot;x1&quot; -&gt; &quot;...&quot; &quot;x2&quot; -&gt; &quot;...&quot; &quot;..&quot; -&gt; &quot;...&quot; &quot;x784&quot; -&gt; &quot;...&quot; &quot;x1&quot; -&gt; &quot;node30&quot; &quot;x2&quot; -&gt; &quot;node30&quot; &quot;..&quot; -&gt; &quot;node30&quot; &quot;x784&quot; -&gt; &quot;node30&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;y&quot; &quot;node2&quot; -&gt; &quot;y&quot; &quot;...&quot; -&gt; &quot;y&quot; &quot;node30&quot; -&gt; &quot;y&quot; label = &quot;Layer 2: sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: relu cluster_3 Layer 2: sigmoid x1 x1 node1 node1 x1&#45;&gt;node1 node2 node2 x1&#45;&gt;node2 ... ... x1&#45;&gt;... node30 node30 x1&#45;&gt;node30 x2 x2 x2&#45;&gt;node1 x2&#45;&gt;node2 x2&#45;&gt;... x2&#45;&gt;node30 .. .. ..&#45;&gt;node1 ..&#45;&gt;node2 ..&#45;&gt;... ..&#45;&gt;node30 x784 x784 x784&#45;&gt;node1 x784&#45;&gt;node2 x784&#45;&gt;... x784&#45;&gt;node30 y y node1&#45;&gt;y node2&#45;&gt;y ...&#45;&gt;y node30&#45;&gt;y tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;sgd&#39;,loss=tf.losses.binary_crossentropy) net.fit(X,y,epochs=100,batch_size=12000) . Epoch 1/100 1/1 [==============================] - 0s 122ms/step - loss: 220.9145 Epoch 2/100 1/1 [==============================] - 0s 9ms/step - loss: 6800.3174 Epoch 3/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7045 Epoch 4/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7012 Epoch 5/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7004 Epoch 6/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6997 Epoch 7/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6991 Epoch 8/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6985 Epoch 9/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6979 Epoch 10/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6976 Epoch 11/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6973 Epoch 12/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6970 Epoch 13/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6968 Epoch 14/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6966 Epoch 15/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6964 Epoch 16/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6963 Epoch 17/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6961 Epoch 18/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6959 Epoch 19/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6958 Epoch 20/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6956 Epoch 21/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6955 Epoch 22/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6953 Epoch 23/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6952 Epoch 24/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6951 Epoch 25/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6949 Epoch 26/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6948 Epoch 27/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6947 Epoch 28/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6946 Epoch 29/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6945 Epoch 30/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6944 Epoch 31/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6943 Epoch 32/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6942 Epoch 33/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6942 Epoch 34/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6941 Epoch 35/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6940 Epoch 36/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6940 Epoch 37/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6939 Epoch 38/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6939 Epoch 39/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6938 Epoch 40/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6937 Epoch 41/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6937 Epoch 42/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6936 Epoch 43/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6936 Epoch 44/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6935 Epoch 45/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6935 Epoch 46/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6934 Epoch 47/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6934 Epoch 48/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6934 Epoch 49/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 50/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 51/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 52/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 53/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 54/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 55/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 56/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 57/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 58/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 59/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 60/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 61/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 62/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6933 Epoch 63/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 64/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 65/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 66/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 67/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6933 Epoch 68/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 69/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 70/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 71/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 72/100 1/1 [==============================] - 0s 10ms/step - loss: 0.6932 Epoch 73/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 74/100 1/1 [==============================] - 0s 10ms/step - loss: 0.6932 Epoch 75/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 76/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 77/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 78/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 79/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 80/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 81/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 82/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 83/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 84/100 1/1 [==============================] - 0s 10ms/step - loss: 0.6932 Epoch 85/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 86/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6932 Epoch 87/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 88/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 89/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 90/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 91/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 92/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 93/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 94/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 95/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 96/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 97/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 98/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 99/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 Epoch 100/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6932 . &lt;keras.callbacks.History at 0x7f640c5e9c40&gt; . np.mean((net(X)&gt;0.5) == y) . 0.5000833333333333 . np.mean((net(XX)&gt;0.5) == yy) . 0.5 . &#54400;&#51060;2: &#50741;&#54000;&#47560;&#51060;&#51200; &#44060;&#49440; . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.binary_crossentropy) net.fit(X,y,epochs=100,batch_size=12000) . Epoch 1/100 1/1 [==============================] - 0s 138ms/step - loss: 220.9145 Epoch 2/100 1/1 [==============================] - 0s 10ms/step - loss: 88.9490 Epoch 3/100 1/1 [==============================] - 0s 10ms/step - loss: 7.5895 Epoch 4/100 1/1 [==============================] - 0s 9ms/step - loss: 33.7521 Epoch 5/100 1/1 [==============================] - 0s 9ms/step - loss: 40.2290 Epoch 6/100 1/1 [==============================] - 0s 9ms/step - loss: 28.9675 Epoch 7/100 1/1 [==============================] - 0s 9ms/step - loss: 16.5128 Epoch 8/100 1/1 [==============================] - 0s 10ms/step - loss: 9.4911 Epoch 9/100 1/1 [==============================] - 0s 10ms/step - loss: 6.2027 Epoch 10/100 1/1 [==============================] - 0s 9ms/step - loss: 5.2417 Epoch 11/100 1/1 [==============================] - 0s 9ms/step - loss: 5.5172 Epoch 12/100 1/1 [==============================] - 0s 9ms/step - loss: 6.5900 Epoch 13/100 1/1 [==============================] - 0s 9ms/step - loss: 7.8605 Epoch 14/100 1/1 [==============================] - 0s 9ms/step - loss: 8.5884 Epoch 15/100 1/1 [==============================] - 0s 9ms/step - loss: 8.3991 Epoch 16/100 1/1 [==============================] - 0s 9ms/step - loss: 7.4675 Epoch 17/100 1/1 [==============================] - 0s 9ms/step - loss: 6.2581 Epoch 18/100 1/1 [==============================] - 0s 9ms/step - loss: 5.1274 Epoch 19/100 1/1 [==============================] - 0s 9ms/step - loss: 4.2382 Epoch 20/100 1/1 [==============================] - 0s 9ms/step - loss: 3.6033 Epoch 21/100 1/1 [==============================] - 0s 9ms/step - loss: 3.1860 Epoch 22/100 1/1 [==============================] - 0s 9ms/step - loss: 2.9233 Epoch 23/100 1/1 [==============================] - 0s 9ms/step - loss: 2.7560 Epoch 24/100 1/1 [==============================] - 0s 9ms/step - loss: 2.6421 Epoch 25/100 1/1 [==============================] - 0s 9ms/step - loss: 2.5490 Epoch 26/100 1/1 [==============================] - 0s 9ms/step - loss: 2.4612 Epoch 27/100 1/1 [==============================] - 0s 9ms/step - loss: 2.3617 Epoch 28/100 1/1 [==============================] - 0s 9ms/step - loss: 2.2378 Epoch 29/100 1/1 [==============================] - 0s 9ms/step - loss: 2.0874 Epoch 30/100 1/1 [==============================] - 0s 9ms/step - loss: 1.9117 Epoch 31/100 1/1 [==============================] - 0s 9ms/step - loss: 1.7239 Epoch 32/100 1/1 [==============================] - 0s 9ms/step - loss: 1.5409 Epoch 33/100 1/1 [==============================] - 0s 9ms/step - loss: 1.3663 Epoch 34/100 1/1 [==============================] - 0s 9ms/step - loss: 1.2210 Epoch 35/100 1/1 [==============================] - 0s 9ms/step - loss: 1.1035 Epoch 36/100 1/1 [==============================] - 0s 9ms/step - loss: 1.0208 Epoch 37/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9766 Epoch 38/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9628 Epoch 39/100 1/1 [==============================] - 0s 8ms/step - loss: 0.9717 Epoch 40/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9883 Epoch 41/100 1/1 [==============================] - 0s 9ms/step - loss: 1.0039 Epoch 42/100 1/1 [==============================] - 0s 9ms/step - loss: 1.0156 Epoch 43/100 1/1 [==============================] - 0s 9ms/step - loss: 1.0181 Epoch 44/100 1/1 [==============================] - 0s 9ms/step - loss: 1.0067 Epoch 45/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9809 Epoch 46/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9443 Epoch 47/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9019 Epoch 48/100 1/1 [==============================] - 0s 9ms/step - loss: 0.8571 Epoch 49/100 1/1 [==============================] - 0s 9ms/step - loss: 0.8146 Epoch 50/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7768 Epoch 51/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7489 Epoch 52/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7294 Epoch 53/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7186 Epoch 54/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7125 Epoch 55/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7080 Epoch 56/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7044 Epoch 57/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7002 Epoch 58/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6949 Epoch 59/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6884 Epoch 60/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6806 Epoch 61/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6715 Epoch 62/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6615 Epoch 63/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6510 Epoch 64/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6404 Epoch 65/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6302 Epoch 66/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6209 Epoch 67/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6127 Epoch 68/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6060 Epoch 69/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6007 Epoch 70/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5963 Epoch 71/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5924 Epoch 72/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5888 Epoch 73/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5853 Epoch 74/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5816 Epoch 75/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5778 Epoch 76/100 1/1 [==============================] - 0s 10ms/step - loss: 0.5736 Epoch 77/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5691 Epoch 78/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5644 Epoch 79/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5595 Epoch 80/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5547 Epoch 81/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5501 Epoch 82/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5457 Epoch 83/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5417 Epoch 84/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5379 Epoch 85/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5344 Epoch 86/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5310 Epoch 87/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5276 Epoch 88/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5242 Epoch 89/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5208 Epoch 90/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5174 Epoch 91/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5140 Epoch 92/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5108 Epoch 93/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5075 Epoch 94/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5044 Epoch 95/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5014 Epoch 96/100 1/1 [==============================] - 0s 8ms/step - loss: 0.4986 Epoch 97/100 1/1 [==============================] - 0s 8ms/step - loss: 0.4960 Epoch 98/100 1/1 [==============================] - 0s 9ms/step - loss: 0.4935 Epoch 99/100 1/1 [==============================] - 0s 8ms/step - loss: 0.4909 Epoch 100/100 1/1 [==============================] - 0s 8ms/step - loss: 0.4885 . &lt;keras.callbacks.History at 0x7f640c513e50&gt; . np.mean((net(X)&gt;0.5) == y) . 0.98125 . np.mean((net(XX)&gt;0.5) == yy) . 0.977 . &#54400;&#51060;3: &#52980;&#54028;&#51068;&#49884; metrics=[&#39;accuracy&#39;] &#52628;&#44032; . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.binary_crossentropy,metrics=[&#39;accuracy&#39;]) net.fit(X,y,epochs=100,batch_size=12000) . Epoch 1/100 1/1 [==============================] - 0s 150ms/step - loss: 220.9145 - accuracy: 0.5000 Epoch 2/100 1/1 [==============================] - 0s 9ms/step - loss: 88.9490 - accuracy: 0.5073 Epoch 3/100 1/1 [==============================] - 0s 9ms/step - loss: 7.5895 - accuracy: 0.8208 Epoch 4/100 1/1 [==============================] - 0s 10ms/step - loss: 33.7521 - accuracy: 0.5972 Epoch 5/100 1/1 [==============================] - 0s 9ms/step - loss: 40.2290 - accuracy: 0.5723 Epoch 6/100 1/1 [==============================] - 0s 9ms/step - loss: 28.9675 - accuracy: 0.6442 Epoch 7/100 1/1 [==============================] - 0s 11ms/step - loss: 16.5128 - accuracy: 0.8061 Epoch 8/100 1/1 [==============================] - 0s 9ms/step - loss: 9.4911 - accuracy: 0.8947 Epoch 9/100 1/1 [==============================] - 0s 9ms/step - loss: 6.2027 - accuracy: 0.9355 Epoch 10/100 1/1 [==============================] - 0s 9ms/step - loss: 5.2417 - accuracy: 0.9404 Epoch 11/100 1/1 [==============================] - 0s 9ms/step - loss: 5.5172 - accuracy: 0.9270 Epoch 12/100 1/1 [==============================] - 0s 9ms/step - loss: 6.5900 - accuracy: 0.9021 Epoch 13/100 1/1 [==============================] - 0s 9ms/step - loss: 7.8605 - accuracy: 0.8788 Epoch 14/100 1/1 [==============================] - 0s 9ms/step - loss: 8.5884 - accuracy: 0.8647 Epoch 15/100 1/1 [==============================] - 0s 9ms/step - loss: 8.3991 - accuracy: 0.8664 Epoch 16/100 1/1 [==============================] - 0s 9ms/step - loss: 7.4675 - accuracy: 0.8793 Epoch 17/100 1/1 [==============================] - 0s 9ms/step - loss: 6.2581 - accuracy: 0.8982 Epoch 18/100 1/1 [==============================] - 0s 9ms/step - loss: 5.1274 - accuracy: 0.9156 Epoch 19/100 1/1 [==============================] - 0s 9ms/step - loss: 4.2382 - accuracy: 0.9302 Epoch 20/100 1/1 [==============================] - 0s 9ms/step - loss: 3.6033 - accuracy: 0.9426 Epoch 21/100 1/1 [==============================] - 0s 9ms/step - loss: 3.1860 - accuracy: 0.9509 Epoch 22/100 1/1 [==============================] - 0s 9ms/step - loss: 2.9233 - accuracy: 0.9551 Epoch 23/100 1/1 [==============================] - 0s 9ms/step - loss: 2.7560 - accuracy: 0.9574 Epoch 24/100 1/1 [==============================] - 0s 9ms/step - loss: 2.6421 - accuracy: 0.9594 Epoch 25/100 1/1 [==============================] - 0s 9ms/step - loss: 2.5490 - accuracy: 0.9599 Epoch 26/100 1/1 [==============================] - 0s 9ms/step - loss: 2.4612 - accuracy: 0.9603 Epoch 27/100 1/1 [==============================] - 0s 9ms/step - loss: 2.3617 - accuracy: 0.9608 Epoch 28/100 1/1 [==============================] - 0s 9ms/step - loss: 2.2378 - accuracy: 0.9612 Epoch 29/100 1/1 [==============================] - 0s 9ms/step - loss: 2.0874 - accuracy: 0.9619 Epoch 30/100 1/1 [==============================] - 0s 9ms/step - loss: 1.9117 - accuracy: 0.9630 Epoch 31/100 1/1 [==============================] - 0s 9ms/step - loss: 1.7239 - accuracy: 0.9641 Epoch 32/100 1/1 [==============================] - 0s 9ms/step - loss: 1.5409 - accuracy: 0.9657 Epoch 33/100 1/1 [==============================] - 0s 9ms/step - loss: 1.3663 - accuracy: 0.9670 Epoch 34/100 1/1 [==============================] - 0s 9ms/step - loss: 1.2210 - accuracy: 0.9685 Epoch 35/100 1/1 [==============================] - 0s 9ms/step - loss: 1.1035 - accuracy: 0.9688 Epoch 36/100 1/1 [==============================] - 0s 9ms/step - loss: 1.0208 - accuracy: 0.9696 Epoch 37/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9766 - accuracy: 0.9705 Epoch 38/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9628 - accuracy: 0.9708 Epoch 39/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9717 - accuracy: 0.9715 Epoch 40/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9883 - accuracy: 0.9706 Epoch 41/100 1/1 [==============================] - 0s 9ms/step - loss: 1.0039 - accuracy: 0.9699 Epoch 42/100 1/1 [==============================] - 0s 9ms/step - loss: 1.0156 - accuracy: 0.9685 Epoch 43/100 1/1 [==============================] - 0s 9ms/step - loss: 1.0181 - accuracy: 0.9681 Epoch 44/100 1/1 [==============================] - 0s 9ms/step - loss: 1.0067 - accuracy: 0.9686 Epoch 45/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9809 - accuracy: 0.9693 Epoch 46/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9443 - accuracy: 0.9703 Epoch 47/100 1/1 [==============================] - 0s 9ms/step - loss: 0.9019 - accuracy: 0.9711 Epoch 48/100 1/1 [==============================] - 0s 9ms/step - loss: 0.8571 - accuracy: 0.9722 Epoch 49/100 1/1 [==============================] - 0s 9ms/step - loss: 0.8146 - accuracy: 0.9737 Epoch 50/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7768 - accuracy: 0.9743 Epoch 51/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7489 - accuracy: 0.9753 Epoch 52/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7294 - accuracy: 0.9759 Epoch 53/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7186 - accuracy: 0.9767 Epoch 54/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7125 - accuracy: 0.9774 Epoch 55/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7080 - accuracy: 0.9776 Epoch 56/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7044 - accuracy: 0.9777 Epoch 57/100 1/1 [==============================] - 0s 9ms/step - loss: 0.7002 - accuracy: 0.9776 Epoch 58/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6949 - accuracy: 0.9778 Epoch 59/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6884 - accuracy: 0.9779 Epoch 60/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6806 - accuracy: 0.9784 Epoch 61/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6715 - accuracy: 0.9786 Epoch 62/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6615 - accuracy: 0.9786 Epoch 63/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6510 - accuracy: 0.9784 Epoch 64/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6404 - accuracy: 0.9786 Epoch 65/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6302 - accuracy: 0.9787 Epoch 66/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6209 - accuracy: 0.9791 Epoch 67/100 1/1 [==============================] - 0s 8ms/step - loss: 0.6127 - accuracy: 0.9787 Epoch 68/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6060 - accuracy: 0.9791 Epoch 69/100 1/1 [==============================] - 0s 9ms/step - loss: 0.6007 - accuracy: 0.9792 Epoch 70/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5963 - accuracy: 0.9795 Epoch 71/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5924 - accuracy: 0.9793 Epoch 72/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5888 - accuracy: 0.9791 Epoch 73/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5853 - accuracy: 0.9790 Epoch 74/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5816 - accuracy: 0.9793 Epoch 75/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5778 - accuracy: 0.9794 Epoch 76/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5736 - accuracy: 0.9795 Epoch 77/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5691 - accuracy: 0.9794 Epoch 78/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5644 - accuracy: 0.9794 Epoch 79/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5595 - accuracy: 0.9796 Epoch 80/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5547 - accuracy: 0.9796 Epoch 81/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5501 - accuracy: 0.9798 Epoch 82/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5457 - accuracy: 0.9800 Epoch 83/100 1/1 [==============================] - 0s 9ms/step - loss: 0.5417 - accuracy: 0.9800 Epoch 84/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5379 - accuracy: 0.9804 Epoch 85/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5344 - accuracy: 0.9807 Epoch 86/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5310 - accuracy: 0.9807 Epoch 87/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5276 - accuracy: 0.9807 Epoch 88/100 1/1 [==============================] - 0s 7ms/step - loss: 0.5242 - accuracy: 0.9808 Epoch 89/100 1/1 [==============================] - 0s 7ms/step - loss: 0.5208 - accuracy: 0.9808 Epoch 90/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5174 - accuracy: 0.9811 Epoch 91/100 1/1 [==============================] - 0s 7ms/step - loss: 0.5140 - accuracy: 0.9812 Epoch 92/100 1/1 [==============================] - 0s 7ms/step - loss: 0.5108 - accuracy: 0.9812 Epoch 93/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5075 - accuracy: 0.9813 Epoch 94/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5044 - accuracy: 0.9814 Epoch 95/100 1/1 [==============================] - 0s 8ms/step - loss: 0.5014 - accuracy: 0.9816 Epoch 96/100 1/1 [==============================] - 0s 8ms/step - loss: 0.4986 - accuracy: 0.9815 Epoch 97/100 1/1 [==============================] - 0s 7ms/step - loss: 0.4960 - accuracy: 0.9815 Epoch 98/100 1/1 [==============================] - 0s 8ms/step - loss: 0.4935 - accuracy: 0.9812 Epoch 99/100 1/1 [==============================] - 0s 8ms/step - loss: 0.4909 - accuracy: 0.9812 Epoch 100/100 1/1 [==============================] - 0s 8ms/step - loss: 0.4885 - accuracy: 0.9812 . &lt;keras.callbacks.History at 0x7f640c46bc70&gt; . net.evaluate(X,y) . 375/375 [==============================] - 0s 349us/step - loss: 0.4860 - accuracy: 0.9812 . [0.48598653078079224, 0.981249988079071] . net.evaluate(XX,yy) . 63/63 [==============================] - 0s 617us/step - loss: 0.4294 - accuracy: 0.9770 . [0.42936256527900696, 0.9769999980926514] . &#54400;&#51060;4: &#54869;&#47456;&#51201;&#44221;&#49324;&#54616;&#44053;&#48277; &#51060;&#50857; // epochs=10 . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(30,activation=&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;adam&#39;,loss=tf.losses.binary_crossentropy,metrics=[&#39;accuracy&#39;]) net.fit(X,y,epochs=10,batch_size=120) . Epoch 1/10 100/100 [==============================] - 0s 827us/step - loss: 5.6484 - accuracy: 0.9418 Epoch 2/10 100/100 [==============================] - 0s 747us/step - loss: 0.5078 - accuracy: 0.9793 Epoch 3/10 100/100 [==============================] - 0s 734us/step - loss: 0.3784 - accuracy: 0.9818 Epoch 4/10 100/100 [==============================] - 0s 765us/step - loss: 0.3390 - accuracy: 0.9828 Epoch 5/10 100/100 [==============================] - 0s 735us/step - loss: 0.2474 - accuracy: 0.9857 Epoch 6/10 100/100 [==============================] - 0s 717us/step - loss: 0.2116 - accuracy: 0.9870 Epoch 7/10 100/100 [==============================] - 0s 734us/step - loss: 0.1724 - accuracy: 0.9889 Epoch 8/10 100/100 [==============================] - 0s 784us/step - loss: 0.1711 - accuracy: 0.9880 Epoch 9/10 100/100 [==============================] - 0s 795us/step - loss: 0.1491 - accuracy: 0.9894 Epoch 10/10 100/100 [==============================] - 0s 723us/step - loss: 0.1550 - accuracy: 0.9896 . &lt;keras.callbacks.History at 0x7f640c2d9fa0&gt; . net.evaluate(X,y) . 375/375 [==============================] - 0s 339us/step - loss: 0.1124 - accuracy: 0.9923 . [0.11242959648370743, 0.9922500252723694] . net.evaluate(XX,yy) . 63/63 [==============================] - 0s 566us/step - loss: 0.2988 - accuracy: 0.9845 . [0.29883989691734314, 0.984499990940094] .",
            "url": "https://guebin.github.io/STBDA2022/2022/05/02/(9%EC%A3%BC%EC%B0%A8)-5%EC%9B%942%EC%9D%BC(2).html",
            "relUrl": "/2022/05/02/(9%EC%A3%BC%EC%B0%A8)-5%EC%9B%942%EC%9D%BC(2).html",
            "date": " • May 2, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "(9주차) 5월2일 (1)",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import numpy as np import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import matplotlib.pyplot as plt . &#50864;&#46020;&#54632;&#49688;&#50752; &#52572;&#45824;&#50864;&#46020;&#52628;&#51221;&#47049; . (예제) . $X_i overset{iid}{ sim} Ber(p)$에서 얻은 샘플이 아래와 같다고 하자. . x=[0,1,0,1] x . [0, 1, 0, 1] . $p$는 얼마라고 볼 수 있는가? --&gt; 0.5 . 왜?? $p$가 0.5라고 주장할 수 있는 이론적 근거, 혹은 논리체계가 무엇인가? . - suppose: $p=0.1$ 이라고 하자. . 그렇다면 $(x_1,x_2,x_3,x_4)=(0,1,0,1)$와 같은 샘플이 얻어질 확률이 아래와 같다. . 0.9 * 0.1 * 0.9 * 0.1 . 0.008100000000000001 . - suppose: $p=0.2$ 이라고 하자. . 그렇다면 $(x_1,x_2,x_3,x_4)=(0,1,0,1)$와 같은 샘플이 얻어질 확률이 아래와 같다. . 0.8 * 0.2 * 0.8 * 0.2 . 0.025600000000000008 . - 질문1: $p=0.1$인것 같냐? 아니면 $p=0.2$인것 같냐? -&gt; $p=0.2$ . 왜?? $p=0.2$일 확률이 더 크다! | . . (여기서 잠깐 중요한것) 확률이라는 말을 함부로 쓸 수 없다. . - 0.0256은 &quot;$p=0.2$일 경우 샘플 (0,1,0,1)이 얻어질 확률&quot;이지 &quot;$p=0.2$일 확률&quot;은 아니다. . &quot;$p=0.2$인 확률&quot; 이라는 개념이 성립하려면 아래코드에서 sum([(1-p)*p*(1-p)*p for p in _plist])이 1보다는 작아야 한다. (그런데 1보다 크다) . _plist = np.linspace(0.499,0.501,1000) sum([(1-p)*p*(1-p)*p for p in _plist]) . 62.49983299986714 . - 확률이라는 말을 쓸 수 없지만 확률의 느낌은 있음 -&gt; 가능도라는 말을 쓰자. . 0.0256 $=$ $p$가 0.2일 경우 샘플 (0,1,0,1)이 얻어질 확률 $=$ $p$가 0.2일 가능도 | . . - 다시 질문1로 돌아가자! . 질문1: $p=0.1$인 것 같냐? 아니면 $p=0.2$인 것 같냐? -&gt; 답 $p=0.2$ -&gt; 왜? $p=0.2$인 가능도가 더 크니까! | 질문2: $p=0.2$인 것 같냐? 아니면 $p=0.3$인 것 같냐? -&gt; 답 $p=0.3$ -&gt; 왜? $p=0.3$인 가능도가 더 크니까! | 질문3: ... | . - 궁극의 질문: $p$가 뭐일 것 같아? . $p$가 입력으로 들어가면 가능도가 계산되는 함수를 만들자. | 그 함수를 최대화하는 $p$를 찾자. | 그 $p$가 궁극의 질문에 대한 대답이 된다. | . - 잠깐 용어정리 . 가능도함수 $=$ 우도함수 $=$ likelihood function $:=$ $L(p)$ | $p$의 maximum likelihood estimator $=$ p의 MLE $:=$ $ hat{p}^{mle}$ $=$ $ text{argmax}_p L(p)$ $=$ $ hat{p}$ | . (예제의 풀이) . - 이 예제의 경우 가능도함수를 정의하자. . $L(p)$: $p$의 가능도함수 = $p$가 모수일때 샘플 (0,1,0,1)이 얻어질 확률 = $p$가 모수일때 $x_1$이 0일 확률 $ times dots times$ $p$가 모수일때 $x_4$가 1일 확률 | $L(p)= prod_{i=1}^{4} f(x_i;p)= prod_{i=1}^{4}p^{x_i}(1-p)^{1-x_i}$ | . . Note: 참고로 이 과정을 일반화 하면 $X_1, dots,X_n overset{iid}{ sim} Ber(p)$ 일때 $p$의 likelihood function은 $ prod_{i=1}^{n}p^{x_i}(1-p)^{1-x_i}$ 라고 볼 수 있다. . . Note: 더 일반화: $x_1, dots,x_n$이 pdf가 $f(x)$인 분포에서 뽑힌 서로 독립인 샘플일때 likelihood function은 $ prod_{i=1}^{n}f(x_i)$라고 볼 수 있다. . - 이 예제의 경우 $p$의 최대우도추정량을 구하면 . $$ hat{p}^{mle} = text{argmax}_p L(p) = text{argmax}_p big { p^2(1-p)^2 big }= frac{1}{2}$$ . &#51473;&#44036;&#44256;&#49324; 1&#48264; . (1) $N( mu, sigma)$에서 얻은 샘플이 아래와 같다고 할때 $ mu, sigma$의 MLE를 구하여라. . &lt;tf.Tensor: shape=(10000,), dtype=float64, numpy= array([ 4.12539849, 5.46696729, 5.27243374, ..., 2.89712332, 5.01072291, -1.13050477])&gt; . (2) $Ber(p)$에서 얻은 샘플이 아래와 같다고 할 때 $p$의 MLE를 구하여라. . &lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 0, 0, 1])&gt; . (3) $y_i = beta_0 + beta_1 x_i + epsilon_i$, $ epsilon_i overset{iid}{ sim} N(0,1)$ 일때 $( beta_0, beta_1)$의 MLE를 구하여라. (회귀모형) . (풀이) 가능도함수 . $$L( beta_0, beta_1)= prod_{i=1}^{n}f(y_i), quad f(y_i)= frac{1}{ sqrt{2 pi}}e^{- frac{1}{2}(y_i- mu_i)^2}, quad mu_i= beta_0+ beta_1 x_i$$ . 를 최대화하는 $ beta_0, beta_1$을 구하면된다. 그런데 이것은 아래를 최소화하는 $ beta_0, beta_1$을 구하는 것과 같다. . $$- log L( beta_0, beta_1) = sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2$$ . 위의 식은 SSE와 같다. 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 $ beta_0, beta_1$을 구하면 된다. . 중간고사 1-(3)의 다른 풀이 . step1: 생성 . x= tf.constant(np.arange(1,10001)/10000) y= tnp.random.randn(10000) + (0.5 + 2*x) . step2: minimize MSEloss (원래는 maximize log-likelihood) . maximize likelihood였던 문제를 minimize MSEloss로 바꾸어도 되는근거? 주어진 함수(=가능도함수)를 최대화하는 $ beta_0, beta_1$은 MSE를 최소화하는 $ beta_0, beta_1$과 동일하므로 | . beta0= tf.Variable(1.0) beta1= tf.Variable(1.0) for i in range(2000): with tf.GradientTape() as tape: #minus_log_likelihood = tf.reduce_sum((y-beta0-beta1*x)**2) loss = tf.reduce_sum((y-beta0-beta1*x)**2) slope1, slope2 = tape.gradient(loss,[beta0,beta1]) beta0.assign_sub(slope1* 0.1/10000) # N=10000 beta1.assign_sub(slope2* 0.1/10000) . beta0,beta1 . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=0.49661896&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.0051448&gt;) . - 문제를 풀면서 생각해보니 손실함수는 -로그가능도함수로 선택하면 될 것 같다? . 손실함수를 선택하는 기준이 -로그가능도함수만 존재하는 것은 아니나 대부분 그러하긴함 | . (4) 출제하지 못한 중간고사 문제 . 아래의 모형을 생각하자. . $Y_i overset{iid}{ sim} Ber( pi_i)$ | $ pi_i = frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}= frac{ exp(-1+5x_i)}{1+ exp(-1+5x_i)}$ | . 아래는 위의 모형에서 얻은 샘플이다. . x = tnp.linspace(-1,1,2000) pi = tnp.exp(-1+5*x) / (1+tnp.exp(-1+5*x)) y = np.random.binomial(1,pi) y = tf.constant(y) . 함수 $L(w_0,w_1)$을 최대화하는 $(w_0,w_1)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $(w_0,w_1)$의 초기값은 모두 0.1로 설정할 것) . $$L(w_0,w_1)= prod_{i=1}^{n}f(y_i), quad f(x_i)={ pi_i}^{y_i}(1- pi_i)^{1-y_i}, quad pi_i= text{sigmoid}(w_0+w_1x_i)$$ . (풀이1) . w0hat = tf.Variable(1.0) w1hat = tf.Variable(1.0) . for i in range(1000): with tf.GradientTape() as tape: pihat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x)) pdf = pihat**y * (1-pihat)**(1-y) logL = tf.reduce_mean(tnp.log(pdf)) slope1,slope2 = tape.gradient(logL,[w0hat,w1hat]) w0hat.assign_add(slope1*0.1) w1hat.assign_add(slope2*0.1) . w0hat,w1hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-0.8875932&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=4.3785405&gt;) . (해석) - 로지스틱에서 가능도함수와 BCEloss의 관계 . $L(w_0,w_1)$를 최대화하는 $w_0,w_1$은 아래를 최소화하는 $w_0,w_1$와 같다. . $$- log L(w_0,w_1) = - sum_{i=1}^{n} big(y_i log( pi_i) + (1-y_i) log(1- pi_i) big)$$ . 이것은 최적의 $w_0,w_1$을 $ hat{w}_0, hat{w}_1$이라고 하면 $ hat{ pi}_i= frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= hat{y}_i$이 되고 따라서 위의 식은 $n times$BCEloss의 형태임을 쉽게 알 수 있다. . 결국 로지스틱 모형에서 $(w_0,w_1)$의 MLE를 구하기 위해서는 BCEloss를 최소화하는 $(w_0,w_1)$을 구하면 된다! . (풀이2) . w0hat = tf.Variable(1.0) w1hat = tf.Variable(1.0) . for i in range(1000): with tf.GradientTape() as tape: yhat = tnp.exp(w0hat+w1hat *x) / (1+tnp.exp(w0hat+w1hat *x)) loss = tf.losses.binary_crossentropy(y,yhat) slope1,slope2 = tape.gradient(loss,[w0hat,w1hat]) w0hat.assign_sub(slope1*0.1) w1hat.assign_sub(slope2*0.1) . w0hat,w1hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-0.8875934&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=4.3785415&gt;) . &#49552;&#49892;&#54632;&#49688;&#51032; &#49444;&#44228; (&#49440;&#53469;) . - 회귀분석이든 로지스틱이든 손실함수는 minus_log_likelihood 로 선택한다. . 그런데 (오차항이 정규분포인) 회귀분석 일때는 minus_log_likelihood 가 MSEloss가 되고 | 로지스틱일때는 minus_log_likelihood 가 BCEloss가 된다 | . - minus_log_likelihood가 손실함수를 선택하는 유일한 기준은 아니다. &lt; 참고만하세요, 이 수업에서는 안중요합니다. . 오차항이 대칭이고 서로독립이며 등분산 가정을 만족하는 어떠한 분포에서의 회귀모형이 있다고 하자. 이 회귀모형에서 $ hat{ beta}$은 여전히 MSEloss를 최소화하는 $ beta$를 구함으로써 얻을 수 있다. | 이 경우 MSEloss를 쓰는 이론적근거? $ hat{ beta}$이 BLUE가 되기 때문임 (가우스-마코프정리) | .",
            "url": "https://guebin.github.io/STBDA2022/2022/05/01/(9%EC%A3%BC%EC%B0%A8)-5%EC%9B%942%EC%9D%BC(1).html",
            "relUrl": "/2022/05/01/(9%EC%A3%BC%EC%B0%A8)-5%EC%9B%942%EC%9D%BC(1).html",
            "date": " • May 1, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "2022년 중간고사 해설",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import numpy as np import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import matplotlib.pyplot as plt . 1. &#44221;&#49324;&#54616;&#44053;&#48277;&#44284; tf.GradientTape()&#51032; &#49324;&#50857;&#48169;&#48277; (30&#51216;) . (1) 아래는 $X_i overset{iid}{ sim} N(3,2^2)$ 를 생성하는 코드이다. . tf.random.set_seed(43052) x= tnp.random.randn(10000)*2+3 x . &lt;tf.Tensor: shape=(10000,), dtype=float64, numpy= array([ 4.12539849, 5.46696729, 5.27243374, ..., 2.89712332, 5.01072291, -1.13050477])&gt; . 함수 $L( mu, sigma)$을 최대화하는 $( mu, sigma)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ mu$의 초기값은 2로 $ sigma$의 초기값은 3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)= frac{1}{ sqrt{2 pi} sigma}e^{- frac{1}{2}( frac{x_i- mu}{ sigma})^2}$$ . (풀이) . sigma = tf.Variable(3.0) mu = tf.Variable(2.0) . with tf.GradientTape() as tape: pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2) logL = tf.reduce_sum(tnp.log(pdf) ) tape.gradient(logL,[mu,sigma]) . [&lt;tf.Tensor: shape=(), dtype=float32, numpy=1129.3353&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=-1488.3431&gt;] . for i in range(1000): with tf.GradientTape() as tape: pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2) logL = tf.reduce_sum(tnp.log(pdf) ) slope1, slope2 = tape.gradient(logL,[mu,sigma]) mu.assign_add(slope1* 0.1/10000) # N=10000 sigma.assign_add(slope2* 0.1/10000) . mu,sigma . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.0163972&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9870595&gt;) . (2) 아래는 $X_i overset{iid}{ sim} Ber(0.8)$을 생성하는 코드이다. . tf.random.set_seed(43052) x= tf.constant(np.random.binomial(1,0.8,(10000,))) x . &lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([1, 1, 1, ..., 0, 0, 1])&gt; . 함수 $L(p)$을 최대화하는 $p$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $p$의 초기값은 0.3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)=p^{x_i}(1-p)^{1-x_i}$$ . (풀이) . p=tf.Variable(0.3) for i in range(1000): with tf.GradientTape() as tape: pdf = p**x * (1-p)**(1-x) logL = tf.reduce_sum(tnp.log(pdf)) slope = tape.gradient(logL,p) p.assign_add(slope* 0.1/10000) # N=10000 . p . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=0.796&gt; . (3) 아래의 모형에 따라서 $ {Y_i }_{i=1}^{10000}$를 생성하는 코드를 작성하라. . $Y_i overset{iid}{ sim} N( mu_i,1)$ | $ mu_i = beta_0 + beta_1 x_i = 0.5 + 2 x_i$ , where $x_i = frac{i}{10000}$ | . 함수 $L( beta_0, beta_1)$을 최대화하는 $( beta_0, beta_1)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ beta_0, beta_1$의 초기값은 모두 1로 설정할 것) . $$L( beta_0, beta_1)= prod_{i=1}^{n}f(y_i), quad f(y_i)= frac{1}{ sqrt{2 pi}}e^{- frac{1}{2}(y_i- mu_i)^2}, quad mu_i= beta_0+ beta_1 x_i$$ . (풀이) . x= tf.constant(np.arange(1,10001)/10000) y= tnp.random.randn(10000) + (0.5 + 2*x) . beta0= tf.Variable(1.0) beta1= tf.Variable(1.0) for i in range(2000): with tf.GradientTape() as tape: mu = beta0 + beta1*x pdf = tnp.exp(-0.5*(y-mu)**2) logL = tf.reduce_sum(tnp.log(pdf)) slope1, slope2 = tape.gradient(logL,[beta0,beta1]) beta0.assign_add(slope1* 0.1/10000) # N=10000 beta1.assign_add(slope2* 0.1/10000) . beta0, beta1 . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=0.5553082&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.8987025&gt;) . 2. &#54924;&#44480;&#48516;&#49437;&#51032; &#51060;&#47200;&#51201;&#54644;&#50752; tf.keras.optimizer &#51060;&#50857;&#48169;&#48277; (20&#51216;) . 아래와 같은 선형모형을 고려하자. . $$y_i = beta_0 + beta_1 x_i + epsilon_i.$$ . 이때 오차항은 정규분포로 가정한다. 즉 $ epsilon_i overset{iid}{ sim} N(0, sigma^2)$라고 가정한다. . 관측데이터가 아래와 같을때 아래의 물음에 답하라. . X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4], [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]]) y= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , 63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286]).reshape(10,1) . (1) MSE loss를 최소화 하는 $ beta_0, beta_1$의 해석해를 구하라. . (풀이) . tf.linalg.inv(X.T @ X ) @ X.T @ y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.94457323], [2.21570461]])&gt; . (2) 경사하강법과 MSE loss의 도함수를 이용하여 $ beta_0, beta_1$을 추정하라. . 주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것. . (풀이) . beta= tnp.array([5,10]).reshape(2,1) . for i in range(50000): beta = beta - 0.0015 * (-2*X.T @y + 2*X.T@X@beta)/10 . beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.28579424], [2.24168098]])&gt; . (3) tf.keras.optimizers의 apply_gradients()를 이용하여 $ beta_0, beta_1$을 추정하라. . (풀이) . beta = tf.Variable(tnp.array([5.0,10.0]).reshape(2,1)) opt = tf.optimizers.SGD(0.0015) for i in range(50000): with tf.GradientTape() as tape: loss = (y-X@beta).T @ (y-X@beta) / 10 slope = tape.gradient(loss,beta) opt.apply_gradients([(slope,beta)]) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[9.28579425], [2.24168098]])&gt; . (4) tf.keras.optimizers의 minimize()를 이용하여 $ beta_0, beta_1$을 추정하라. . (풀이) . beta = tf.Variable(tnp.array([5.0,10.0]).reshape(2,1)) opt = tf.optimizers.SGD(0.0015) loss_fn = lambda: (y-X@beta).T @ (y-X@beta) / 10 for i in range(50000): opt.minimize(loss_fn,beta) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[9.28579425], [2.24168098]])&gt; . 3. keras&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; (30&#51216;) . (1) 아래와 같은 모형을 고려하자. . $$y_i= beta_0 + sum_{k=1}^{5} beta_k cos(k t_i)+ epsilon_i, quad i=0,1, dots, 999$$ . 여기에서 $t_i= frac{2 pi i}{1000}$ 이다. 그리고 $ epsilon_i sim i.i.d~ N(0, sigma^2)$, 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자. . np.random.seed(43052) t= np.array(range(1000))* np.pi/1000 y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2 plt.plot(t,y,&#39;.&#39;,alpha=0.2) . [&lt;matplotlib.lines.Line2D at 0x7f14d0ac6e00&gt;] . tf.keras를 이용하여 $ beta_0, dots, beta_5$를 추정하라. ($ beta_0, dots, beta_5$의 참값은 각각 -2,3,1,0,0,0.5 이다) . (풀이) . y = y.reshape(1000,1) x1 = np.cos(t) x2 = np.cos(2*t) x3 = np.cos(3*t) x4 = np.cos(4*t) x5 = np.cos(5*t) X = tf.stack([x1,x2,x3,x4,x5],axis=1) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(loss=&#39;mse&#39;,optimizer=&#39;sgd&#39;) net.fit(X,y,batch_size=1000, epochs = 1000, verbose=0) . &lt;keras.callbacks.History at 0x7f14d0ac6b00&gt; . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(5, 1) dtype=float32, numpy= array([[ 3.0008891e+00], [ 1.0066563e+00], [ 1.8562324e-03], [-3.8704993e-03], [ 4.9712175e-01]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-2.0122595], dtype=float32)&gt;] . (2) 아래와 같은 모형을 고려하자. . $$y_i sim Ber( pi_i), ~ text{where} ~ pi_i= frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$$ . 위의 모형에서 관측한 데이터는 아래와 같다. . tf.random.set_seed(43052) x = tnp.linspace(-1,1,2000) y = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64) plt.plot(x,y,&#39;.&#39;,alpha=0.05) . [&lt;matplotlib.lines.Line2D at 0x7f14d0b01690&gt;] . tf.keras를 이용하여 $w_0,w_1$을 추정하라. (참고: $w_0, w_1$에 대한 참값은 -1과 5이다.) . (풀이) . x= x.reshape(2000,1) y= y.reshape(2000,1) . net= tf.keras.Sequential() net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) net.compile(optimizer=&#39;sgd&#39;, loss= tf.losses.binary_crossentropy) net.fit(x,y,epochs=10000,batch_size=2000, verbose=0) . &lt;keras.callbacks.History at 0x7f14b818d840&gt; . net.weights . [&lt;tf.Variable &#39;dense_2/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[4.201613]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_2/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-0.90172565], dtype=float32)&gt;] . plt.plot(y,&#39;.&#39;) plt.plot(net(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f14b3f06b30&gt;] . 4. Piecewise-linear regression (15&#51216;) . 아래의 모형을 고려하자. . model: $y_i= begin{cases} x_i +0.3 epsilon_i &amp; x leq 0 3.5x_i +0.3 epsilon_i &amp; x&gt;0 end{cases}$ . 아래는 위의 모형에서 생성한 샘플이다. . np.random.seed(43052) N=100 x= np.linspace(-1,1,N).reshape(N,1) y= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1) . (1) 다음은 $(x_i,y_i)$를 아래와 같은 아키텍처로 적합시키는 코드이다. . $ hat{y} = hat{ beta}_0+ hat{ beta}_1x $ | . tf.random.set_seed(43054) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감 . &lt;keras.callbacks.History at 0x7f14b3e7fb50&gt; . 케라스에 의해 추정된 $ hat{ beta}_0, hat{ beta}_1$을 구하라. . net.weights . [&lt;tf.Variable &#39;dense_6/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_6/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)&gt;] . (풀이) . $ hat{ beta}_0= 0.6069048$ | $ hat{ beta}_1= 2.2616348$ | . (2) 다음은 $(x_i,y_i)$를 아래와 같은 아키텍처로 적합시키는 코드이다. . $ boldsymbol{u}= x boldsymbol{W}^{(1)}+ boldsymbol{b}^{(1)}$ | $ boldsymbol{v}= text{relu}(u)$ | $yhat= boldsymbol{v} boldsymbol{W}^{(2)}+b^{(2)}$ | . tf.random.set_seed(43056) ## 1단계 net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(2)) net.add(tf.keras.layers.Activation(&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f14b3dc7490&gt; . ${ boldsymbol u}$를 이용하여 ${ boldsymbol v}$를 만드는 코드와 ${ boldsymbol v}$를 이용하여 $yhat$를 만드는 코드를 작성하라. . (풀이) . u=net.layers[0](x) v=net.layers[1](u) yhat=net.layers[2](v) . (3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라. . (곤이) (2) 모형은 활성화함수로 relu를 사용하였다. . (철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다. . (아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다. . (짝귀) (1) 의 모형은 오버피팅의 위험이 있다. . 5. &#45796;&#51020;&#51012; &#51096; &#51069;&#44256; &#52280;&#44284; &#44144;&#51667;&#51012; &#54032;&#45800;&#54616;&#46972;. (5&#51216;) . (1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제나 전역최소해를 찾을 수 있다. . (2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다. . (3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다. . (4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다. . (5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다. .",
            "url": "https://guebin.github.io/STBDA2022/2022/04/27/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC-%ED%95%B4%EC%84%A4.html",
            "relUrl": "/2022/04/27/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC-%ED%95%B4%EC%84%A4.html",
            "date": " • Apr 27, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "빅데이터분석 특강 중간고사",
            "content": "import numpy as np import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import matplotlib.pyplot as plt . 1. &#44221;&#49324;&#54616;&#44053;&#48277;&#44284; tf.GradientTape()&#51032; &#49324;&#50857;&#48169;&#48277; (30&#51216;) . (1) 아래는 $X_i overset{iid}{ sim} N(3,2^2)$ 를 생성하는 코드이다. . tf.random.set_seed(43052) x= tnp.random.randn(10000)*2+3 x . &lt;tf.Tensor: shape=(10000,), dtype=float64, numpy= array([ 4.12539849, 5.46696729, 5.27243374, ..., 2.89712332, 5.01072291, -1.13050477])&gt; . 함수 $L( mu, sigma)$을 최대화하는 $( mu, sigma)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ mu$의 초기값은 2로 $ sigma$의 초기값은 3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)= frac{1}{ sqrt{2 pi} sigma}e^{- frac{1}{2}( frac{x_i- mu}{ sigma})^2}$$ . (2) 아래는 $X_i overset{iid}{ sim} Ber(0.8)$을 생성하는 코드이다. . tf.random.set_seed(43052) x= tf.constant(np.random.binomial(1,0.8,(10000,))) x . &lt;tf.Tensor: shape=(10000,), dtype=int64, numpy=array([0, 0, 1, ..., 1, 1, 1])&gt; . 함수 $L(p)$을 최대화하는 $p$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $p$의 초기값은 0.3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)=p^{x_i}(1-p)^{1-x_i}$$ . (3) 아래의 모형에 따라서 $ {Y_i }_{i=1}^{10000}$를 생성하는 코드를 작성하라. . $Y_i overset{iid}{ sim} N( mu_i,1)$ | $ mu_i = beta_0 + beta_1 x_i = 0.5 + 2 x_i$ , where $x_i = frac{i}{10000}$ | . 함수 $L( beta_0, beta_1)$을 최대화하는 $( beta_0, beta_1)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ beta_0, beta_1$의 초기값은 모두 1로 설정할 것) . $$L( beta_0, beta_1)= prod_{i=1}^{n}f(y_i), quad f(y_i)= frac{1}{ sqrt{2 pi}}e^{- frac{1}{2}(y_i- mu_i)^2}, quad mu_i= beta_0+ beta_1 x_i$$ . 2. &#54924;&#44480;&#48516;&#49437;&#51032; &#51060;&#47200;&#51201;&#54644;&#50752; tf.keras.optimizer &#51060;&#50857;&#48169;&#48277; (20&#51216;) . 아래와 같은 선형모형을 고려하자. . $$y_i = beta_0 + beta_1 x_i + epsilon_i.$$ . 이때 오차항은 정규분포로 가정한다. 즉 $ epsilon_i overset{iid}{ sim} N(0, sigma^2)$라고 가정한다. . 관측데이터가 아래와 같을때 아래의 물음에 답하라. . x= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , 63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286]) # X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4], # [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]]) . (1) MSE loss를 최소화 하는 $ beta_0, beta_1$의 해석해를 구하라. . (2) 경사하강법과 MSE loss의 도함수를 이용하여 $ beta_0, beta_1$을 추정하라. . 주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것. . (3) tf.keras.optimizers의 apply_gradients()를 이용하여 $ beta_0, beta_1$을 추정하라. . (4) tf.keras.optimizers의 minimize()를 이용하여 $ beta_0, beta_1$을 추정하라. . 3. keras&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; (30&#51216;) . (1) 아래와 같은 모형을 고려하자. . $$y_i= beta_0 + sum_{k=1}^{5} beta_k cos(k t_i)+ epsilon_i, quad i=0,1, dots, 999$$ . 여기에서 $t_i= frac{2 pi i}{1000}$ 이다. 그리고 $ epsilon_i sim i.i.d~ N(0, sigma^2)$, 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자. . np.random.seed(43052) t= np.array(range(1000))* np.pi/1000 y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2 plt.plot(t,y,&#39;.&#39;,alpha=0.2) . [&lt;matplotlib.lines.Line2D at 0x7fceb6997340&gt;] . tf.keras를 이용하여 $ beta_0, dots, beta_5$를 추정하라. ($ beta_0, dots, beta_5$의 참값은 각각 -2,3,1,0,0,0.5 이다) . (2) 아래와 같은 모형을 고려하자. . $$y_i sim Ber( pi_i), ~ text{where} ~ pi_i= frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$$ . 위의 모형에서 관측한 데이터는 아래와 같다. . tf.random.set_seed(43052) x = tnp.linspace(-1,1,2000) y = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64) plt.plot(x,y,&#39;.&#39;,alpha=0.05) . [&lt;matplotlib.lines.Line2D at 0x7fceb9941ed0&gt;] . tf.keras를 이용하여 $w_0,w_1$을 추정하라. (참고: $w_0, w_1$에 대한 참값은 -1과 5이다.) . 4. Piecewise-linear regression (15&#51216;) . 아래의 모형을 고려하자. . model: $y_i= begin{cases} x_i +0.3 epsilon_i &amp; x leq 0 3.5x_i +0.3 epsilon_i &amp; x&gt;0 end{cases}$ . 아래는 위의 모형에서 생성한 샘플이다. . np.random.seed(43052) N=100 x= np.linspace(-1,1,N).reshape(N,1) y= np.array(list(map(lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3,x))).reshape(N,1) . (1) 다음은 $(x_i,y_i)$를 아래와 같은 아키텍처로 적합시키는 코드이다. . $ hat{y} = hat{ beta}_0+ hat{ beta}_1x $ | . tf.random.set_seed(43054) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감 . &lt;keras.callbacks.History at 0x7f6b142800d0&gt; . 케라스에 의해 추정된 $ hat{ beta}_0, hat{ beta}_1$을 구하라. . (2) 다음은 $(x_i,y_i)$를 아래와 같은 아키텍처로 적합시키는 코드이다. . $ boldsymbol{u}= x boldsymbol{W}^{(1)}+ boldsymbol{b}^{(1)}$ | $ boldsymbol{v}= text{relu}(u)$ | $y= boldsymbol{v} boldsymbol{W}^{(2)}+b^{(2)}$ | . tf.random.set_seed(43056) ## 1단계 net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(2)) net.add(tf.keras.layers.Activation(&#39;relu&#39;)) net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f6af6c39f30&gt; . ${ boldsymbol u}$를 이용하여 ${ boldsymbol v}$를 만드는 코드와 ${ boldsymbol v}$를 이용하여 $y$를 만드는 코드를 작성하라. . (3) 아래는 (1)-(2)번 모형에 대한 discussion이다. 올바른 것을 모두 골라라. . (곤이) (2) 모형은 활성화함수로 relu를 사용하였다. . (철용) (1) 모형에서 추정해야할 파라메터의 수는 2개이다. . (아귀) (2) 모형이 (1) 모형보다 복잡한 모형이다. . (짝귀) (1) 의 모형은 오버피팅의 위험이 있다. . 5. &#45796;&#51020;&#51012; &#51096; &#51069;&#44256; &#52280;&#44284; &#44144;&#51667;&#51012; &#54032;&#45800;&#54616;&#46972;. (5&#51216;) . (1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제 전역최소해를 찾을 수 있다. . (2) tf.GradeintTape()는 경사하강법을 이용하여 최적점을 찾아주는 tool이다. . (3) 학습률이 크다는 것은 파라메터는 1회 업데이트 하는 양이 크다는 것을 의미한다. . (4) 학습률이 크면 학습파라메터의 수렴속도가 빨라지지만 때때로 과적합에 빠질 수도 있다. . (5) 단순회귀분석에서 MSE loss를 최소화 하는 해는 경사하강법을 이용하지 않아도 해석적으로 구할 수 있다. .",
            "url": "https://guebin.github.io/STBDA2022/2022/04/25/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC.html",
            "relUrl": "/2022/04/25/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC.html",
            "date": " • Apr 25, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "중간고사 예상문제",
            "content": "import numpy as np import tensorflow as tf import tensorflow.experimental.numpy as tnp . import matplotlib.pyplot as plt . 1. &#44221;&#49324;&#54616;&#44053;&#48277;&#44284; tf.GradientTape()&#51032; &#49324;&#50857;&#48169;&#48277; (30&#51216;) . (1) 아래는 $X_i overset{iid}{ sim} N(3,2^2)$ 를 생성하는 코드이다. (10점) . tf.random.set_seed(43052) x= tnp.random.randn(10000)*2+3 x . &lt;tf.Tensor: shape=(10000,), dtype=float64, numpy= array([ 4.12539849, 5.46696729, 5.27243374, ..., 2.89712332, 5.01072291, -1.13050477])&gt; . 함수 $L( mu, sigma)$을 최대화하는 $( mu, sigma)$를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 $ mu$의 초기값은 2로 $ sigma$의 초기값은 3으로 설정할 것) . $$L( mu, sigma)= prod_{i=1}^{n}f(x_i), quad f(x_i)= frac{1}{ sqrt{2 pi} sigma}e^{- frac{1}{2}( frac{x_i- mu}{ sigma})^2}$$ . hint: $L( mu, sigma)$를 최대화하는 $( mu, sigma)$는 $ log L( mu, sigma)$를 역시 최대화한다는 사실을 이용할 것. . hint: $ mu$의 참값은 3, $ sigma$의 참값은 2이다. (따라서 $ mu$와 $ sigma$는 각각 2와 3근처로 추정되어야 한다.) . (2) . (3) . 2. &#54924;&#44480;&#48516;&#49437;&#51032; &#51060;&#47200;&#51201;&#54644;&#50752; tf.keras.optimizer &#51060;&#50857;&#48169;&#48277; (20&#51216;) . 아래와 같은 선형모형을 고려하자. . $$y_i = beta_0 + beta_1 x_i + epsilon_i.$$ . 이때 오차항은 정규분포로 가정한다. 즉 $ epsilon_i overset{iid}{ sim} N(0, sigma^2)$라고 가정한다. . 관측데이터가 아래와 같을때 아래의 물음에 답하라. . x= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , 63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286]) # X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4], # [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]]) . (1) MSE loss를 최소화 하는 $ beta_0, beta_1$의 해석해를 구하라. . (2) 경사하강법과 MSE loss의 도함수를 이용하여 $ beta_0, beta_1$을 추정하라. . 주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것. . (3) tf.keras.optimizers의 apply_gradients()를 이용하여 $ beta_0, beta_1$을 추정하라. . (4) tf.keras.optimizers의 minimize()를 이용하여 $ beta_0, beta_1$을 추정하라. . hint1 alpha=0.0015로 설정할 것 . hint2 epoc은 10000번정도 반복실행하며 적당한 횟수를 찾을 것 . hint3 (1)의 최적값에 반드시 정확히 수렴시킬 필요는 없음 (너무 많은 에폭이 소모됨) . hint4 초기값으로 [5,10] 정도 이용할 것 . 3. keras&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; (30&#51216;) . (1) 아래와 같은 모형을 고려하자. . $$y_i= beta_0 + sum_{k=1}^{5} beta_k cos(k t_i)+ epsilon_i, quad i=0,1, dots, 999$$ . 여기에서 $t_i= frac{2 pi i}{1000}$ 이다. 그리고 $ epsilon_i sim i.i.d~ N(0, sigma^2)$, 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자. . np.random.seed(43052) t= np.array(range(1000))* np.pi/1000 y = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2 plt.plot(t,y,&#39;.&#39;,alpha=0.2) . [&lt;matplotlib.lines.Line2D at 0x7fceb6997340&gt;] . tf.keras를 이용하여 $ beta_0, dots, beta_5$를 추정하라. ($ beta_0, dots, beta_5$의 참값은 각각 -2,3,1,0,0,0.5 이다) . (2) 아래와 같은 모형을 고려하자. . $$y_i sim Ber( pi_i), ~ text{where} ~ pi_i= frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$$ . 위의 모형에서 관측한 데이터는 아래와 같다. . tf.random.set_seed(43052) x = tnp.linspace(-1,1,2000) y = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64) plt.plot(x,y,&#39;.&#39;,alpha=0.05) . [&lt;matplotlib.lines.Line2D at 0x7fdf77a9a2f0&gt;] . tf.keras를 이용하여 $w_0,w_1$을 추정하라. (참고: $w_0, w_1$에 대한 참값은 -1과 5이다.) . 4. Piecewise-linear regression (15&#51216;) . 5. &#45796;&#51020;&#51012; &#51096; &#51069;&#44256; &#52280;&#44284; &#44144;&#51667;&#51012; &#54032;&#45800;&#54616;&#46972;. (5&#51216;) . (1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제 전역최소해를 찾을 수 있다. . (2) . (3) . (4) . (5) . some notes . - 용어를 모르겠는 분은 질문하시기 바랍니다. . - 풀다가 에러나는 코드 질문하면 에러 수정해드립니다. .",
            "url": "https://guebin.github.io/STBDA2022/2022/04/23/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC%EC%98%88%EC%83%81%EB%AC%B8%EC%A0%9C.html",
            "relUrl": "/2022/04/23/%EC%A4%91%EA%B0%84%EA%B3%A0%EC%82%AC%EC%98%88%EC%83%81%EB%AC%B8%EC%A0%9C.html",
            "date": " • Apr 23, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "시험관련 안내사항",
            "content": "&#49884;&#54744;&#50976;&#54805; . - 오픈북: 강의노트, 본인이 정리한 노트, 인터넷 검색 가능 . - 비대면: Zoom을 활용하여 응시 . &#49884;&#54744;&#49884;&#44036; . - 일시: LMS를 통해 공지한 날의 수업시간 . - 시험시간 중 처음 30분은 장비점검시간으로 활용함 (단, 모든 사람이 준비될 경우 30분을 기다리지 않고 시작) . 따라서 2시간 수업일 경우 실질적으로 문제를 풀고 답안을 제출할때 까지 쓸 수 있는 시간은 1시간30분입니다. | . &#49884;&#54744;&#49892; &#51077;&#51109; . - LMS $ to$ 강의대화 $ to$ Zoom 화상강의 바로 가기로 입장 . &#49884;&#54744;&#47928;&#51228; &#44277;&#44060;&#48169;&#49885; . - LMS에 주피터노트북 파일 업로드 + LMS 공지사항을 통하여 시험문제의 URL을 공개 . 주피터파일의 장점: URL보다 LMS의 주피터노트북이 더 빠르게 공개됨. | URL의 장점: 시험문제의 오류가 있을 경우 수정이후 URL에 반영됨. | . &#51228;&#52636; . - 답안지: LMS의 레포트 메뉴를 활용하여 답안지를 제출 (종료시간 이전에 미리 제출가능) . - 동영상: 시험시간동안 컴퓨터전체화면 녹화후 제출 . 듀얼모니터의 경우 작업표시줄이 나타나는 모니터를 녹화 | 아이패드와 동시사용시에는 아이패드도 함께 녹화 | . &#51456;&#48708;&#47932; . - 컴퓨터 및 노트북: 시험지 확인 및 문제풀이 용도 . - 핸드폰: Zoom을 통하여 주변상황을 및 컴퓨터 화면을 촬영하는 용도 . 중간에 핸드폰 및 노트북이 꺼지지 않도록 충전기를 준비한다. | . - 학생증 혹은 신분증 (본인확인용도) . &#49884;&#54744;&#51204; &#51456;&#48708;&#49324;&#54637; . - 시험준비시간 동안 핸드폰을 아래와 같이 배치하여 학생의 컴퓨터 화면 및 주변상황이 보이도록 함 . . 적절한 각도를 설정하기 어려운 경우 주변환경보다 컴퓨터의 화면이 잘 보이도록 설정할 것 | . - 학생증을 준비하여 시험 시작 직전에 본인의 얼굴과 학생증을 함께 촬영한다. (5초간) . &#50976;&#51032;&#49324;&#54637; . - 줌의 대화명은 이름과 학번을 모두 적는다. (예시: 최규빈_202143052) . 동명이인이 있을 수 있으므로 학번을 같이 적으세요 | . - 질문은 카카오톡 채널 혹은 줌의 채팅기능을 이용한다. . - Zoom에서 스피커 음소거를 하지 않는다. (전체 공지사항등이 있을때 음성으로 공지함) . - 핸드폰으로 Zoom참가 중 전화가 오면 거절하고 받지 않는다. (전화통화시 Zoom연결이 종료되므로 부정행위로 의심할 수 있음) . &#44592;&#53440; &#52280;&#44256;&#49324;&#54637; . - 핸드폰과 피씨를 이용하여 줌에 동시접속할 경우 . 최규빈_202143052_핸드폰 | 최규빈_202143052_컴퓨터 | . 와 같이 기기를 분리하여 적는다. . - 시험문제는 코랩으로 풀어도 무방하며 시험문제를 다운받아 개인 주피터노트북 등으로 풀어도 무방하다. . - 답안지 제출형식은 주피터 노트북파일을 권장한다. 하지만 풀이 및 코드를 알아볼 수 있는 어떠한 형식으로 제출하여도 무방하다. (ex: txt, hwp, pdf, html..) .",
            "url": "https://guebin.github.io/STBDA2022/2022/04/19/%EC%8B%9C%ED%97%98%EA%B4%80%EB%A0%A8-%EC%95%88%EB%82%B4%EC%82%AC%ED%95%AD.html",
            "relUrl": "/2022/04/19/%EC%8B%9C%ED%97%98%EA%B4%80%EB%A0%A8-%EC%95%88%EB%82%B4%EC%82%AC%ED%95%AD.html",
            "date": " • Apr 19, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "(7주차) 4월18일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . piece-wise linear regression . model: $y_i= begin{cases} x_i +0.3 epsilon_i &amp; x leq 0 3.5x_i +0.3 epsilon_i &amp; x&gt;0 end{cases}$ . np.random.seed(43052) N=100 x = np.linspace(-1,1,N) lamb = lambda x: x*1+np.random.normal()*0.3 if x&lt;0 else x*3.5+np.random.normal()*0.3 y= np.array(list(map(lamb,x))) y . array([-0.88497385, -0.65454563, -0.61676249, -0.84702584, -0.84785569, -0.79220455, -1.3777105 , -1.27341781, -1.41643729, -1.26404671, -0.79590224, -0.78824395, -0.86064773, -0.52468679, -1.18247354, -0.29327295, -0.69373049, -0.90561768, -1.07554911, -0.7225404 , -0.69867774, -0.34811037, 0.11188474, -1.05046296, -0.03840085, -0.38356861, -0.24299798, -0.58403161, -0.20344022, -0.13872303, -0.529586 , -0.27814478, -0.10852781, -0.38294596, 0.02669763, -0.23042603, -0.77720364, -0.34287396, -0.04512022, -0.30180793, -0.26711438, -0.51880349, -0.53939672, -0.32052379, -0.32080763, 0.28917092, 0.18175206, -0.48988124, -0.08084459, 0.37706178, 0.14478908, 0.07621827, -0.071864 , 0.05143365, 0.33932009, -0.35071776, 0.87742867, 0.51370399, 0.34863976, 0.55855514, 1.14196717, 0.86421076, 0.72957843, 0.57342304, 1.54803332, 0.98840018, 1.11129366, 1.42410801, 1.44322465, 1.25926455, 1.12940772, 1.46516829, 1.16365096, 1.45560853, 1.9530553 , 2.45940445, 1.52921129, 1.8606463 , 1.86406718, 1.5866523 , 1.49033473, 2.35242686, 2.12246412, 2.41951931, 2.43615052, 1.96024441, 2.65843789, 2.46854394, 2.76381882, 2.78547462, 2.56568465, 3.15212157, 3.11482949, 3.17901774, 3.31268904, 3.60977818, 3.40949166, 3.30306495, 3.74590922, 3.85610433]) . plt.plot(x,y,&#39;.&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3b7f1187f0&gt;] . &#54400;&#51060;1: &#45800;&#49692;&#54924;&#44480;&#47784;&#54805; . x= x.reshape(N,1) y= y.reshape(N,1) . net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1)) net.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) net.fit(x,y,batch_size=N,epochs=1000,verbose=0) # numpy로 해도 돌아감 . 2022-04-18 11:40:03.840482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . &lt;keras.callbacks.History at 0x7f3b7d0bf670&gt; . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[2.2616348]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.6069048], dtype=float32)&gt;] . yhat = x * 2.2616348 + 0.6069048 yhat = net.predict(x) . plt.plot(x,y,&#39;.&#39;) plt.plot(x,yhat,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3b7f163370&gt;] . - 실패: 이 모형은 epoch을 10억번 돌려도 실패할 모형임 . 왜? 아키텍처 설계자체가 틀렸음 | 꺽인부분을 표현하기에는 아키텍처의 표현력이 너무 부족하다 -&gt; under fit의 문제 | . &#54400;&#51060;2: &#48708;&#49440;&#54805; &#54876;&#49457;&#54868; &#54632;&#49688;&#51032; &#46020;&#51077; . - 여기에서 비선형 활성화 함수는 relu . - 네트워크를 아래와 같이 수정하자. . (수정전) hat은 생략 . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*w, bias=True&quot;[label=&quot;*w&quot;] ; &quot;x*w, bias=True&quot; -&gt; &quot;y&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*w, &#160;&#160;&#160;bias=True x*w, &#160;&#160;&#160;bias=True x&#45;&gt;x*w, &#160;&#160;&#160;bias=True *w y y x*w, &#160;&#160;&#160;bias=True&#45;&gt;y indentity (수정후) hat은 생략 . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*w, bias=True&quot;[label=&quot;*w&quot;] ; &quot;x*w, bias=True&quot; -&gt; &quot;y&quot;[label=&quot;relu&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*w, &#160;&#160;&#160;bias=True x*w, &#160;&#160;&#160;bias=True x&#45;&gt;x*w, &#160;&#160;&#160;bias=True *w y y x*w, &#160;&#160;&#160;bias=True&#45;&gt;y relu 마지막에 $f(x)=x$ 라는 함수대신에 relu를 취하는 것으로 구조를 약간 변경 | 활성화함수(acitivation function)를 indentity에서 relu로 변경 | . - relu함수란? . _x = np.linspace(-1,1,100) tf.nn.relu(_x) . &lt;tf.Tensor: shape=(100,), dtype=float64, numpy= array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01010101, 0.03030303, 0.05050505, 0.07070707, 0.09090909, 0.11111111, 0.13131313, 0.15151515, 0.17171717, 0.19191919, 0.21212121, 0.23232323, 0.25252525, 0.27272727, 0.29292929, 0.31313131, 0.33333333, 0.35353535, 0.37373737, 0.39393939, 0.41414141, 0.43434343, 0.45454545, 0.47474747, 0.49494949, 0.51515152, 0.53535354, 0.55555556, 0.57575758, 0.5959596 , 0.61616162, 0.63636364, 0.65656566, 0.67676768, 0.6969697 , 0.71717172, 0.73737374, 0.75757576, 0.77777778, 0.7979798 , 0.81818182, 0.83838384, 0.85858586, 0.87878788, 0.8989899 , 0.91919192, 0.93939394, 0.95959596, 0.97979798, 1. ])&gt; . plt.plot(_x,_x) plt.plot(_x,tf.nn.relu(_x)) . [&lt;matplotlib.lines.Line2D at 0x7f3b7c1ab610&gt;] . 파란색을 주황색으로 바꿔주는 것이 렐루함수임 | $f(x)= max(0,x)= begin{cases} 0 &amp; x leq 0 x &amp; x&gt;0 end{cases}$ | . - 아키텍처: $ hat{y}_i=relu( hat{w}_0+ hat{w}_1x_i)$, $relu(x)= max(0,x)$ . - 풀이시작 . 1단계 . net2 = tf.keras.Sequential() . 2단계 . tf.random.set_seed(43053) l1 = tf.keras.layers.Dense(1, input_shape=(1,)) a1 = tf.keras.layers.Activation(tf.nn.relu) . net2.add(l1) . net2.layers . [&lt;keras.layers.core.dense.Dense at 0x7f3b5c6e74f0&gt;] . net2.add(a1) . net2.layers . [&lt;keras.layers.core.dense.Dense at 0x7f3b5c6e74f0&gt;, &lt;keras.layers.core.activation.Activation at 0x7f3aa99a0ee0&gt;] . l1.get_weights() . [array([[0.41721308]], dtype=float32), array([0.], dtype=float32)] . net2.get_weights() . [array([[0.41721308]], dtype=float32), array([0.], dtype=float32)] . (네트워크 상황 확인) . u1= l1(x) #u1= x@l1.weights[0] + l1.weights[1] . v1= a1(u1) #v1= tf.nn.relu(u1) . plt.plot(x,x) plt.plot(x,u1,&#39;--r&#39;) plt.plot(x,v1,&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3aa98243a0&gt;] . 3단계 . net2.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) . 4단계 . net2.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f3aa9885990&gt; . - result . yhat = tf.nn.relu(x@l1.weights[0] + l1.weights[1]) yhat = net2.predict(x) yhat = net2(x) yhat = a1(l1(x)) yhat = net2.layers[1](net2.layers[0](x)) . plt.plot(x,y,&#39;.&#39;) plt.plot(x,yhat,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3aa636bee0&gt;] . - discussion . 이것 역시 수백억번 에폭을 반복해도 이 이상 적합이 힘들다 $ to$ 모형의 표현력이 떨어진다. | 해결책: 주황색점선이 2개 있다면 어떨까? | . &#54400;&#51060;3: &#45432;&#46300;&#49688;&#52628;&#44032; + &#47112;&#51060;&#50612;&#52628;&#44032; . 목표: 2개의 주황색 점선을 만들자. . 1단계 . net3 = tf.keras.Sequential() . 2단계 . tf.random.set_seed(43053) l1 = tf.keras.layers.Dense(2,input_shape=(1,)) a1 = tf.keras.layers.Activation(tf.nn.relu) . net3.add(l1) net3.add(a1) . (네트워크 상황 확인) . l1(x).shape # l1(x) : (100,1) -&gt; (100,2) . TensorShape([100, 2]) . plt.plot(x,x) plt.plot(x,l1(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a9c5f7730&gt;, &lt;matplotlib.lines.Line2D at 0x7f3a9c5f7970&gt;] . plt.plot(x,x) plt.plot(x,a1(l1(x)),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a9c48cf70&gt;, &lt;matplotlib.lines.Line2D at 0x7f3a9c48d1b0&gt;] . - 이 상태에서는 yhat이 안나온다. 왜? . 차원이 안맞음. a1(l1(x))의 차원은 (N,2)인데 최종적인 yhat의 차원은 (N,1)이어야 함. | 차원이 어찌저찌 맞다고 쳐도 relu를 통과하면 항상 yhat&gt;0 임. 따라서 음수값을 가지는 y는 0으로 밖에 맞출 수 없음. | . - 해결책: a1(l1(x))에 연속으로(Sequential하게!) 또 다른 레이어를 설계! (N,2) -&gt; (N,1) 이 되도록! . yhat= bias + weight1 * a1(l1(x))[0] + weight2 * a1(l1(x))[1] | . - 즉 a1(l1(x)) 를 새로운 입력으로 해석하고 출력을 만들어주는 선형모형을 다시태우면 된다. . 입력차원: 2 | 출력차원: 1 | . net3.layers . [&lt;keras.layers.core.dense.Dense at 0x7f3aa62bb3d0&gt;, &lt;keras.layers.core.activation.Activation at 0x7f3aa62baad0&gt;] . tf.random.set_seed(43053) l2 = tf.keras.layers.Dense(1, input_shape=(2,)) . net3.add(l2) . net3.layers . [&lt;keras.layers.core.dense.Dense at 0x7f3aa62bb3d0&gt;, &lt;keras.layers.core.activation.Activation at 0x7f3aa62baad0&gt;, &lt;keras.layers.core.dense.Dense at 0x7f3aa61c3160&gt;] . net3.summary() . Model: &#34;sequential_8&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_10 (Dense) (None, 2) 4 activation_9 (Activation) (None, 2) 0 dense_11 (Dense) (None, 1) 3 ================================================================= Total params: 7 Trainable params: 7 Non-trainable params: 0 _________________________________________________________________ . - 추정해야할 파라메터수가 4,0,3으로 나온다. . - 수식표현: $X to X@W^{(1)}+b^{(1)} to relu(X@W^{(1)}+b^{(1)}) to relu(X@W^{(1)}+b^{(1)})@W^{(2)}+b^{(2)}=yhat$ . $X$: (N,1) | $W^{(1)}$: (1,2) ==&gt; 파라메터 2개 추정 | $b^{(1)}$: (2,) ==&gt; 파라메터 2개가 추가 // 여기까지 추정할 파라메터는 4개 | $W^{(2)}$: (2,1) ==&gt; 파라메터 2개 추정 | $b^{(2)}$: (1,) ==&gt; 파라메터 1개가 추가 // 따라서 3개 | . - 참고: 추정할 파라메터수가 많다 = 복잡한 모형이다. . 초거대AI: 추정할 파라메터수가 엄청 많은.. | . net3.weights . [&lt;tf.Variable &#39;dense_10/kernel:0&#39; shape=(1, 2) dtype=float32, numpy=array([[ 0.34065306, -0.7533803 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_10/bias:0&#39; shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_11/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[ 0.34065306], [-0.7533803 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_11/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;] . l1.weights . [&lt;tf.Variable &#39;dense_10/kernel:0&#39; shape=(1, 2) dtype=float32, numpy=array([[ 0.34065306, -0.7533803 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_10/bias:0&#39; shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)&gt;] . l2.weights . [&lt;tf.Variable &#39;dense_11/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[ 0.34065306], [-0.7533803 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_11/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;] . - 좀 더 간단한 수식표현: $X to (u_1 to v_1) to (u_2 to v_2) = yhat$ . $u_1= X@W^{(1)}+b^{(1)}$ | $v_1= relu(u_1)$ | $u_2= v_1@W^{(2)}+b^{(2)}$ | $v_2= indentity(u_2):=yhat$ | . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;u1[:,0]&quot;[label=&quot;*W1[0,0]&quot;] &quot;X&quot; -&gt; &quot;u1[:,1]&quot;[label=&quot;*W1[0,1]&quot;] &quot;u1[:,0]&quot; -&gt; &quot;v1[:,0]&quot;[label=&quot;relu&quot;] &quot;u1[:,1]&quot; -&gt; &quot;v1[:,1]&quot;[label=&quot;relu&quot;] label = &quot;Layer 1&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;v1[:,0]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[0,0]&quot;] &quot;v1[:,1]&quot; -&gt; &quot;yhat&quot;[label=&quot;*W2[1,0]&quot;] label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 cluster_3 Layer 2 X X u1[:,0] u1[:,0] X&#45;&gt;u1[:,0] *W1[0,0] u1[:,1] u1[:,1] X&#45;&gt;u1[:,1] *W1[0,1] v1[:,0] v1[:,0] u1[:,0]&#45;&gt;v1[:,0] relu v1[:,1] v1[:,1] u1[:,1]&#45;&gt;v1[:,1] relu yhat yhat v1[:,0]&#45;&gt;yhat *W2[0,0] v1[:,1]&#45;&gt;yhat *W2[1,0] gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;node1&quot; &quot;X&quot; -&gt; &quot;node2&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat&quot; &quot;node2&quot; -&gt; &quot;yhat&quot; label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: relu cluster_3 Layer 2 X X node1 node1 X&#45;&gt;node1 node2 node2 X&#45;&gt;node2 yhat yhat node1&#45;&gt;yhat node2&#45;&gt;yhat 3단계 . net3.compile(loss=&#39;mse&#39;,optimizer=tf.optimizers.SGD(0.1)) . 4단계 . net3.fit(x,y,epochs=1000,verbose=0, batch_size=N) . &lt;keras.callbacks.History at 0x7f3aa61ba560&gt; . - 결과확인 . net3.weights . [&lt;tf.Variable &#39;dense_10/kernel:0&#39; shape=(1, 2) dtype=float32, numpy=array([[ 1.6352799 , -0.85507524]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_10/bias:0&#39; shape=(2,) dtype=float32, numpy=array([-0.08284465, 0.85552216], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_11/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[ 1.6328746], [-1.2001747]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_11/bias:0&#39; shape=(1,) dtype=float32, numpy=array([1.0253307], dtype=float32)&gt;] . plt.plot(x,y,&#39;.&#39;) plt.plot(x,net3(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a9c384040&gt;] . - 분석 . plt.plot(x,y,&#39;.&#39;) plt.plot(x,l1(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a9c2bdde0&gt;, &lt;matplotlib.lines.Line2D at 0x7f3a9c2be020&gt;] . plt.plot(x,y,&#39;.&#39;) plt.plot(x,a1(l1(x)),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a9c131870&gt;, &lt;matplotlib.lines.Line2D at 0x7f3a9c131ab0&gt;] . plt.plot(x,y,&#39;.&#39;) plt.plot(x,l2(a1(l1(x))),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a9c1ad480&gt;] . - 마지막 2개의 그림을 분석 . l2.weights . [&lt;tf.Variable &#39;dense_11/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[ 1.6328746], [-1.2001747]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_11/bias:0&#39; shape=(1,) dtype=float32, numpy=array([1.0253307], dtype=float32)&gt;] . fig, (ax1,ax2,ax3) = plt.subplots(1,3) fig.set_figwidth(12) ax1.plot(x,y,&#39;.&#39;) ax1.plot(x,a1(l1(x))[:,0],&#39;--r&#39;) ax1.plot(x,a1(l1(x))[:,1],&#39;--b&#39;) ax2.plot(x,y,&#39;.&#39;) ax2.plot(x,a1(l1(x))[:,0]*1.6328746,&#39;--r&#39;) ax2.plot(x,a1(l1(x))[:,1]*(-1.2001747)+1.0253307,&#39;--b&#39;) ax3.plot(x,y,&#39;.&#39;) ax3.plot(x,a1(l1(x))[:,0]*1.6328746+a1(l1(x))[:,1]*(-1.2001747)+1.0253307,&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a71028c40&gt;] . &#54400;&#51060;3&#51032; &#49892;&#54056; . tf.random.set_seed(43054) ## 1단계 net3 = tf.keras.Sequential() ## 2단계 net3.add(tf.keras.layers.Dense(2)) net3.add(tf.keras.layers.Activation(&#39;relu&#39;)) net3.add(tf.keras.layers.Dense(1)) ## 3단계 net3.compile(optimizer=tf.optimizers.SGD(0.1),loss=&#39;mse&#39;) ## 4단계 net3.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f3a70c237c0&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,net3(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a7032f700&gt;] . - 엥? 에폭이 부족한가? . net3.fit(x,y,epochs=10000,verbose=0,batch_size=N) plt.plot(x,y,&#39;.&#39;) plt.plot(x,net3(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a70278160&gt;] . - 실패분석 . l1,a1,l2 = net3.layers . l2.weights . [&lt;tf.Variable &#39;dense_13/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[0.65121335], [1.8592643 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_13/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)&gt;] . fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4) fig.set_figwidth(16) ax1.plot(x,y,&#39;.&#39;) ax1.plot(x,l1(x)[:,0],&#39;--r&#39;) ax1.plot(x,l1(x)[:,1],&#39;--b&#39;) ax2.plot(x,y,&#39;.&#39;) ax2.plot(x,a1(l1(x))[:,0],&#39;--r&#39;) ax2.plot(x,a1(l1(x))[:,1],&#39;--b&#39;) ax3.plot(x,y,&#39;.&#39;) ax3.plot(x,a1(l1(x))[:,0]*0.65121335,&#39;--r&#39;) ax3.plot(x,a1(l1(x))[:,1]*(1.8592643)+(-0.60076195),&#39;--b&#39;) ax4.plot(x,y,&#39;.&#39;) ax4.plot(x,a1(l1(x))[:,0]*0.65121335+a1(l1(x))[:,1]*(1.8592643)+(-0.60076195),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f3a7081c130&gt;] . 보니까 빨간색선이 하는 역할을 없음 | 그런데 생각해보니까 이 상황에서는 빨간색선이 할수 있는 일이 별로 없음 | 왜? 지금은 나름 파란색선에 의해서 최적화가 된 상태임 $ to$ 빨간선이 뭔가 하려고하면 최적화된 상태가 깨질 수 있음 (loss 증가) | 즉 이 상황 자체가 나름 최적회된 상태이다. 이러한 현상을 &quot;global minimum을 찾지 못하고 local minimum에 빠졌다&quot;라고 표현한다. | . 확인: . net3.weights . [&lt;tf.Variable &#39;dense_12/kernel:0&#39; shape=(1, 2) dtype=float32, numpy=array([[-0.03077251, 1.8713338 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_12/bias:0&#39; shape=(2,) dtype=float32, numpy=array([-0.04834982, 0.3259186 ], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_13/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[0.65121335], [1.8592643 ]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_13/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-0.60076195], dtype=float32)&gt;] . W1= tf.Variable(tnp.array([[-0.03077251, 1.8713338 ]])) b1= tf.Variable(tnp.array([-0.04834982, 0.3259186 ])) W2= tf.Variable(tnp.array([[0.65121335],[1.8592643 ]])) b2= tf.Variable(tnp.array([-0.60076195])) . with tf.GradientTape() as tape: u = tf.constant(x) @ W1 + b1 v = tf.nn.relu(u) yhat = v@W2 + b2 loss = tf.losses.mse(y,yhat) . tape.gradient(loss,[W1,b1,W2,b2]) . [&lt;tf.Tensor: shape=(1, 2), dtype=float64, numpy=array([[ 0.00000000e+00, -4.77330119e-05]])&gt;, &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([0.0000000e+00, 3.1478608e-06])&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ 0.00000000e+00], [-4.74910706e-05]])&gt;, &lt;tf.Tensor: shape=(1,), dtype=float64, numpy=array([-2.43031263e-05])&gt;] . 예상대로 계수값이 거의 다 0이다. . &#54400;&#51060;4: &#45432;&#46300;&#49688;&#47484; &#45908; &#52628;&#44032;&#54620;&#45796;&#47732;? . - 노드수를 더 추가해보면 어떻게 될까? (주황색 점선이 더 여러개 있다면?) . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;X&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;X&quot; -&gt; &quot;node1&quot; &quot;X&quot; -&gt; &quot;node2&quot; &quot;X&quot; -&gt; &quot;...&quot; &quot;X&quot; -&gt; &quot;node512&quot; label = &quot;Layer 1: relu&quot; } subgraph cluster_3{ style=filled; color=lightgrey; &quot;node1&quot; -&gt; &quot;yhat&quot; &quot;node2&quot; -&gt; &quot;yhat&quot; &quot;...&quot; -&gt; &quot;yhat&quot; &quot;node512&quot; -&gt; &quot;yhat&quot; label = &quot;Layer 2&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: relu cluster_3 Layer 2 X X node1 node1 X&#45;&gt;node1 node2 node2 X&#45;&gt;node2 ... ... X&#45;&gt;... node512 node512 X&#45;&gt;node512 yhat yhat node1&#45;&gt;yhat node2&#45;&gt;yhat ...&#45;&gt;yhat node512&#45;&gt;yhat tf.random.set_seed(43056) net4= tf.keras.Sequential() net4.add(tf.keras.layers.Dense(512,activation=&#39;relu&#39;)) # 이렇게 해도됩니다. net4.add(tf.keras.layers.Dense(1)) net4.compile(loss=&#39;mse&#39;,optimizer=tf.optimizers.SGD(0.1)) net4.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f34ac815d20&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,net4(x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f34ac6d26b0&gt;] . 잘된다.. | 한두개의 노드가 역할을 못해도 다른노드들이 잘 보완해주는듯! | . - 노드수가 많으면 무조건 좋다? -&gt; 대부분 나쁘지 않음. 그런데 종종 맞추지 말아야할것도 맞춤.. (overfit) . np.random.seed(43052) N=100 _x = np.linspace(0,1,N).reshape(N,1) _y = np.random.normal(loc=0,scale=0.001,size=(N,1)) plt.plot(_x,_y) . [&lt;matplotlib.lines.Line2D at 0x7f34ac4202b0&gt;] . tf.random.set_seed(43052) net4 = tf.keras.Sequential() net4.add(tf.keras.layers.Dense(512,activation=&#39;relu&#39;)) net4.add(tf.keras.layers.Dense(1)) net4.compile(loss=&#39;mse&#39;,optimizer=tf.optimizers.SGD(0.5)) net4.fit(_x,_y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f34ac2f3640&gt; . plt.plot(_x,_y) plt.plot(_x,net4(_x),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f34ac07a1a0&gt;] . 이 예제는 추후 다시 공부할 예정 | . Logistic regression . motive . - 현실에서 이런 경우가 많음 . $x$가 커질수록 (혹은 작아질수록) 성공확률이 올라간다. | . - 이러한 모형은 아래와 같이 설계할 수 있음 &lt;-- 외우세요!! . $y_i sim Ber( pi_i)$, where $ pi_i= frac{ exp(w_0+w_1x_i)}{1+ exp(w_0+w_1x_i)}$ . | $ hat{y}_i = frac{ exp( hat{w}_0+ hat{w}_1x_i)}{1+ exp( hat{w}_0+ hat{w}_1x_i)}= frac{1}{1+exp(- hat{w}_0- hat{w}_1x_i)}$ . | $loss=- frac{1}{n} sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ . | . - 위와 같은 손실함수를 BCEloss라고 부른다. (BCE는 Binary Cross Entropy의 약자) . &#50696;&#51228; . N = 2000 . x = tnp.linspace(-1,1,N).reshape(N,1) w0 = -1 w1 = 5 u = w0 + x*w1 #v = tf.constant(np.exp(u)/(1+np.exp(u))) # v=πi v = tf.nn.sigmoid(u) y = tf.constant(np.random.binomial(1,v),dtype=tf.float64) . plt.plot(x,y,&#39;.&#39;,alpha=0.02) plt.plot(x,v,&#39;--r&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f349e22fa60&gt;] . - 이 아키텍처(yhat을 얻어내는 과정)를 다어어그램으로 나타내면 아래와 같다. . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; &quot;x&quot; label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; &quot;x&quot; -&gt; &quot;x*w, bias=True&quot;[label=&quot;*w&quot;] &quot;x*w, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;sigmoid&quot;] label = &quot;Layer 1&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1 x x x*w, bias=True x*w, bias=True x&#45;&gt;x*w, bias=True *w yhat yhat x*w, bias=True&#45;&gt;yhat sigmoid - 또는 간단하게 아래와 같이 쓸 수 있다. . gv(&#39;&#39;&#39; subgraph cluster_1{ style=filled; color=lightgrey; x label = &quot;Layer 0&quot; } subgraph cluster_2{ style=filled; color=lightgrey; x -&gt; &quot;node1=yhat&quot; label = &quot;Layer 1: sigmoid&quot; } &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G cluster_1 Layer 0 cluster_2 Layer 1: sigmoid x x node1=yhat node1=yhat x&#45;&gt;node1=yhat - 케라스를 이용하여 적합을 해보면 . $loss=- frac{1}{n} sum_{i=1}^{n} big(y_i log( hat{y}_i)+(1-y_i) log(1- hat{y}_i) big)$ | . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) bceloss_fn = lambda y,yhat: -tf.reduce_mean(y*tnp.log(yhat) + (1-y)*tnp.log(1-yhat)) net.compile(loss=bceloss_fn, optimizer=tf.optimizers.SGD(0.1)) net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f349c638670&gt; . net.weights . [&lt;tf.Variable &#39;dense_28/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[4.1423755]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_28/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-0.820938], dtype=float32)&gt;] . plt.plot(x,y,&#39;.&#39;,alpha=0.1) plt.plot(x,v,&#39;--r&#39;) plt.plot(x,net(x),&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f349df177c0&gt;] . MSE loss? . - mse loss를 쓰면 왜 안되는지? . tf.random.set_seed(43052) net = tf.keras.Sequential() net.add(tf.keras.layers.Dense(1,activation=&#39;sigmoid&#39;)) mseloss_fn = lambda y,yhat: tf.reduce_mean((y-yhat)**2) net.compile(loss=mseloss_fn, optimizer=tf.optimizers.SGD(0.1)) net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f349df8cc10&gt; . plt.plot(x,y,&#39;.&#39;,alpha=0.1) plt.plot(x,v,&#39;--r&#39;) plt.plot(x,net(x),&#39;--b&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f349ce42230&gt;] . 일단 BCE loss와 비교해보니까 동일 초기값, 동일 epochs에서 적합이 별로임 | . MSE loss vs BCE loss . - MSEloss, BCEloss의 시각화 . w0, w1 = np.meshgrid(np.arange(-10,3,0.2), np.arange(-1,10,0.2), indexing=&#39;ij&#39;) w0, w1 = w0.reshape(-1), w1.reshape(-1) def loss_fn1(w0,w1): u = w0+w1*x yhat = np.exp(u)/(np.exp(u)+1) return mseloss_fn(y,yhat) def loss_fn2(w0,w1): u = w0+w1*x yhat = np.exp(u)/(np.exp(u)+1) return bceloss_fn(y,yhat) loss1 = list(map(loss_fn1,w0,w1)) loss2 = list(map(loss_fn2,w0,w1)) . fig = plt.figure() fig.set_figwidth(9) fig.set_figheight(9) ax1=fig.add_subplot(1,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(1,2,2,projection=&#39;3d&#39;) ax1.elev=15 ax2.elev=15 ax1.azim=75 ax2.azim=75 ax1.scatter(w0,w1,loss1,s=0.1) ax2.scatter(w0,w1,loss2,s=0.1) . &lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f34a457df00&gt; . 왼쪽곡면(MSEloss)보다 오른쪽곡면(BCEloss)이 좀더 예쁘게 생김 -&gt; 오른쪽 곡면에서 더 학습이 잘될것 같음 | . &#54617;&#49845;&#44284;&#51221; &#49884;&#44033;&#54868;&#50696;&#49884;1 . - 파라메터학습과정 시각화 // 옵티마이저: SGD, 초기값: (w0,w1) = (-3.0,-1.0) . (1) 데이터정리 . X = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1) X . &lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy= array([[ 1. , -1. ], [ 1. , -0.9989995], [ 1. , -0.997999 ], ..., [ 1. , 0.997999 ], [ 1. , 0.9989995], [ 1. , 1. ]])&gt; . (2) 1ter돌려봄 . net_mse = tf.keras.Sequential() net_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_mse.compile(optimizer=tf.optimizers.SGD(0.1),loss=mseloss_fn) net_mse.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 66ms/step - loss: 0.1554 . &lt;keras.callbacks.History at 0x7f349dad3760&gt; . net_bce = tf.keras.Sequential() net_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_bce.compile(optimizer=tf.optimizers.SGD(0.1),loss=bceloss_fn) net_bce.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 76ms/step - loss: 0.9265 . &lt;keras.callbacks.History at 0x7f349da22fe0&gt; . net_mse.get_weights(), net_bce.get_weights() . ([array([[-0.5575908], [ 1.1560522]], dtype=float32)], [array([[-0.8477989 ], [-0.91781974]], dtype=float32)]) . net_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)]) net_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)]) . net_mse.get_weights(), net_bce.get_weights() . ([array([[-3.], [-1.]], dtype=float32)], [array([[-3.], [-1.]], dtype=float32)]) . (4) 학습과정기록: 15에폭마다 기록 . What_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32) What_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32) . for k in range(29): net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0) net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0) What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) . (5) 시각화 . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure() fig.set_figwidth(6) fig.set_figheight(6) fig.suptitle(&quot;SGD, Winit=(-3,-1)&quot;) ax1=fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75 ax3=fig.add_subplot(2,2,3) ax4=fig.add_subplot(2,2,4) ax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax3.plot(x,y,&#39;,&#39;); ax3.plot(x,v,&#39;--r&#39;); line3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),&#39;--b&#39;) ax4.plot(x,y,&#39;,&#39;); ax4.plot(x,v,&#39;--r&#39;) line4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),&#39;--b&#39;) def animate(i): _w0_mse,_w1_mse = What_mse[:,i] _w0_bce,_w1_bce = What_bce[:,i] ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color=&#39;gray&#39;) ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color=&#39;gray&#39;) line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i]))) line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i]))) ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#54617;&#49845;&#44284;&#51221; &#49884;&#44033;&#54868;&#50696;&#49884;2 . - 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-3.0,-1.0) . (1) 데이터정리 . X = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1) X . &lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy= array([[ 1. , -1. ], [ 1. , -0.9989995], [ 1. , -0.997999 ], ..., [ 1. , 0.997999 ], [ 1. , 0.9989995], [ 1. , 1. ]])&gt; . (2) 1ter돌려봄 . net_mse = tf.keras.Sequential() net_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn) net_mse.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 79ms/step - loss: 0.2311 . &lt;keras.callbacks.History at 0x7f349d524070&gt; . net_bce = tf.keras.Sequential() net_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn) net_bce.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 94ms/step - loss: 0.5606 . &lt;keras.callbacks.History at 0x7f349d526fb0&gt; . net_mse.get_weights(), net_bce.get_weights() . ([array([[0.07441761], [0.40206426]], dtype=float32)], [array([[-0.86062825], [ 0.9297301 ]], dtype=float32)]) . net_mse.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)]) net_bce.set_weights([tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32)]) . net_mse.get_weights(), net_bce.get_weights() . ([array([[-3.], [-1.]], dtype=float32)], [array([[-3.], [-1.]], dtype=float32)]) . (4) 학습과정기록: 15에폭마다 기록 . What_mse = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32) What_bce = tnp.array([[-3.0 ],[ -1.0]],dtype=tf.float32) . for k in range(29): net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0) net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0) What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) . (5) 시각화 . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure() fig.set_figwidth(6) fig.set_figheight(6) fig.suptitle(&quot;Adam, Winit=(-3,-1)&quot;) ax1=fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75 ax3=fig.add_subplot(2,2,3) ax4=fig.add_subplot(2,2,4) ax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax3.plot(x,y,&#39;,&#39;); ax3.plot(x,v,&#39;--r&#39;); line3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),&#39;--b&#39;) ax4.plot(x,y,&#39;,&#39;); ax4.plot(x,v,&#39;--r&#39;) line4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),&#39;--b&#39;) def animate(i): _w0_mse,_w1_mse = What_mse[:,i] _w0_bce,_w1_bce = What_bce[:,i] ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color=&#39;gray&#39;) ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color=&#39;gray&#39;) line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i]))) line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i]))) ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect &#54617;&#49845;&#44284;&#51221; &#49884;&#44033;&#54868;&#50696;&#49884;3 . - 파라메터학습과정 시각화 // 옵티마이저: Adam, 초기값: (w0,w1) = (-10.0,-1.0) . (1) 데이터정리 . X = tf.concat([tf.ones(N,dtype=tf.float64).reshape(N,1),x],axis=1) X . &lt;tf.Tensor: shape=(2000, 2), dtype=float64, numpy= array([[ 1. , -1. ], [ 1. , -0.9989995], [ 1. , -0.997999 ], ..., [ 1. , 0.997999 ], [ 1. , 0.9989995], [ 1. , 1. ]])&gt; . (2) 1ter돌려봄 . net_mse = tf.keras.Sequential() net_mse.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_mse.compile(optimizer=tf.optimizers.Adam(0.1),loss=mseloss_fn) net_mse.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 75ms/step - loss: 0.2175 . &lt;keras.callbacks.History at 0x7f349c9d9e40&gt; . net_bce = tf.keras.Sequential() net_bce.add(tf.keras.layers.Dense(1,use_bias=False,activation=&#39;sigmoid&#39;)) net_bce.compile(optimizer=tf.optimizers.Adam(0.1),loss=bceloss_fn) net_bce.fit(X,y,epochs=1,batch_size=N) . 1/1 [==============================] - 0s 82ms/step - loss: 0.5323 . &lt;keras.callbacks.History at 0x7f349caa17b0&gt; . net_mse.get_weights(), net_bce.get_weights() . ([array([[-0.02143217], [ 0.484821 ]], dtype=float32)], [array([[-0.8675074], [ 1.1268172]], dtype=float32)]) . net_mse.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)]) net_bce.set_weights([tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32)]) . net_mse.get_weights(), net_bce.get_weights() . ([array([[-10.], [ -1.]], dtype=float32)], [array([[-10.], [ -1.]], dtype=float32)]) . (4) 학습과정기록: 15에폭마다 기록 . What_mse = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32) What_bce = tnp.array([[-10.0 ],[ -1.0]],dtype=tf.float32) . for k in range(29): net_mse.fit(X,y,epochs=15,batch_size=N,verbose=0) net_bce.fit(X,y,epochs=15,batch_size=N,verbose=0) What_mse = tf.concat([What_mse,net_mse.weights[0]],axis=1) What_bce = tf.concat([What_bce,net_bce.weights[0]],axis=1) . (5) 시각화 . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure() fig.set_figwidth(6) fig.set_figheight(6) fig.suptitle(&quot;Adam, Winit=(-10,-1)&quot;) ax1=fig.add_subplot(2,2,1,projection=&#39;3d&#39;) ax2=fig.add_subplot(2,2,2,projection=&#39;3d&#39;) ax1.elev=15;ax2.elev=15;ax1.azim=75;ax2.azim=75 ax3=fig.add_subplot(2,2,3) ax4=fig.add_subplot(2,2,4) ax1.scatter(w0,w1,loss1,s=0.1);ax1.scatter(-1,5,loss_fn1(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax2.scatter(w0,w1,loss2,s=0.1);ax2.scatter(-1,5,loss_fn2(-1,5),color=&#39;red&#39;,marker=&#39;*&#39;,s=200) ax3.plot(x,y,&#39;,&#39;); ax3.plot(x,v,&#39;--r&#39;); line3, = ax3.plot(x,1/(1+np.exp(-X@What_mse[:,0])),&#39;--b&#39;) ax4.plot(x,y,&#39;,&#39;); ax4.plot(x,v,&#39;--r&#39;) line4, = ax4.plot(x,1/(1+np.exp(-X@What_bce[:,0])),&#39;--b&#39;) def animate(i): _w0_mse,_w1_mse = What_mse[:,i] _w0_bce,_w1_bce = What_bce[:,i] ax1.scatter(_w0_mse, _w1_mse, loss_fn1(_w0_mse, _w1_mse),color=&#39;gray&#39;) ax2.scatter(_w0_bce, _w1_bce, loss_fn2(_w0_bce, _w1_bce),color=&#39;gray&#39;) line3.set_ydata(1/(1+np.exp(-X@What_mse[:,i]))) line4.set_ydata(1/(1+np.exp(-X@What_bce[:,i]))) ani = animation.FuncAnimation(fig, animate, frames=30) plt.close() ani . &lt;/input&gt; Once Loop Reflect 아무리 아담이라고 해도 이건 힘듬 | . - discussion . mse_loss는 경우에 따라서 엄청 수렴속도가 느릴수도 있음. | 근본적인 문제점: mse_loss일 경우 loss function의 곡면이 예쁘지 않음. (전문용어로 convex가 아니라고 말함) | 좋은 옵티마지어를 이용하면 mse_loss일 경우에도 수렴속도를 올릴 수 있음 (학습과정 시각화예시2). 그렇지만 이는 근본적인 해결책은 아님. (학습과정 시각화예시3) | . - 요약: 왜 logistic regression에서 mse loss를 쓰면 안되는가? . mse loss를 사용하면 손실함수가 convex하지 않으니까! | 그리고 bce loss를 사용하면 손실함수가 convex하니까! | .",
            "url": "https://guebin.github.io/STBDA2022/2022/04/18/(7%EC%A3%BC%EC%B0%A8)-4%EC%9B%9418%EC%9D%BC.html",
            "relUrl": "/2022/04/18/(7%EC%A3%BC%EC%B0%A8)-4%EC%9B%9418%EC%9D%BC.html",
            "date": " • Apr 18, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "(6주차) 4월11일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . $x to hat{y}$ &#44032; &#46104;&#45716; &#44284;&#51221;&#51012; &#44536;&#47548;&#51004;&#47196; &#44536;&#47532;&#44592; . - 단순회귀분석의 예시 . $ hat{y}_i = hat{ beta}_0 + hat{ beta}_1 x_i, quad i=1,2, dots,n$ | . (표현1) . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;β̂₀ + xₙ*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;xₙ&quot; -&gt; &quot;β̂₀ + xₙ*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + xₙ*β̂₁, bias=False&quot; -&gt; &quot;ŷₙ&quot;[label=&quot;identity&quot;] &quot;.&quot; -&gt; &quot;....................................&quot;[label=&quot;* β̂₀&quot;] &quot;..&quot; -&gt; &quot;....................................&quot;[label=&quot;* β̂₁&quot;] &quot;....................................&quot; -&gt; &quot;...&quot;[label=&quot; &quot;] &quot;1 &quot; -&gt; &quot;β̂₀ + x₂*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;x₂&quot; -&gt; &quot;β̂₀ + x₂*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + x₂*β̂₁, bias=False&quot; -&gt; &quot;ŷ₂&quot;[label=&quot;identity&quot;] &quot;1 &quot; -&gt; &quot;β̂₀ + x₁*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;x₁&quot; -&gt; &quot;β̂₀ + x₁*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + x₁*β̂₁, bias=False&quot; -&gt; &quot;ŷ₁&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False 1&#45;&gt;β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷₙ ŷₙ β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷₙ identity xₙ xₙ xₙ&#45;&gt;β̂₀ + xₙ*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ . . .................................... .................................... .&#45;&gt;.................................... * β̂₀ ... ... ....................................&#45;&gt;... .. .. ..&#45;&gt;.................................... * β̂₁ 1 1 β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False 1 &#45;&gt;β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷ₂ ŷ₂ β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷ₂ identity x₂ x₂ x₂&#45;&gt;β̂₀ + x₂*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ 1 &#160; 1 &#160; β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False 1 &#160;&#45;&gt;β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷ₁ ŷ₁ β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷ₁ identity x₁ x₁ x₁&#45;&gt;β̂₀ + x₁*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ - 표현1의 소감? . 교수님이 고생해서 만든것 같음 | 그런데 그냥 다 똑같은 그림의 반복이라 사실 고생한 의미가 없음. | . (표현2) . - 그냥 아래와 같이 그리고 &quot;모든 $i=1,2,3, dots,n$에 대하여 $ hat{y}_i$을 아래의 그림과 같이 그린다&quot;고 하면 될것 같다. . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;β̂₀ + xᵢ*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;xᵢ&quot; -&gt; &quot;β̂₀ + xᵢ*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + xᵢ*β̂₁, bias=False&quot; -&gt; &quot;ŷᵢ&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False 1&#45;&gt;β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷᵢ ŷᵢ β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷᵢ identity xᵢ xᵢ xᵢ&#45;&gt;β̂₀ + xᵢ*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ (표현3) . - 그런데 &quot;모든 $i=1,2,3, dots,n$에 대하여 $ hat{y}_i$을 아래의 그림과 같이 그린다&quot; 라는 언급자체도 반복할 필요가 없을 것 같다. (어차피 당연히 그럴테니까) 그래서 단순히 아래와 같이 그려도 무방할듯 하다. . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;β̂₀ + x*β̂₁, bias=False&quot;[label=&quot;* β̂₀&quot;] &quot;x&quot; -&gt; &quot;β̂₀ + x*β̂₁, bias=False&quot;[label=&quot;* β̂₁&quot;] &quot;β̂₀ + x*β̂₁, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False 1&#45;&gt;β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False * β̂₀ ŷ ŷ β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False&#45;&gt;ŷ identity x x x&#45;&gt;β̂₀ + x*β̂₁, &#160;&#160;&#160;bias=False * β̂₁ (표현4) . - 위의 모델은 아래와 같이 쓸 수 있다. ($ beta_0$를 바이어스로 표현) . gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*β̂₁, bias=True&quot;[label=&quot;*β̂₁&quot;] ; &quot;x*β̂₁, bias=True&quot; -&gt; &quot;ŷ&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*β̂₁, &#160;&#160;&#160;bias=True x*β̂₁, &#160;&#160;&#160;bias=True x&#45;&gt;x*β̂₁, &#160;&#160;&#160;bias=True *β̂₁ ŷ ŷ x*β̂₁, &#160;&#160;&#160;bias=True&#45;&gt;ŷ indentity 실제로는 이 표현을 많이 사용함 | . (표현5) . - 벡터버전으로 표현하면 아래와 같다. 이 경우에는 ${ bf X}=[1,x]$에 포함된 1이 bias의 역할을 해주므로 bias = False 임. . gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@β̂, bias=False&quot;[label=&quot;@β̂&quot;] ; &quot;X@β̂, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@β̂, &#160;&#160;&#160;bias=False X@β̂, &#160;&#160;&#160;bias=False X&#45;&gt;X@β̂, &#160;&#160;&#160;bias=False @β̂ ŷ ŷ X@β̂, &#160;&#160;&#160;bias=False&#45;&gt;ŷ indentity 저는 이걸 좋아해요 | . (표현5)&#39; . - 딥러닝에서는 $ hat{ boldsymbol{ beta}}$ 대신에 $ hat$을 라고 표현한다. . gv(&#39;&#39;&#39; &quot;X&quot; -&gt; &quot;X@Ŵ, bias=False&quot;[label=&quot;@Ŵ&quot;] ; &quot;X@Ŵ, bias=False&quot; -&gt; &quot;ŷ&quot;[label=&quot;identity&quot;] &#39;&#39;&#39;) . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X X X@Ŵ, &#160;&#160;&#160;bias=False X@Ŵ, &#160;&#160;&#160;bias=False X&#45;&gt;X@Ŵ, &#160;&#160;&#160;bias=False @Ŵ ŷ ŷ X@Ŵ, &#160;&#160;&#160;bias=False&#45;&gt;ŷ identity - 실제로는 표현4 혹은 표현5를 외우면 된다. . Layer&#51032; &#44060;&#45392; . - (표현4) 혹은 (표현5)의 그림은 레이어로 설명할 수 있다. . - 레이어는 항상 아래와 같은 규칙을 가진다. . 첫 동그라미는 레이어의 입력이다. | 첫번째 화살표는 선형변환을 의미한다. | 두번째 동그라미는 선형변환의 결과이다. (이때 bias가 false인지 true인지에 따라서 실제 수식이 조금 다름) | 두번째 화살표는 두번째 동그라미에 어떠한 함수 $f$를 취하는 과정을 의미한다. (우리의 그림에서는 $f(x)=x$) | 세번째 동그라미는 레이어의 최종출력이다. | . - 엄청 복잡한데, 결국 레이어를 만들때 위의 그림들을 의미하도록 하려면 아래의 4개의 요소만 필요하다. . 레이어의 입력차원 | 선형변환의 결과로 얻어지는 차원 | 선형변환에서 바이어스를 쓸지? 안쓸지? | 함수 $f$ | - 주목: 1,2가 결정되면 자동으로 $ hat$의 차원이 결정된다. . (예시) . 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=1: $ hat{ bf W}$는 (2,1) 매트릭스 | 레이어의 입력차원=20, 선형변환의 결과로 얻어지는 차원=5: $ hat{ bf W}$는 (20,5) 매트릭스 | 레이어의 입력차원=2, 선형변환의 결과로 얻어지는 차원=50: $ hat{ bf W}$는 (2,50) 매트릭스 | . - 주목2: 이중에서 절대 생략불가능 것은 &quot;2. 선형변환의 결과로 얻어지는 차원&quot; 이다. . 레이어의 입력차원: 실제 레이어에 데이터가 들어올 때 데이터의 입력차원을 컴퓨터 스스로 체크하여 $ hat{ bf W}$의 차원을 결정할 수 있음. | 바이어스를 쓸지? 안쓸지? 기본적으로 쓴다고 가정한다. | 함수 $f$: 기본적으로 항등함수를 가정하면 된다. | . Keras&#47484; &#51060;&#50857;&#54620; &#54400;&#51060; . - 기본뼈대: net생성 $ to$ add(layer) $ to$ compile(opt,loss) $ to$ fit(data,epochs) . - 데이터정리 . $${ bf y} approx 2.5 +4*x$$ . tnp.random.seed(43052) N= 200 x= tnp.linspace(0,1,N) epsilon= tnp.random.randn(N)*0.5 y= 2.5+4*x +epsilon . X=tf.stack([tf.ones(N,dtype=&#39;float64&#39;),x],axis=1) . &#54400;&#51060;1: &#49828;&#52860;&#46972;&#48260;&#51204; . (0단계) 데이터정리 . y=y.reshape(N,1) x=x.reshape(N,1) x.shape,y.shape . (TensorShape([200, 1]), TensorShape([200, 1])) . (1단계) net 생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1) # 입력차원? 데이터를 넣어보고 결정, 바이어스=디폴드값을 쓰겠음 (use_bias=true), 함수도 디폴트값을 쓰겠음 (f(x)=x) net.add(layer) . (3단계) net.compile(opt,loss_fn) . net.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) . (4단계) net.fit(x,y,epochs) . net.fit(x,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 . &lt;keras.callbacks.History at 0x7f109a893550&gt; . (결과확인) . net.weights . [&lt;tf.Variable &#39;dense/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[3.9330251]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense/bias:0&#39; shape=(1,) dtype=float32, numpy=array([2.5836723], dtype=float32)&gt;] . &#54400;&#51060;2: &#48289;&#53552;&#48260;&#51204; . (0단계) 데이터정리 . X.shape,y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net 생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,use_bias=False) net.add(layer) . (3단계) net.compile(opt,loss_fn) . net.compile(tf.keras.optimizers.SGD(0.1), tf.keras.losses.MSE) . (4단계) net.fit(x,y,epochs) . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) # batch_size=N 일 경우에 경사하강법이 적용, batch_size!=N 이면 확률적 경사하강법 적용 . &lt;keras.callbacks.History at 0x7f102c2b3b20&gt; . (결과확인) . net.weights . [&lt;tf.Variable &#39;dense_1/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836723], [3.9330251]], dtype=float32)&gt;] . &#51104;&#49884;&#47928;&#48277;&#51221;&#47532; . - 잠깐 Dense layer를 만드는 코드를 정리해보자. . (1) 아래는 모두 같은 코드이다. . tf.keras.layers.Dense(1) | tf.keras.layers.Dense(units=1) | tf.keras.layers.Dense(units=1,activation=&#39;linear&#39;) // identity 가 더 맞는것 같은데.. | tf.keras.layers.Dense(units=1,activation=&#39;linear&#39;,use_bias=True) | . (2) 아래의 코드1,2는 (1)의 코드들과 살짝 다른코드이다. (코드1과 코드2는 같은코드임) . tf.keras.layers.Dense(1,input_dim=2) # 코드1 | tf.keras.layers.Dense(1,input_shape=(2,)) # 코드2 | . (3) 아래는 사용불가능한 코드이다. . tf.keras.layers.Dense(1,input_dim=(2,)) # 코드1 | tf.keras.layers.Dense(1,input_shape=2) # 코드2 | . - 왜 input_dim이 필요한가? . net1 = tf.keras.Sequential() net1.add(tf.keras.layers.Dense(1,use_bias=False)) . net2 = tf.keras.Sequential() net2.add(tf.keras.layers.Dense(1,use_bias=False,input_dim=2)) . net1.weights . ValueError Traceback (most recent call last) Input In [36], in &lt;cell line: 1&gt;() -&gt; 1 net1.weights File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py:2542, in Model.weights(self) 2532 @property 2533 def weights(self): 2534 &#34;&#34;&#34;Returns the list of all layer variables/weights. 2535 2536 Note: This will not track the weights of nested `tf.Modules` that are not (...) 2540 A list of variables. 2541 &#34;&#34;&#34; -&gt; 2542 return self._dedup_weights(self._undeduplicated_weights) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py:2547, in Model._undeduplicated_weights(self) 2544 @property 2545 def _undeduplicated_weights(self): 2546 &#34;&#34;&#34;Returns the undeduplicated list of all layer variables/weights.&#34;&#34;&#34; -&gt; 2547 self._assert_weights_created() 2548 weights = [] 2549 for layer in self._self_tracked_trackables: File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/sequential.py:471, in Sequential._assert_weights_created(self) 468 return 469 # When the graph has not been initialized, use the Model&#39;s implementation to 470 # to check if the weights has been created. --&gt; 471 super(functional.Functional, self)._assert_weights_created() File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py:2736, in Model._assert_weights_created(self) 2728 return 2730 if (&#39;build&#39; in self.__class__.__dict__ and 2731 self.__class__ != Model and 2732 not self.built): 2733 # For any model that has customized build() method but hasn&#39;t 2734 # been invoked yet, this will cover both sequential and subclass model. 2735 # Also make sure to exclude Model class itself which has build() defined. -&gt; 2736 raise ValueError(f&#39;Weights for model {self.name} have not yet been &#39; 2737 &#39;created. &#39; 2738 &#39;Weights are created when the Model is first called on &#39; 2739 &#39;inputs or `build()` is called with an `input_shape`.&#39;) ValueError: Weights for model sequential_4 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`. . net2.weights . [&lt;tf.Variable &#39;dense_5/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[0.4367702], [0.8878907]], dtype=float32)&gt;] . net1.summary() . ValueError Traceback (most recent call last) Input In [38], in &lt;cell line: 1&gt;() -&gt; 1 net1.summary() File ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/training.py:2579, in Model.summary(self, line_length, positions, print_fn, expand_nested) 2559 &#34;&#34;&#34;Prints a string summary of the network. 2560 2561 Args: (...) 2576 ValueError: if `summary()` is called before the model is built. 2577 &#34;&#34;&#34; 2578 if not self.built: -&gt; 2579 raise ValueError( 2580 &#39;This model has not yet been built. &#39; 2581 &#39;Build the model first by calling `build()` or by calling &#39; 2582 &#39;the model on a batch of data.&#39;) 2583 layer_utils.print_summary( 2584 self, 2585 line_length=line_length, 2586 positions=positions, 2587 print_fn=print_fn, 2588 expand_nested=expand_nested) ValueError: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data. . net2.summary() . Model: &#34;sequential_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_5 (Dense) (None, 1) 2 ================================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _________________________________________________________________ . &#54400;&#51060;3: &#49828;&#52860;&#46972;&#48260;&#51204;, &#51076;&#51032;&#51032; &#52488;&#44592;&#44050;&#51012; &#49444;&#51221; . (0단계) 데이터정리 . y=y.reshape(N,1) x=x.reshape(N,1) x.shape,y.shape . (TensorShape([200, 1]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,input_dim=1) . net.add(layer) . . 초기값을 설정 . net.weights . [&lt;tf.Variable &#39;dense_6/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[1.2078832]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_6/bias:0&#39; shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)&gt;] . net.get_weights() . [array([[1.2078832]], dtype=float32), array([0.], dtype=float32)] . weight, bias순으로 출력 | . net.set_weights? . Signature: net.set_weights(weights) Docstring: Sets the weights of the layer, from NumPy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer&#39;s weights must be instantiated before calling this function, by calling the layer. For example, a `Dense` layer returns a list of two values: the kernel matrix and the bias vector. These can be used to set the weights of another `Dense` layer: &gt;&gt;&gt; layer_a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) &gt;&gt;&gt; a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]])) &gt;&gt;&gt; layer_a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] &gt;&gt;&gt; layer_b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) &gt;&gt;&gt; b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]])) &gt;&gt;&gt; layer_b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] &gt;&gt;&gt; layer_b.set_weights(layer_a.get_weights()) &gt;&gt;&gt; layer_b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Args: weights: a list of NumPy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer&#39;s specifications. File: ~/anaconda3/envs/py310/lib/python3.10/site-packages/keras/engine/base_layer.py Type: method . layer_b.set_weights(layer_a.get_weights()) 와 같은방식으로 쓴다는 것이군? | . - 한번따라해보자. . _w = net.get_weights() _w . [array([[1.2078832]], dtype=float32), array([0.], dtype=float32)] . 길이가 2인 리스트이고, 각 원소는 numpy array 임 | . net.set_weights( [np.array([[10.0]],dtype=np.float32), # weight, β1_hat np.array([-5.0],dtype=np.float32)] # bias, β0_hat ) . net.weights . [&lt;tf.Variable &#39;dense_6/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[10.]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_6/bias:0&#39; shape=(1,) dtype=float32, numpy=array([-5.], dtype=float32)&gt;] . . (3단계) net.compile() . net.compile(tf.keras.optimizers.SGD(0.1),tf.losses.MSE) . (4단계) net.fit() . net.fit(x,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f0f90b1d120&gt; . 결과확인 . net.weights . [&lt;tf.Variable &#39;dense_6/kernel:0&#39; shape=(1, 1) dtype=float32, numpy=array([[3.933048]], dtype=float32)&gt;, &lt;tf.Variable &#39;dense_6/bias:0&#39; shape=(1,) dtype=float32, numpy=array([2.58366], dtype=float32)&gt;] . &#54400;&#51060;4: &#48289;&#53552;&#48260;&#51204;, &#51076;&#51032;&#51032; &#52488;&#44592;&#44050;&#51012; &#49444;&#51221; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,use_bias=False,input_dim=2) . net.add(layer) . . 초기값을 설정하자 . net.set_weights([np.array([[ -5.0],[10.0]], dtype=np.float32)]) . net.get_weights() . [array([[-5.], [10.]], dtype=float32)] . . (3단계) net.compile() . net.compile(tf.keras.optimizers.SGD(0.1), tf.losses.MSE) . (4단계) net.fit() . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f0f7b9175e0&gt; . net.weights . [&lt;tf.Variable &#39;dense_7/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.58366 ], [3.933048]], dtype=float32)&gt;] . - 사실 실전에서는 초기값을 설정할 필요가 별로 없음. . &#54400;&#51060;5: &#48289;&#53552;&#48260;&#51204; &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . layer = tf.keras.layers.Dense(1,use_bias=False) . net.add(layer) . (3단계) net.compile() . loss_fn = lambda y,yhat: (y-yhat).T @ (y-yhat) / N . net.compile(tf.keras.optimizers.SGD(0.1), loss_fn) . (4단계) net.fit() . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f0f914134f0&gt; . net.weights . [&lt;tf.Variable &#39;dense_8/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836723], [3.9330251]], dtype=float32)&gt;] . &#54400;&#51060;6: &#48289;&#53552;&#48260;&#51204;, net.compile&#51032; &#50741;&#49496;&#51004;&#47196; &#49552;&#49892;&#54632;&#49688; &#51648;&#51221; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . net.add(tf.keras.layers.Dense(1,use_bias=False)) . (3단계) net.compile() . net.compile(tf.keras.optimizers.SGD(0.1), loss=&#39;mse&#39;) . (4단계) net.fit() . net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f0f7b9e02e0&gt; . net.weights . [&lt;tf.Variable &#39;dense_11/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5836723], [3.9330251]], dtype=float32)&gt;] . &#54400;&#51060;7: &#48289;&#53552;&#48260;&#51204;, net.compile&#51032; &#50741;&#49496;&#51004;&#47196; &#49552;&#49892;&#54632;&#49688; &#51648;&#51221; + &#50741;&#54000;&#47560;&#51060;&#51200; &#51648;&#51221; . (0단계) 데이터정리 . X.shape, y.shape . (TensorShape([200, 2]), TensorShape([200, 1])) . (1단계) net생성 . net = tf.keras.Sequential() . (2단계) net.add(layer) . net.add(tf.keras.layers.Dense(1,use_bias=False)) . (3단계) net.compile() . net.compile(optimizer=&#39;sgd&#39;, loss=&#39;mse&#39;) #net.optimizer.lr = tf.Variable(0.1,dtype=tf.float32) #net.optimizer.lr = 0.1 . (4단계) net.fit() . net.fit(X,y,epochs=5000,verbose=0,batch_size=N) . &lt;keras.callbacks.History at 0x7f0f785482e0&gt; . net.weights . [&lt;tf.Variable &#39;dense_22/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.5848842], [3.9307659]], dtype=float32)&gt;] . &#50668;&#47084;&#44032;&#51648; &#54924;&#44480;&#47784;&#54805;&#51032; &#51201;&#54633;&#44284; &#54617;&#49845;&#44284;&#51221;&#51032; &#47784;&#45768;&#53552;&#47553; . &#50696;&#51228;1 . model: $y_i approx beta_0 + beta_1 x_i$ . np.random.seed(43052) N= 100 x= np.random.randn(N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*x +epsilon . X= np.stack([np.ones(N),x],axis=1) y= y.reshape(N,1) . plt.plot(x,y,&#39;o&#39;) # 관측한 자료 . [&lt;matplotlib.lines.Line2D at 0x7f0f7baa94e0&gt;] . beta_hat = np.array([-3,-2]).reshape(2,1) . yhat = X@beta_hat . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.reshape(-1),&#39;-&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f0f7b9177c0&gt;] . 더 좋은 적합선을 얻기위해서! . slope = (2*X.T@X@beta_hat - 2*X.T@y)/ N beta_hat2 = beta_hat - 0.1*slope yhat2 = X@beta_hat2 . plt.plot(x,y,&#39;o&#39;) plt.plot(x,yhat.reshape(-1),&#39;-&#39;) plt.plot(x,yhat2.reshape(-1),&#39;-&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f0f7a0b6230&gt;] . 초록색이 좀 더 나아보인다. . beta_hat = np.array([-3,-2]).reshape(2,1) beta_hats = beta_hat # beta_hats = beta_hat.copy() 가 더 안전한 코드입니다. for i in range(1,30): yhat = X@beta_hat slope = (2*X.T@X@beta_hat - 2*X.T@y) / N beta_hat = beta_hat - 1.0*slope # 0.1은 적당, 0.3은 쪼금빠르지만 그래도 적당, 0.9는 너무 나간것같음, 1.0 은 수렴안함, 1.2 beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) . beta_hats . array([[-3. , 7.12238255, -1.2575366 , 5.73166742, -0.1555309 , 4.86767499, 0.51106397, 4.36611576, 0.87316777, 4.12348617, 1.01165173, 4.07771926, 0.97282343, 4.19586617, 0.77814101, 4.46653491, 0.4299822 , 4.89562729, -0.08537358, 5.50446319, -0.79684366, 6.32975688, -1.74933031, 7.42517729, -3.00603683, 8.86442507, -4.6523303 , 10.74592463, -6.80132547, 13.19938129], [-2. , 8.70824998, 0.16165717, 6.93399596, 1.62435964, 5.72089586, 2.63858056, 4.86387722, 3.37280529, 4.22385379, 3.94259478, 3.70397678, 4.43004465, 3.23363047, 4.89701606, 2.75741782, 5.39439054, 2.22728903, 5.96886945, 1.59655409, 6.66836857, 0.81489407, 7.54676324, -0.17628423, 8.66856437, -1.44867655, 10.11401544, -3.09256176, 11.98507323, -5.22340389]]) . b0hats = beta_hats[0].tolist() b1hats = beta_hats[1].tolist() . np.linalg.inv(X.T@X) @ X.T @ y . array([[2.5451404 ], [3.94818596]]) . from matplotlib import animation plt.rcParams[&quot;animation.html&quot;] = &quot;jshtml&quot; . fig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12) . &lt;Figure size 864x360 with 0 Axes&gt; . ax1= fig.add_subplot(1,2,1) ax2= fig.add_subplot(1,2,2,projection=&#39;3d&#39;) # ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,b0hats[0] + b1hats[0]*x) # ax2: 오른쪽그림 β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing=&#39;ij&#39;) β0=β0.reshape(-1) β1=β1.reshape(-1) loss_fn = lambda b0,b1: np.sum((y-b0-b1*x)**2) loss = list(map(loss_fn, β0,β1)) ax2.scatter(β0,β1,loss,alpha=0.02) ax2.scatter(2.5451404,3.94818596,loss_fn(2.5451404,3.94818596),s=200,marker=&#39;*&#39;) def animate(i): line.set_ydata(b0hats[i] + b1hats[i]*x) ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=&quot;grey&quot;) ani = animation.FuncAnimation(fig,animate,frames=30) ani . &lt;/input&gt; Once Loop Reflect &#50696;&#51228;2 . model: $y_i approx beta_0 + beta_1 e^{-x_i}$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) +epsilon . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f0f797c2020&gt;] . X= np.stack([np.ones(N),np.exp(-x)],axis=1) y= y.reshape(N,1) . beta_hat = np.array([-3,-2]).reshape(2,1) beta_hats = beta_hat.copy() # shallow copy, deep copy &lt; 여름 방학 특강 for i in range(1,30): yhat = X@beta_hat slope = (2*X.T@X@beta_hat - 2*X.T@y) /N beta_hat = beta_hat - 0.05*slope beta_hats = np.concatenate([beta_hats,beta_hat],axis=1) . beta_hats . array([[-3. , -1.74671631, -0.82428979, -0.14453919, 0.35720029, 0.72834869, 1.0036803 , 1.20869624, 1.36209751, 1.47759851, 1.56525696, 1.63244908, 1.68458472, 1.72563174, 1.75850062, 1.78532638, 1.80767543, 1.82669717, 1.84323521, 1.85790889, 1.8711731 , 1.88336212, 1.89472176, 1.90543297, 1.91562909, 1.92540859, 1.93484428, 1.94399023, 1.9528867 , 1.96156382], [-2. , -0.25663415, 1.01939241, 1.95275596, 2.63488171, 3.13281171, 3.49570765, 3.75961951, 3.95098231, 4.08918044, 4.18842797, 4.2591476 , 4.30898175, 4.34353413, 4.36691339, 4.38213187, 4.39139801, 4.39633075, 4.39811673, 4.3976256 , 4.3954946 , 4.3921905 , 4.38805511, 4.3833386 , 4.37822393, 4.37284482, 4.36729887, 4.36165718, 4.35597148, 4.35027923]]) . b0hats= beta_hats[0].tolist() b1hats= beta_hats[1].tolist() . np.linalg.inv(X.T@X)@X.T@y . array([[2.46307644], [3.99681332]]) . fig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12) . &lt;Figure size 864x360 with 0 Axes&gt; . ax1= fig.add_subplot(1,2,1) ax2= fig.add_subplot(1,2,2,projection=&#39;3d&#39;) # ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x)) # ax2: 오른쪽그림 β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing=&#39;ij&#39;) β0=β0.reshape(-1) β1=β1.reshape(-1) loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2) loss = list(map(loss_fn, β0,β1)) ax2.scatter(β0,β1,loss,alpha=0.02) ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker=&#39;*&#39;) def animate(i): line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x)) ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=&quot;grey&quot;) ani = animation.FuncAnimation(fig,animate,frames=30) ani . &lt;/input&gt; Once Loop Reflect &#50696;&#51228;3 . model: $y_i approx beta_0 + beta_1 e^{-x_i} + beta_2 cos(5x_i)$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon . plt.plot(x,y,&#39;o&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f0f6915f850&gt;] . X=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) y=y.reshape(N,1) . beta_hat = np.array([-3,-2,-1]).reshape(3,1) beta_hats = beta_hat.copy() for i in range(1,30): yhat = X@beta_hat slope = (2*X.T@X@beta_hat -2*X.T@y) /N beta_hat = beta_hat - 0.1 * slope beta_hats= np.concatenate([beta_hats,beta_hat],axis=1) . beta_hats . array([[-3. , -0.71767532, 0.36255782, 0.89072137, 1.16423101, 1.31925078, 1.41819551, 1.48974454, 1.54713983, 1.59655416, 1.64091846, 1.68167278, 1.71956758, 1.75503084, 1.78833646, 1.81968188, 1.84922398, 1.877096 , 1.90341567, 1.92828934, 1.95181415, 1.97407943, 1.99516755, 2.01515463, 2.0341111 , 2.05210214, 2.06918818, 2.08542523, 2.10086524, 2.11555643], [-2. , 1.16947474, 2.64116513, 3.33411605, 3.66880042, 3.83768856, 3.92897389, 3.98315095, 4.01888831, 4.04486085, 4.06516144, 4.08177665, 4.09571971, 4.10754954, 4.1176088 , 4.12613352, 4.13330391, 4.13926816, 4.14415391, 4.14807403, 4.15112966, 4.1534121 , 4.15500404, 4.15598045, 4.15640936, 4.15635249, 4.15586584, 4.15500014, 4.15380139, 4.1523112 ], [-1. , -0.95492718, -0.66119313, -0.27681968, 0.12788212, 0.52254445, 0.89491388, 1.24088224, 1.55993978, 1.85310654, 2.12199631, 2.36839745, 2.59408948, 2.8007666 , 2.99000967, 3.16327964, 3.32192026, 3.46716468, 3.60014318, 3.72189116, 3.83335689, 3.93540864, 4.02884144, 4.11438316, 4.19270026, 4.26440288, 4.33004965, 4.39015202, 4.44517824, 4.49555703]]) . b0hats,b1hats,b2hats = beta_hats . np.linalg.inv(X.T@X) @ X.T @ y . array([[2.46597526], [4.00095138], [5.04161877]]) . fig = plt.figure(); fig.set_figheight(5); fig.set_figwidth(12) . &lt;Figure size 864x360 with 0 Axes&gt; . ax1= fig.add_subplot(1,2,1) ax2= fig.add_subplot(1,2,2,projection=&#39;3d&#39;) # ax1: 왼쪽그림 ax1.plot(x,y,&#39;o&#39;) line, = ax1.plot(x,b0hats[0] + b1hats[0]*np.exp(-x) + b2hats[0]*np.cos(5*x)) # ax2: 오른쪽그림 # β0,β1 = np.meshgrid(np.arange(-6,11,0.25),np.arange(-6,11,0.25),indexing=&#39;ij&#39;) # β0=β0.reshape(-1) # β1=β1.reshape(-1) # loss_fn = lambda b0,b1: np.sum((y-b0-b1*np.exp(-x))**2) # loss = list(map(loss_fn, β0,β1)) # ax2.scatter(β0,β1,loss,alpha=0.02) # ax2.scatter(2.46307644,3.99681332,loss_fn(2.46307644,3.99681332),s=200,marker=&#39;*&#39;) def animate(i): line.set_ydata(b0hats[i] + b1hats[i]*np.exp(-x) + b2hats[i]*np.cos(5*x)) # ax2.scatter(b0hats[i],b1hats[i],loss_fn(b0hats[i],b1hats[i]),color=&quot;grey&quot;) ani = animation.FuncAnimation(fig,animate,frames=30) ani . &lt;/input&gt; Once Loop Reflect &#50696;&#51228;3: &#52992;&#46972;&#49828;&#47196; &#54644;&#48372;&#51088;! . model: $y_i approx beta_0 + beta_1 e^{-x_i} + beta_2 cos(5x_i)$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) + 5*np.cos(5*x) + epsilon . X=np.stack([np.ones(N),np.exp(-x),np.cos(5*x)],axis=1) y=y.reshape(N,1) . net = tf.keras.Sequential() # 1: 네트워크 생성 net.add(tf.keras.layers.Dense(1,use_bias=False)) # 2: add layer net.compile(tf.optimizers.SGD(0.1), loss=&#39;mse&#39;) # 3: compile net.fit(X,y,epochs=30, batch_size=N) # 4: fit . Epoch 1/30 1/1 [==============================] - 0s 60ms/step - loss: 47.7087 Epoch 2/30 1/1 [==============================] - 0s 1ms/step - loss: 21.4259 Epoch 3/30 1/1 [==============================] - 0s 1ms/step - loss: 14.1095 Epoch 4/30 1/1 [==============================] - 0s 1ms/step - loss: 11.0534 Epoch 5/30 1/1 [==============================] - 0s 1ms/step - loss: 9.1350 Epoch 6/30 1/1 [==============================] - 0s 915us/step - loss: 7.6614 Epoch 7/30 1/1 [==============================] - 0s 758us/step - loss: 6.4544 Epoch 8/30 1/1 [==============================] - 0s 733us/step - loss: 5.4484 Epoch 9/30 1/1 [==============================] - 0s 741us/step - loss: 4.6063 Epoch 10/30 1/1 [==============================] - 0s 768us/step - loss: 3.9007 Epoch 11/30 1/1 [==============================] - 0s 766us/step - loss: 3.3093 Epoch 12/30 1/1 [==============================] - 0s 745us/step - loss: 2.8135 Epoch 13/30 1/1 [==============================] - 0s 735us/step - loss: 2.3979 Epoch 14/30 1/1 [==============================] - 0s 876us/step - loss: 2.0495 Epoch 15/30 1/1 [==============================] - 0s 889us/step - loss: 1.7574 Epoch 16/30 1/1 [==============================] - 0s 886us/step - loss: 1.5126 Epoch 17/30 1/1 [==============================] - 0s 788us/step - loss: 1.3073 Epoch 18/30 1/1 [==============================] - 0s 850us/step - loss: 1.1352 Epoch 19/30 1/1 [==============================] - 0s 717us/step - loss: 0.9910 Epoch 20/30 1/1 [==============================] - 0s 678us/step - loss: 0.8700 Epoch 21/30 1/1 [==============================] - 0s 695us/step - loss: 0.7686 Epoch 22/30 1/1 [==============================] - 0s 746us/step - loss: 0.6836 Epoch 23/30 1/1 [==============================] - 0s 703us/step - loss: 0.6123 Epoch 24/30 1/1 [==============================] - 0s 710us/step - loss: 0.5526 Epoch 25/30 1/1 [==============================] - 0s 838us/step - loss: 0.5025 Epoch 26/30 1/1 [==============================] - 0s 935us/step - loss: 0.4605 Epoch 27/30 1/1 [==============================] - 0s 947us/step - loss: 0.4252 Epoch 28/30 1/1 [==============================] - 0s 824us/step - loss: 0.3957 Epoch 29/30 1/1 [==============================] - 0s 864us/step - loss: 0.3709 Epoch 30/30 1/1 [==============================] - 0s 831us/step - loss: 0.3501 . &lt;keras.callbacks.History at 0x7f0f68b8fbb0&gt; . net.weights . [&lt;tf.Variable &#39;dense_23/kernel:0&#39; shape=(3, 1) dtype=float32, numpy= array([[2.354784 ], [3.9989622], [4.58522 ]], dtype=float32)&gt;] . plt.plot(x,y,&#39;o&#39;) plt.plot(x,(X@net.weights).reshape(-1),&#39;--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f095472a3b0&gt;] . &#49689;&#51228; . &#50696;&#51228;2: &#52992;&#46972;&#49828;&#47484; &#51060;&#50857;&#54616;&#50668; &#50500;&#47000;&#47484; &#47564;&#51313;&#54616;&#45716; &#51201;&#51208;&#54620; $ beta_0$&#50752; $ beta_1$&#51012; &#44396;&#54616;&#46972;. &#51201;&#54633;&#44208;&#44284;&#47484; &#49884;&#44033;&#54868;&#54616;&#46972;. (&#50528;&#45768;&#47700;&#51060;&#49496; &#49884;&#44033;&#54868; X) . model: $y_i approx beta_0 + beta_1 e^{-x_i}$ . np.random.seed(43052) N= 100 x= np.linspace(-1,1,N) epsilon = np.random.randn(N)*0.5 y= 2.5+4*np.exp(-x) +epsilon .",
            "url": "https://guebin.github.io/STBDA2022/2022/04/11/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%9411%EC%9D%BC.html",
            "relUrl": "/2022/04/11/(6%EC%A3%BC%EC%B0%A8)-4%EC%9B%9411%EC%9D%BC.html",
            "date": " • Apr 11, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "(5주차) 4월4일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . #!conda install -c conda-forge python-graphviz -y . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . &#52572;&#51201;&#54868;&#51032; &#47928;&#51228; . - $loss=( frac{1}{2} beta-1)^2$ . - 기존에 했던 방법은 수식을 알고 있어야 한다는 단점이 있음 . tf.keras.optimizers&#47484; &#51060;&#50857;&#54620; &#52572;&#51201;&#54868;&#48169;&#48277; . &#48169;&#48277;1: opt.apply_gradients()&#47484; &#51060;&#50857; . alpha= 0.01/6 . beta= tf.Variable(-10.0) . opt = tf.keras.optimizers.SGD(alpha) . - iter1 . with tf.GradientTape() as tape: tape.watch(beta) loss=(beta/2-1)**2 slope = tape.gradient(loss,beta) . opt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . - iter2 . with tf.GradientTape() as tape: tape.watch(beta) loss=(beta/2-1)**2 slope = tape.gradient(loss,beta) opt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.980008&gt; . - for문으로 정리 . alpha= 0.01/6 beta= tf.Variable(-10.0) opt = tf.keras.optimizers.SGD(alpha) . for epoc in range(10000): with tf.GradientTape() as tape: tape.watch(beta) loss=(beta/2-1)**2 slope = tape.gradient(loss,beta) opt.apply_gradients([(slope,beta)]) # beta.assign_sub(slope * alpha) beta . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . opt.apply_gradients()의 입력은 pair 의 list | . &#48169;&#48277;2: opt.minimize() . alpha= 0.01/6 beta= tf.Variable(-10.0) opt = tf.keras.optimizers.SGD(alpha) . loss_fn = lambda: (beta/2-1)**2 . lambda x: x**2 &lt;=&gt; lambda(x)=x^2 | lambda x,y: x+y &lt;=&gt; lambda(x,y)=x+y | lambda: y &lt;=&gt; lambda()=y, 입력이 없으며 출력은 항상 y인 함수 | . loss_fn() # 입력은 없고 출력은 뭔가 계산되는 함수 . &lt;tf.Tensor: shape=(), dtype=float32, numpy=36.0&gt; . - iter 1 . opt.minimize(loss_fn, beta) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=int64, numpy=1&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . - iter2 . opt.minimize(loss_fn, beta) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.980008&gt; . - for문으로 정리하면 . alpha= 0.01/6 beta= tf.Variable(-10.0) opt = tf.keras.optimizers.SGD(alpha) loss_fn = lambda: (beta/2-1)**2 for epoc in range(10000): opt.minimize(loss_fn, beta) beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.9971251&gt; . &#54924;&#44480;&#48516;&#49437; &#47928;&#51228; . - ${ bf y} approx 2.5 + 4.0 { bf x}$ . tnp.random.seed(43052) N = 200 x = tnp.linspace(0,1,N) epsilon = tnp.random.randn(N)*0.5 y = 2.5+4*x + epsilon y_true = 2.5+4*x . plt.plot(x,y,&#39;.&#39;) plt.plot(x,y_true,&#39;r--&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f69cc6a2890&gt;] . &#51060;&#47200;&#51201; &#54400;&#51060; . &#54400;&#51060;1: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . $S_{xx}=$, $S_{xy}=$ | $ hat{ beta}_0=$, $ hat{ beta}_1=$ | . - 풀이 . Sxx = sum((x-x.mean())**2) Sxy = sum((x-x.mean())*(y-y.mean())) . beta1_hat = Sxy/Sxx beta1_hat . &lt;tf.Tensor: shape=(), dtype=float64, numpy=3.933034516733168&gt; . beta0_hat = y.mean() - x.mean()*beta1_hat beta0_hat . &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.583667211565867&gt; . &#54400;&#51060;2: &#48289;&#53552;&#48260;&#51204; . - 포인트 . $ hat{ beta}=(X&#39;X)^{-1}X&#39;y$ | . - 풀이 . y=y.reshape(N,1) X=tf.stack([tf.ones(N,dtype=tf.float64),x],axis=1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . tf.linalg.inv(X.T @ X ) @ X.T @ y . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[2.58366721], [3.93303452]])&gt; . &#54400;&#51060;3: &#48289;&#53552;&#48260;&#51204;, &#49552;&#49892;&#54632;&#49688;&#51032; &#46020;&#54632;&#49688;&#51060;&#50857; . - 포인트 . $loss&#39;( beta)=-2X&#39;y +2X&#39;X beta$ | $ beta_{new} = beta_{old} - alpha times loss&#39;( beta_{old})$ | . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tnp.array([-5,10]).reshape(2,1) beta_hat . &lt;tf.Tensor: shape=(2, 1), dtype=int64, numpy= array([[-5], [10]])&gt; . slope = (-2*X.T @ y + 2*X.T @ X @ beta_hat) / N slope . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-9.10036894], [-3.52886113]])&gt; . alpha= 0.1 . step = slope*alpha step . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-0.91003689], [-0.35288611]])&gt; . for epoc in range(1000): slope = (-2*X.T @ y + 2*X.T @ X @ beta_hat)/N beta_hat = beta_hat - alpha* slope . beta_hat . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . GradientTape&#47484; &#51060;&#50857; . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204; . - 포인트 . ## 포인트코드1: 그레디언트 테입 with tf.GradientTape() as tape: loss = ## 포인트코드2: 미분 slope = tape.gradient(loss,beta_hat) ## 포인트코드3: update beta_hat.assign_sub(slope*alph) . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 . for epoc in range(1000): with tf.GradientTape() as tape: yhat= X@beta_hat loss= (y-yhat).T @ (y-yhat) / N slope = tape.gradient(loss,beta_hat) beta_hat.assign_sub(alpha*slope) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . ## 포인트코드: 미분 slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) . - 풀이 . y=y.reshape(-1) y.shape,x.shape . (TensorShape([200]), TensorShape([200])) . beta0_hat = tf.Variable(-5.0) beta1_hat = tf.Variable(10.0) . alpha=0.1 . for epoc in range(1000): with tf.GradientTape() as tape: yhat= beta0_hat + x*beta1_hat loss= tf.reduce_sum((y-yhat)**2)/N #loss= sum((y-yhat)**2)/N slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) beta0_hat.assign_sub(alpha*slope0) beta1_hat.assign_sub(alpha*slope1) . beta0_hat,beta1_hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.58366&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.933048&gt;) . GradientTape + opt.apply_gradients . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204; . - 포인트 . ## 포인트코드: 업데이트 opt.apply_gradients([(slope,beta_hat)]) ## pair의 list가 입력 . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): with tf.GradientTape() as tape: yhat= X@beta_hat loss= (y-yhat).T @ (y-yhat) / N slope = tape.gradient(loss,beta_hat) opt.apply_gradients([(slope,beta_hat)]) #beta_hat.assign_sub(alpha*slope) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204; . - 포인트 . ## 포인트코드: 업데이트 opt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)]) ## pair의 list가 입력 . - 풀이 . y=y.reshape(-1) y.shape,x.shape . (TensorShape([200]), TensorShape([200])) . beta0_hat = tf.Variable(-5.0) beta1_hat = tf.Variable(10.0) . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): with tf.GradientTape() as tape: yhat= beta0_hat + beta1_hat*x #X@beta_hat loss= tf.reduce_sum((y-yhat)**2) / N slope0,slope1 = tape.gradient(loss,[beta0_hat,beta1_hat]) opt.apply_gradients([(slope0,beta0_hat),(slope1,beta1_hat)]) . beta0_hat,beta1_hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.58366&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.933048&gt;) . opt.minimize . &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; with lambda . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . loss_fn = lambda: (y-X@beta_hat).T @ (y-X@beta_hat) / N . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;2: &#49828;&#52860;&#46972;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; with lambda . - 포인트 . ## 포인트코드: 미분 &amp; 업데이트 = minimize opt.minimize(loss_fn,[beta0_hat,beta1_hat]) . - 풀이 . y=y.reshape(-1) y.shape,x.shape . (TensorShape([200]), TensorShape([200])) . beta0_hat = tf.Variable(-5.0) beta1_hat = tf.Variable(10.0) . loss_fn = lambda: tf.reduce_sum((y-beta0_hat-beta1_hat*x )**2) / N . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): opt.minimize(loss_fn,[beta0_hat,beta1_hat]) . beta0_hat,beta1_hat . (&lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=2.58366&gt;, &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=3.933048&gt;) . &#54400;&#51060;3: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; (&#51687;&#51008;) &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드: 손실함수정의 def loss_fn(): return ?? . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . def loss_fn(): return (y-X@beta_hat).T @ (y-X@beta_hat) / N . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;4: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; (&#44596;) &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드: 손실함수정의 def loss_fn(): ?? ?? return ?? . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . def loss_fn(): yhat= X@beta_hat # 컴퓨터한테 전달할 수식1 loss = (y-yhat).T @ (y-yhat) / N # 컴퓨터한테 전달할 수식 2 return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;5: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; &lt;- tf.losses.MSE . - 포인트 . ## 포인트코드: 미리구현되어있는 손실함수 이용 tf.losses.MSE(y,yhat) . - 풀이 . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . def loss_fn(): yhat= X@beta_hat # 컴퓨터한테 전달할 수식1 loss = tf.keras.losses.MSE(y.reshape(-1),yhat.reshape(-1)) # 컴퓨터한테 전달할 수식 2 return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . &#54400;&#51060;6: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; &lt;- tf.losses.MeaSquaredError . - 포인트 . ## 포인트코드: 클래스로부터 손실함수 오브젝트 생성 (함수를 찍어내는 클래스) mse_fn = tf.losses.MeanSquaredError() mse_fn(y,yhat) . - 풀이 . mseloss_fn = tf.losses.MeanSquaredError() . mseloss_fn = tf.keras.losses.MSE 라고 보면된다. | . y=y.reshape(N,1) y.shape,X.shape . (TensorShape([200, 1]), TensorShape([200, 2])) . beta_hat = tf.Variable(tnp.array([-5.0,10.0]).reshape(2,1)) beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[-5.], [10.]])&gt; . def loss_fn(): yhat= X@beta_hat # 컴퓨터한테 전달할 수식1 loss = mseloss_fn(y.reshape(-1),yhat.reshape(-1)) # 컴퓨터한테 전달할 수식 2 return loss # tape.gradient(loss,beta_hat) 에서의 미분당하는애 . alpha=0.1 opt = tf.optimizers.SGD(alpha) . for epoc in range(1000): opt.minimize(loss_fn,beta_hat) . beta_hat . &lt;tf.Variable &#39;Variable:0&#39; shape=(2, 1) dtype=float64, numpy= array([[2.58366061], [3.93304684]])&gt; . tf.keras.Sequential . - $ hat{y}_i= hat{ beta}_0+ hat{ beta}_1x_i$ 의 서로다른 표현 . import graphviz def gv(s): return graphviz.Source(&#39;digraph G{ rankdir=&quot;LR&quot;&#39;+s + &#39;; }&#39;) . gv(&#39;&#39;&#39; &quot;1&quot; -&gt; &quot;beta0_hat + x*beta1_hat, bias=False&quot;[label=&quot;* beta0_hat&quot;] &quot;x&quot; -&gt; &quot;beta0_hat + x*beta1_hat, bias=False&quot;[label=&quot;* beta1_hat&quot;] &quot;beta0_hat + x*beta1_hat, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G 1 1 beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False 1&#45;&gt;beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False * beta0_hat yhat yhat beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False&#45;&gt;yhat indentity x x x&#45;&gt;beta0_hat + x*beta1_hat, &#160;&#160;&#160;bias=False * beta1_hat gv(&#39;&#39;&#39; &quot;x&quot; -&gt; &quot;x*beta1_hat, bias=True&quot;[label=&quot;*beta1_hat&quot;] ; &quot;x*beta1_hat, bias=True&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G x x x*beta1_hat, &#160;&#160;&#160;bias=True x*beta1_hat, &#160;&#160;&#160;bias=True x&#45;&gt;x*beta1_hat, &#160;&#160;&#160;bias=True *beta1_hat yhat yhat x*beta1_hat, &#160;&#160;&#160;bias=True&#45;&gt;yhat indentity gv(&#39;&#39;&#39; &quot;X=[1 x]&quot; -&gt; &quot;X@beta_hat, bias=False&quot;[label=&quot;@beta_hat&quot;] ; &quot;X@beta_hat, bias=False&quot; -&gt; &quot;yhat&quot;[label=&quot;indentity&quot;] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G X=[1 x] X=[1 x] X@beta_hat, &#160;&#160;&#160;bias=False X@beta_hat, &#160;&#160;&#160;bias=False X=[1 x]&#45;&gt;X@beta_hat, &#160;&#160;&#160;bias=False @beta_hat yhat yhat X@beta_hat, &#160;&#160;&#160;bias=False&#45;&gt;yhat indentity &#54400;&#51060;1: &#48289;&#53552;&#48260;&#51204;, &#49324;&#50857;&#51088;&#51221;&#51032; &#49552;&#49892;&#54632;&#49688; . - 포인트 . ## 포인트코드1: 네트워크 생성 net = tf.keras.Sequential() ## 포인트코드2: 네트워크의 아키텍처 설계 net.add(tf.keras.layers.Dense(1,input_shape=(2,),use_bias=False)) ## 포인트코드3: 네트워크 컴파일 = 아키텍처 + 손실함수 + 옵티마이저 net.compile(opt,loss=loss_fn2) ## 포인트코드4: 미분 &amp; update net.fit(X,y,epochs=1000,verbose=0,batch_size=N) . - 풀이 . net = tf.keras.Sequential() . net.add(tf.keras.layers.Dense(units=1,input_shape=(2,),use_bias=False)) ## yhat을 구하는 방법정의 = 아키텍처가 설계 . units는 layer의 출력의 차원, 이 경우는 yhat의 차원, yhat은 (200,1) 이므로 1임. | input_shape는 layer의 입력의 차원, 이 경우는 X의 차원, X는 (200,2) 이므로 2임. | . def loss_fn2(y,yhat): return (y-yhat).T @ (y-yhat) / N . alpha=0.1 opt =tf.optimizers.SGD(alpha) . [np.array([[-5.0],[10.0]],dtype=np.float32)] . [array([[-5.], [10.]], dtype=float32)] . net.set_weights([np.array([[-5.0],[10.0]],dtype=np.float32)]) . net.weights . [&lt;tf.Variable &#39;dense_5/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[-5.], [10.]], dtype=float32)&gt;] . net.compile(opt,loss=tf.losses.MSE) # 아키텍처 + 손실함수 + 옵티마이저 =&gt; 네트워크에 다 합치자 =&gt; 네트워크를 컴파일한다. . net.fit(X,y,epochs=1000,batch_size=N,verbose=0) # 미분 + 파라메터업데이트 = net.fit . &lt;keras.callbacks.History at 0x7f6366237640&gt; . net.weights . [&lt;tf.Variable &#39;dense_5/kernel:0&#39; shape=(2, 1) dtype=float32, numpy= array([[2.58366 ], [3.933048]], dtype=float32)&gt;] .",
            "url": "https://guebin.github.io/STBDA2022/2022/04/04/(5%EC%A3%BC%EC%B0%A8)-4%EC%9B%944%EC%9D%BC.html",
            "relUrl": "/2022/04/04/(5%EC%A3%BC%EC%B0%A8)-4%EC%9B%944%EC%9D%BC.html",
            "date": " • Apr 4, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "(4주차) 3월28일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import numpy as np import matplotlib.pyplot as plt . import tensorflow.experimental.numpy as tnp . tnp.experimental_enable_numpy_behavior() . &#48120;&#48516; . tf.GradientTape() &#49324;&#50857;&#48169;&#48277; . - 예제9: 카페예제로 돌아오자. . x= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) . 2022-03-28 22:00:22.099906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero . tf.random.set_seed(43052) epsilon=tf.random.normal([10]) y=10.2 + 2.2*x + epsilon . y . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 , 63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286])&gt; . beta0 = tf.Variable(9.0) beta1 = tf.Variable(2.0) . with tf.GradientTape(persistent=True) as tape: loss = sum((y-beta0-beta1*x)**2) . tape.gradient(loss,beta0), tape.gradient(loss,beta1) . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=-126.78691&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=-3208.8396&gt;) . - 예제10: 카페예제의 매트릭스 버전 . X= tnp.array([1]*10 +[20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]).reshape(2,10).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float64, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]])&gt; . beta= tnp.array([9.0,2.0]).reshape(2,1) beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[9.], [2.]])&gt; . X@beta . &lt;tf.Tensor: shape=(10, 1), dtype=float64, numpy= array([[49.2], [53.4], [54.4], [55.6], [57.8], [59.2], [61.4], [63.6], [65.8], [69.8]])&gt; . beta_true= tnp.array([10.2,2.2]).reshape(2,1) y= X@beta_true+epsilon.reshape(10,1) y . &lt;tf.Tensor: shape=(10, 1), dtype=float64, numpy= array([[55.4183651 ], [58.19427589], [61.23082496], [62.31255873], [63.1070028 ], [63.69569103], [67.24704918], [71.43650092], [73.10130336], [77.84988286]])&gt; . with tf.GradientTape(persistent=True) as tape: tape.watch(beta) yhat= X@beta loss= (y-yhat).T @(y-yhat) . tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -126.78690968], [-3208.83947922]])&gt; . - 이론적인 값을 확인하면 . -2*X.T @ y + 2*X.T@X@beta . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -126.78690968], [-3208.83947922]])&gt; . - 예제11: 위의 예제에서 이론적인 $ boldsymbol{ beta}$의 최적값을 찾아보고 (즉 $ boldsymbol{ hat beta}$을 찾고) 그 지점에서 loss의 미분값(=접선의 기울기)를 구하라. 결과가 $ bf{0}$인지 확인하라. (단 ${ bf 0}$은 길이가 2이고 각 원소가 0인 벡터) . $ beta$의 최적값은 $(X&#39;X)^{-1}X&#39;y$이다. . beta_optimal = tf.linalg.inv(X.T @ X) @ X.T @y . with tf.GradientTape(persistent=True) as tape: tape.watch(beta_optimal) yhat= X@beta_optimal loss= (y-yhat).T @(y-yhat) . tape.gradient(loss,beta_optimal) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[-6.67910172e-12], [-1.67774636e-10]])&gt; . - beta_true에서의 기울기도 계산해보자. . with tf.GradientTape(persistent=True) as tape: tape.watch(beta_true) yhat= X@beta_true loss= (y-yhat).T @(y-yhat) . tape.gradient(loss,beta_true) . &lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy= array([[ -2.74690968], [-71.45947922]])&gt; . 샘플사이즈가 커진다면 tape.gradient(loss,beta_true) $ approx$ tape.gradient(loss,beta_optimal) | 샘플사이즈가 커진다면 beta_true $ approx$ beta_optimal | . &#44221;&#49324;&#54616;&#44053;&#48277; . &#52572;&#51201;&#54868;&#47928;&#51228; . - $loss=( frac{1}{2} beta-1)^2$를 최소하는 $ beta$를 컴퓨터를 활용하여 구하는 문제를 생각해보자. . 답은 $ beta=2$임을 알고 있다. | . &#48169;&#48277;1: grid search . &#50508;&#44256;&#47532;&#51608; . (1) beta = [-10.00,-9.99,...,10.00] 와 같은 리스트를 만든다. . (2) (1)의 리스트의 각원소에 해당하는 loss를 구한다. . (3) (2)에서 구한 loss를 제일 작게 만드는 beta를 찾는다. . &#44396;&#54788;&#53076;&#46300; . beta = np.linspace(-10,10,100) loss = (beta/2 -1)**2 . tnp.argmin([1,2,-3,3,4]) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=2&gt; . tnp.argmin([1,2,3,-3,4]) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=3&gt; . tnp.argmin(loss) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=59&gt; . beta[59] . 1.9191919191919187 . &#44536;&#47532;&#46300;&#49436;&#52824;&#51032; &#47928;&#51228;&#51216; . - 비판1: [-10,10]이외에 해가 존재하면? . 이 예제의 경우는 운좋게 [-10,10]에서 해가 존재했음 | 하지만 임의의 고정된 $x,y$에 대하여 $loss( beta)=(x beta-y)^2$ 의 형태의 해가 항상 [-10,10]에서 존재한다는 보장은 없음 | 해결책: 더 넓게 많은 범위를 탐색하자? | . - 비판2: 효율적이지 않음 . 알고리즘을 요약하면 결국 -10부터 10까지 작은 간격으로 조금씩 이동하며 loss를 조사하는 것이 grid search의 아이디어 | $ to$ 생각해보니까 $ beta=2$인 순간 $loss=( frac{1}{2} beta-1)^2=0$이 되어서 이것보다 작은 최소값은 존재하지 않는다(제곱은 항상 양수이어야 하므로) | $ to$ 따라서 $ beta=2$ 이후로는 탐색할 필요가 없다 | . &#48169;&#48277;2: gradient descent . &#50508;&#44256;&#47532;&#51608;! . (1) beta = -5 로 셋팅한다. . (-5/2-1)**2 . 12.25 . (2) beta=-5 근처에서 조금씩 이동하여 loss를 조사해본다. . (-4.99/2-1)**2 ## 오른쪽으로 0.01 이동하고 loss조사 . 12.215025 . (-5.01/2-1)**2 ## 왼쪽으로 0.01 이동하고 l`oss조사 . 12.285025 . (3) (2)의 결과를 잘 해석하고 더 유리한 쪽으로 이동 . (4) 위의 과정을 반복하고 왼쪽, 오른쪽 어느쪽으로 움직여도 이득이 없다면 멈춘다. . &#50508;&#44256;&#47532;&#51608; &#48516;&#49437; . - (2)-(3)의 과정은 beta=-5 에서 미분계수를 구하고 미분계수가 양수이면 왼쪽으로 움직이고 음수이면 오른쪽으로 움직인다고 해석가능. 아래그림을 보면 더 잘 이해가 된다. . plt.plot(beta,loss) . [&lt;matplotlib.lines.Line2D at 0x7efc781b6bf0&gt;] . &#50812;&#51901;/&#50724;&#47480;&#51901;&#51473;&#50640; &#50612;&#46356;&#47196; &#44040;&#51648; &#50612;&#46523;&#44172; &#54032;&#45800;&#54616;&#45716; &#44284;&#51221;&#51012; &#49688;&#49885;&#54868;? . - 아래와 같이 해석가능 . 오른쪽으로 0.01 간다 = beta_old에 0.01을 더함. (if, 미분계수가 음수) | 왼쪽으로 0.01 간다. = beta_old에 0.01을 뺀다. (if, 미분계수가 양수) | . - 그렇다면 $ beta_{new} = begin{cases} beta_{old} + 0.01, &amp; loss&#39;( beta_{old})&lt; 0 beta_{old} - 0.01, &amp; loss&#39;( beta_{old})&gt; 0 end{cases} $ . &#54841;&#49884; &#50508;&#44256;&#47532;&#51608;&#51012; &#51328; &#44060;&#49440;&#54624;&#49688; &#51080;&#51012;&#44620;? . - 항상 0.01씩 움직여야 하는가? . plt.plot(beta,loss) . [&lt;matplotlib.lines.Line2D at 0x7efc780bd4b0&gt;] . - $ beta=-10$ 일 경우의 접선의 기울기? $ beta=-4$ 일때 접선의 기울기? . $ beta=-10$ =&gt; 기울기는 -6 | $ beta=-4$ =&gt; 기울기는 -3 | . - 실제로 6,3씩 이동할순 없으니 적당한 $ alpha$ (예를들면 $ alpha=0.01$) 를 잡아서 곱한만큼 이동하자. . - 수식화하면 . $ beta_{new} = beta_{old} - alpha~ loss&#39;( beta_{old})$ | $ beta_{new} = beta_{old} - alpha~ left[ frac{ partial}{ partial beta }loss( beta) right]_{ beta= beta_{old}}$ | . - $ alpha$의 의미 . $ alpha$가 크면 크게크게 움직이고 작으면 작게작게 움직인다. | $ alpha&gt;0$ 이어야 한다. | . &#44396;&#54788;&#53076;&#46300; . - iter 1 . $ beta=-10$이라고 하자. . beta = tf.Variable(-10.0) . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 . tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-6.0&gt; . $ beta = -10$ 에서 0.01만큼 움직이고 싶음 . alpha= 0.01/6 . alpha * tape.gradient(loss,beta) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-0.01&gt; . beta.assign_sub(alpha * tape.gradient(loss,beta)) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.99&gt; . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.99&gt; . - iter2 . with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 . beta.assign_sub(tape.gradient(loss,beta)*alpha) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.980008&gt; . - for 문을 이용하자. . (강의용) . beta = tf.Variable(-10.0) . for k in range(10000): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(tape.gradient(loss,beta)*alpha) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.997125&gt; . (시도1) . beta = tf.Variable(-10.0) . for k in range(100): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(tape.gradient(loss,beta)*alpha) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-9.040152&gt; . (시도2) . beta = tf.Variable(-10.0) . for k in range(1000): with tf.GradientTape(persistent=True) as tape: loss = (beta/2-1)**2 beta.assign_sub(tape.gradient(loss,beta)*alpha) . beta . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-3.2133687&gt; . - 너무 느린 것 같다? $ to$ $ alpha$를 키워보자! . &#54617;&#49845;&#47456; . - 목표: $ alpha$에 따라서 수렴과정이 어떻게 달라지는 시각화해보자. . [&#49884;&#44033;&#54868; &#53076;&#46300; &#50696;&#48708;&#54617;&#49845;] . fig = plt.figure() # 도화지가 만들어지고 fig라는 이름을 붙인다. . &lt;Figure size 432x288 with 0 Axes&gt; . ax = fig.add_subplot() # fig는 ax라는 물체를 만든다. . id(fig.axes[0]) . 139622811236624 . id(ax) . 139622811236624 . pnts, = ax.plot([1,2,3],[4,5,6],&#39;or&#39;) pnts . &lt;matplotlib.lines.Line2D at 0x7efc7071da20&gt; . pnts.get_xdata() . array([1, 2, 3]) . pnts.get_ydata() . array([4, 5, 6]) . fig . pnts.set_ydata([5,5,5]) . pnts.get_ydata() . [5, 5, 5] . fig . - 응용 . plt.rcParams[&quot;animation.html&quot;]=&quot;jshtml&quot; from matplotlib import animation . def animate(i): if i%2 == 0: pnts.set_ydata([4,5,6]) else: pnts.set_ydata([5,5,5]) . ani = animation.FuncAnimation(fig,animate,frames=10) ani . &lt;/input&gt; Once Loop Reflect 예비학습 끝 . - beta_lst=[-10,-9,-8] 로 이동한다고 하자. . beta_lst = [-10,-9,-8] loss_lst = [(-10/2-1)**2,(-9/2-1)**2,(-8/2-1)**2] . fig = plt.figure() . &lt;Figure size 432x288 with 0 Axes&gt; . ax= fig.add_subplot() . _beta = np.linspace(-15,19,100) . ax.plot(_beta,(_beta/2-1)**2) . [&lt;matplotlib.lines.Line2D at 0x7efc707d3970&gt;] . fig . pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;ro&#39;) fig . def animate(i): pnts.set_xdata(beta_lst[:(i+1)]) pnts.set_ydata(loss_lst[:(i+1)]) . ani =animation.FuncAnimation(fig, animate, frames=3) ani . &lt;/input&gt; Once Loop Reflect - 최종아웃풋 . beta = tf.Variable(-10.0) alpha = 0.01/6 . beta_lst=[] loss_lst=[] . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . with tf.GradientTape(persistent=True) as tape: tape.watch(beta) loss = (beta/2-1)**2 . beta.assign_sub(tape.gradient(loss,beta)*alpha) . &lt;tf.Variable &#39;UnreadVariable&#39; shape=() dtype=float32, numpy=-9.99&gt; . beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . beta_lst, loss_lst . ([-10.0, -9.99], [36.0, 35.94002362785341]) . - for . beta = tf.Variable(-10.0) alpha = 0.01/6 beta_lst=[] loss_lst=[] beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) for k in range(100): with tf.GradientTape(persistent=True) as tape: tape.watch(beta) loss = (beta/2-1)**2 beta.assign_sub(tape.gradient(loss,beta)*alpha) beta_lst.append(beta.numpy()) loss_lst.append((beta.numpy()/2-1)**2) . fig = plt.figure() ax = fig.add_subplot() ax.plot(_beta,(_beta/2-1)**2) pnts, = ax.plot(beta_lst[0],loss_lst[0],&#39;or&#39;) . ani = animation.FuncAnimation(fig,animate,frames=100) ani . &lt;/input&gt; Once Loop Reflect &#49689;&#51228; . $y=(x-1)^2$를 최소화 하는 $x$를 경사하강법을 이용하여 찾아라. 수렴과정을 animation으로 시각화하라. . x의 초기값은 -3으로 설정한다. | 적당한 $ alpha$를 골라서 100번의 반복안에 수렴하도록 하라. | .",
            "url": "https://guebin.github.io/STBDA2022/2022/03/28/(4%EC%A3%BC%EC%B0%A8)-3%EC%9B%9428%EC%9D%BC.html",
            "relUrl": "/2022/03/28/(4%EC%A3%BC%EC%B0%A8)-3%EC%9B%9428%EC%9D%BC.html",
            "date": " • Mar 28, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "(3주차) 3월21일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . imports . import tensorflow as tf import numpy as np . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . &#51648;&#45212;&#44053;&#51032; &#48372;&#52649; . - max, min, sum, mean . a= tf.constant([1.0,2.0,3.0,4.0]) a . &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt; . tf.reduce_mean(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.5&gt; . concat, stack . - 예제: (2,3,4,5) stack (2,3,4,5) -&gt; (?,?,?,?,?) . a = tf.reshape(tf.constant(range(2*3*4*5)),(2,3,4,5)) b = -a . case1 (1,2,3,4,5) stack (1,2,3,4,5) --&gt; (2,2,3,4,5) # axis=0 . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]]], [[[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . case2 (2,1,3,4,5) stack (2,1,3,4,5) --&gt; (2,2,3,4,5) # axis=1 . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2, 3, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]]], [[[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . case3 (2,3,1,4,5) stack (2,3,1,4,5) --&gt; (2,3,2,4,5) # axis=2 . tf.stack([a,b],axis=2) . &lt;tf.Tensor: shape=(2, 3, 2, 4, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]]], [[[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]]], [[[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]]], [[[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]]], [[[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]]], [[[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . case4 (2,3,4,1,5) stack (2,3,4,1,5) --&gt; (2,3,4,2,5) # axis=3 . tf.stack([a,b],axis=-2) . &lt;tf.Tensor: shape=(2, 3, 4, 2, 5), dtype=int32, numpy= array([[[[[ 0, 1, 2, 3, 4], [ 0, -1, -2, -3, -4]], [[ 5, 6, 7, 8, 9], [ -5, -6, -7, -8, -9]], [[ 10, 11, 12, 13, 14], [ -10, -11, -12, -13, -14]], [[ 15, 16, 17, 18, 19], [ -15, -16, -17, -18, -19]]], [[[ 20, 21, 22, 23, 24], [ -20, -21, -22, -23, -24]], [[ 25, 26, 27, 28, 29], [ -25, -26, -27, -28, -29]], [[ 30, 31, 32, 33, 34], [ -30, -31, -32, -33, -34]], [[ 35, 36, 37, 38, 39], [ -35, -36, -37, -38, -39]]], [[[ 40, 41, 42, 43, 44], [ -40, -41, -42, -43, -44]], [[ 45, 46, 47, 48, 49], [ -45, -46, -47, -48, -49]], [[ 50, 51, 52, 53, 54], [ -50, -51, -52, -53, -54]], [[ 55, 56, 57, 58, 59], [ -55, -56, -57, -58, -59]]]], [[[[ 60, 61, 62, 63, 64], [ -60, -61, -62, -63, -64]], [[ 65, 66, 67, 68, 69], [ -65, -66, -67, -68, -69]], [[ 70, 71, 72, 73, 74], [ -70, -71, -72, -73, -74]], [[ 75, 76, 77, 78, 79], [ -75, -76, -77, -78, -79]]], [[[ 80, 81, 82, 83, 84], [ -80, -81, -82, -83, -84]], [[ 85, 86, 87, 88, 89], [ -85, -86, -87, -88, -89]], [[ 90, 91, 92, 93, 94], [ -90, -91, -92, -93, -94]], [[ 95, 96, 97, 98, 99], [ -95, -96, -97, -98, -99]]], [[[ 100, 101, 102, 103, 104], [-100, -101, -102, -103, -104]], [[ 105, 106, 107, 108, 109], [-105, -106, -107, -108, -109]], [[ 110, 111, 112, 113, 114], [-110, -111, -112, -113, -114]], [[ 115, 116, 117, 118, 119], [-115, -116, -117, -118, -119]]]]], dtype=int32)&gt; . case5 (2,3,4,5,1) stack (2,3,4,5,1) --&gt; (2,3,4,5,2) # axis=4 . tf.stack([a,b],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 4, 5, 2), dtype=int32, numpy= array([[[[[ 0, 0], [ 1, -1], [ 2, -2], [ 3, -3], [ 4, -4]], [[ 5, -5], [ 6, -6], [ 7, -7], [ 8, -8], [ 9, -9]], [[ 10, -10], [ 11, -11], [ 12, -12], [ 13, -13], [ 14, -14]], [[ 15, -15], [ 16, -16], [ 17, -17], [ 18, -18], [ 19, -19]]], [[[ 20, -20], [ 21, -21], [ 22, -22], [ 23, -23], [ 24, -24]], [[ 25, -25], [ 26, -26], [ 27, -27], [ 28, -28], [ 29, -29]], [[ 30, -30], [ 31, -31], [ 32, -32], [ 33, -33], [ 34, -34]], [[ 35, -35], [ 36, -36], [ 37, -37], [ 38, -38], [ 39, -39]]], [[[ 40, -40], [ 41, -41], [ 42, -42], [ 43, -43], [ 44, -44]], [[ 45, -45], [ 46, -46], [ 47, -47], [ 48, -48], [ 49, -49]], [[ 50, -50], [ 51, -51], [ 52, -52], [ 53, -53], [ 54, -54]], [[ 55, -55], [ 56, -56], [ 57, -57], [ 58, -58], [ 59, -59]]]], [[[[ 60, -60], [ 61, -61], [ 62, -62], [ 63, -63], [ 64, -64]], [[ 65, -65], [ 66, -66], [ 67, -67], [ 68, -68], [ 69, -69]], [[ 70, -70], [ 71, -71], [ 72, -72], [ 73, -73], [ 74, -74]], [[ 75, -75], [ 76, -76], [ 77, -77], [ 78, -78], [ 79, -79]]], [[[ 80, -80], [ 81, -81], [ 82, -82], [ 83, -83], [ 84, -84]], [[ 85, -85], [ 86, -86], [ 87, -87], [ 88, -88], [ 89, -89]], [[ 90, -90], [ 91, -91], [ 92, -92], [ 93, -93], [ 94, -94]], [[ 95, -95], [ 96, -96], [ 97, -97], [ 98, -98], [ 99, -99]]], [[[ 100, -100], [ 101, -101], [ 102, -102], [ 103, -103], [ 104, -104]], [[ 105, -105], [ 106, -106], [ 107, -107], [ 108, -108], [ 109, -109]], [[ 110, -110], [ 111, -111], [ 112, -112], [ 113, -113], [ 114, -114]], [[ 115, -115], [ 116, -116], [ 117, -117], [ 118, -118], [ 119, -119]]]]], dtype=int32)&gt; . - 예제: (2,3,4), (2,3,4), (2,3,4) . a= tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b= -a c= 2*a . (예시1) (2,3,4), (2,3,4), (2,3,4) $ to$ (6,3,4) . tf.concat([a,b,c],axis=0) . &lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]], dtype=int32)&gt; . (예시2) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,9,4) . tf.concat([a,b,c],axis=1) . &lt;tf.Tensor: shape=(2, 9, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11], [ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23], [-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23], [ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]], dtype=int32)&gt; . (예시3) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,12) . tf.concat([a,b,c],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 12), dtype=int32, numpy= array([[[ 0, 1, 2, 3, 0, -1, -2, -3, 0, 2, 4, 6], [ 4, 5, 6, 7, -4, -5, -6, -7, 8, 10, 12, 14], [ 8, 9, 10, 11, -8, -9, -10, -11, 16, 18, 20, 22]], [[ 12, 13, 14, 15, -12, -13, -14, -15, 24, 26, 28, 30], [ 16, 17, 18, 19, -16, -17, -18, -19, 32, 34, 36, 38], [ 20, 21, 22, 23, -20, -21, -22, -23, 40, 42, 44, 46]]], dtype=int32)&gt; . (예시4) (2,3,4), (2,3,4), (2,3,4) $ to$ (3,2,3,4) . tf.stack([a,b,c],axis=0) . &lt;tf.Tensor: shape=(3, 2, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]]], [[[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]]], [[[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시5) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,3,4) . tf.stack([a,b,c],axis=1) . &lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[ 0, 2, 4, 6], [ 8, 10, 12, 14], [ 16, 18, 20, 22]]], [[[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[ 24, 26, 28, 30], [ 32, 34, 36, 38], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시6) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,3,4) . tf.stack([a,b,c],axis=2) . &lt;tf.Tensor: shape=(2, 3, 3, 4), dtype=int32, numpy= array([[[[ 0, 1, 2, 3], [ 0, -1, -2, -3], [ 0, 2, 4, 6]], [[ 4, 5, 6, 7], [ -4, -5, -6, -7], [ 8, 10, 12, 14]], [[ 8, 9, 10, 11], [ -8, -9, -10, -11], [ 16, 18, 20, 22]]], [[[ 12, 13, 14, 15], [-12, -13, -14, -15], [ 24, 26, 28, 30]], [[ 16, 17, 18, 19], [-16, -17, -18, -19], [ 32, 34, 36, 38]], [[ 20, 21, 22, 23], [-20, -21, -22, -23], [ 40, 42, 44, 46]]]], dtype=int32)&gt; . (예시7) (2,3,4), (2,3,4), (2,3,4) $ to$ (2,3,4,3) . tf.stack([a,b,c],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 4, 3), dtype=int32, numpy= array([[[[ 0, 0, 0], [ 1, -1, 2], [ 2, -2, 4], [ 3, -3, 6]], [[ 4, -4, 8], [ 5, -5, 10], [ 6, -6, 12], [ 7, -7, 14]], [[ 8, -8, 16], [ 9, -9, 18], [ 10, -10, 20], [ 11, -11, 22]]], [[[ 12, -12, 24], [ 13, -13, 26], [ 14, -14, 28], [ 15, -15, 30]], [[ 16, -16, 32], [ 17, -17, 34], [ 18, -18, 36], [ 19, -19, 38]], [[ 20, -20, 40], [ 21, -21, 42], [ 22, -22, 44], [ 23, -23, 46]]]], dtype=int32)&gt; . - 예제: (2,3,4) (4,3,4) $ to$ (6,3,4) . a=tf.reshape(tf.constant(range(2*3*4)),(2,3,4)) b=tf.reshape(-tf.constant(range(4*3*4)),(4,3,4)) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(6, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]], [[ 0, -1, -2, -3], [ -4, -5, -6, -7], [ -8, -9, -10, -11]], [[-12, -13, -14, -15], [-16, -17, -18, -19], [-20, -21, -22, -23]], [[-24, -25, -26, -27], [-28, -29, -30, -31], [-32, -33, -34, -35]], [[-36, -37, -38, -39], [-40, -41, -42, -43], [-44, -45, -46, -47]]], dtype=int32)&gt; . tf.concat([a,b],axis=1) . InvalidArgumentError Traceback (most recent call last) Input In [22], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=1) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . tf.concat([a,b],axis=2) . InvalidArgumentError Traceback (most recent call last) Input In [23], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=2) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Dimensions of inputs should match: shape[0] = [2,3,4] vs. shape[1] = [4,3,4] [Op:ConcatV2] name: concat . - (2,2) @ (2,) 의 연산? . numpy . np.array([[1,0],[0,1]]) @ np.array([77,-88]) . array([ 77, -88]) . np.array([77,-88]) @ np.array([[1,0],[0,1]]) . array([ 77, -88]) . np.array([[1,0],[0,1]]) @ np.array([77,-88]).reshape(2,1) . array([[ 77], [-88]]) . np.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]]) . ValueError Traceback (most recent call last) Input In [27], in &lt;cell line: 1&gt;() -&gt; 1 np.array([77,-88]).reshape(2,1) @ np.array([[1,0],[0,1]]) ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 1) . np.array([77,-88]).reshape(1,2) @ np.array([[1,0],[0,1]]) . array([[ 77, -88]]) . tensorflow . I = tf.constant([[1.0,0.0],[0.0,1.0]]) x = tf.constant([77.0,-88.0]) . I @ x . InvalidArgumentError Traceback (most recent call last) Input In [30], in &lt;cell line: 1&gt;() -&gt; 1 I @ x File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: In[0] and In[1] has different ndims: [2,2] vs. [2] [Op:MatMul] . x @ I . InvalidArgumentError Traceback (most recent call last) Input In [31], in &lt;cell line: 1&gt;() -&gt; 1 x @ I File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: In[0] and In[1] has different ndims: [2] vs. [2,2] [Op:MatMul] . I @ tf.reshape(x,(2,1)) . &lt;tf.Tensor: shape=(2, 1), dtype=float32, numpy= array([[ 77.], [-88.]], dtype=float32)&gt; . tf.reshape(x,(1,2)) @ I . &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 77., -88.]], dtype=float32)&gt; . . tf.Variable . &#49440;&#50616; . - tf.Variable()로 선언 . tf.Variable([1,2,3,4]) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . tf.Variable([1.0,2.0,3.0,4.0]) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=float32, numpy=array([1., 2., 3., 4.], dtype=float32)&gt; . - tf.constant() 선언후 변환 . tf.Variable(tf.constant([1,2,3,4])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . - np 등으로 선언후 변환 . tf.Variable(np.array([1,2,3,4])) . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int64, numpy=array([1, 2, 3, 4])&gt; . &#53440;&#51077; . type(tf.Variable([1,2,3,4])) . tensorflow.python.ops.resource_variable_ops.ResourceVariable . &#51064;&#45937;&#49905; . a=tf.Variable([1,2,3,4]) a . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . a[:2] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . &#50672;&#49328;&#44032;&#45733; . a=tf.Variable([1,2,3,4]) b=tf.Variable([-1,-2,-3,-4]) . a+b . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([0, 0, 0, 0], dtype=int32)&gt; . tf.Variable&#46020; &#50416;&#44592; &#48520;&#54200;&#54632; . tf.Variable([1,2])+tf.Variable([3.14,3.14]) . InvalidArgumentError Traceback (most recent call last) Input In [43], in &lt;cell line: 1&gt;() -&gt; 1 tf.Variable([1,2])+tf.Variable([3.14,3.14]) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/ops/variables.py:1078, in Variable._OverloadOperator.&lt;locals&gt;._run_op(a, *args, **kwargs) 1076 def _run_op(a, *args, **kwargs): 1077 # pylint: disable=protected-access -&gt; 1078 return tensor_oper(a.value(), *args, **kwargs) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2] . tnp&#51032; &#51008;&#52509;&#46020; &#51068;&#48512;&#47564; &#44032;&#45733; . import tensorflow.experimental.numpy as tnp tnp.experimental_enable_numpy_behavior() . - 알아서 형 변환 . tf.Variable([1,2])+tf.Variable([3.14,3.14]) . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([4.1400001, 5.1400001])&gt; . - .reshape 메소드 . tf.Variable([1,2,3,4]).reshape(2,2) . AttributeError Traceback (most recent call last) Input In [46], in &lt;cell line: 1&gt;() -&gt; 1 tf.Variable([1,2,3,4]).reshape(2,2) AttributeError: &#39;ResourceVariable&#39; object has no attribute &#39;reshape&#39; . &#45824;&#48512;&#48516;&#51032; &#46041;&#51089;&#51008; tf.constant&#46993; &#53360; &#52264;&#51060;&#47484; &#47784;&#47476;&#44192;&#51020; . - tf.concat . a= tf.Variable([[1,2],[3,4]]) b= tf.Variable([[-1,-2],[-3,-4]]) tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, 2], [ 3, 4], [-1, -2], [-3, -4]], dtype=int32)&gt; . - tf.stack . a= tf.Variable([[1,2],[3,4]]) b= tf.Variable([[-1,-2],[-3,-4]]) tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2, 2), dtype=int32, numpy= array([[[ 1, 2], [ 3, 4]], [[-1, -2], [-3, -4]]], dtype=int32)&gt; . &#48320;&#49688;&#44050;&#48320;&#44221;&#44032;&#45733;(?) . a= tf.Variable([1,2,3,4]) id(a) . 140652736059120 . a.assign_add([-1,-2,-3,-4]) id(a) . 140652736059120 . &#50836;&#50557; . - tf.Variable()로 만들어야 하는 뚜렷한 차이는 모르겠음. . - 애써 tf.Variable()로 만들어도 간단한연산을 하면 그 결과는 tf.constant()로 만든 오브젝트와 동일해짐. . &#48120;&#48516; . &#47784;&#54000;&#48652; . - 예제: 컴퓨터를 이용하여 $x=2$에서 $y=3x^2$의 접선의 기울기를 구해보자. . (손풀이) . $$ frac{dy}{dx}=6x$$ . 이므로 $x=2$를 대입하면 12이다. . (컴퓨터를 이용한 풀이) . 단계1 . x1=2 y1= 3*x1**2 . x2=2+0.000000001 y2= 3*x2**2 . (y2-y1)/(x2-x1) . 12.0 . 단계2 . def f(x): return(3*x**2) . f(3) . 27 . def d(f,x): return (f(x+0.000000001)-f(x))/0.000000001 . d(f,2) . 12.000000992884452 . 단계3 . d(lambda x: 3*x**2 ,2) . 12.000000992884452 . d(lambda x: x**2 ,0) . 1e-09 . 단계4 . $$f(x,y)= x^2 +3y$$ . def f(x,y): return(x**2 +3*y) . d(f,(2,3)) . TypeError Traceback (most recent call last) Input In [61], in &lt;cell line: 1&gt;() -&gt; 1 d(f,(2,3)) Input In [56], in d(f, x) 1 def d(f,x): -&gt; 2 return (f(x+0.000000001)-f(x))/0.000000001 TypeError: can only concatenate tuple (not &#34;float&#34;) to tuple . tf.GradientTape() &#49324;&#50857;&#48169;&#48277; . - 예제1: $x=2$에서 $y=3x^2$의 도함수값을 구하라. . x=tf.Variable(2.0) a=tf.constant(3.0) . mytape=tf.GradientTape() mytape.__enter__() # 기록 시작 y=a*x**2 # y=ax^2 = 3x^2 mytape.__exit__(None,None,None) # 기록 끝 . mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . - 예제2: 조금 다른예제 . x=tf.Variable(2.0) #a=tf.constant(3.0) mytape=tf.GradientTape() mytape.__enter__() # 기록 시작 a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 mytape.__exit__(None,None,None) # 기록 끝 mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . $$a= frac{3}{2}x$$ $$y=ax^2= frac{3}{2}x^3$$ . $$ frac{dy}{dx}= frac{3}{2} 3x^2$$ . 3/2*3*4 . 18.0 . - 테이프의 개념 ($ star$) . (상황) . 우리가 어려운 미분계산을 컴퓨터에게 부탁하는 상황임. (예를들면 $y=3x^2$) 컴퓨터에게 부탁을 하기 위해서는 연습장(=테이프)에 $y=3x^2$이라는 수식을 써서 보여줘야하는데 이때 컴퓨터에게 target이 무엇인지 그리고 무엇으로 미분하고 싶은 것인지를 명시해야함. . (1) mytape = tf.GradientTape(): tf.GradientTape()는 연습장을 만드는 명령어, 만들어진 연습장을 mytape라고 이름을 붙인다. . (2) mytape.__enter__(): 만들어진 공책을 연다 (=기록할수 있는 상태로 만든다) . (3) a=x/2*3; y=a*x**2: 컴퓨터에게 전달할 수식을 쓴다 . (4) mytape.__exit__(None,None,None): 공책을 닫는다. . (5) mytape.gradient(y,x): $y$를 $x$로 미분하라는 메모를 남기고 컴퓨터에게 전달한다. . - 예제3: 연습장을 언제 열고 닫을지 결정하는건 중요하다. . x=tf.Variable(2.0) a=(x/2)*3 ## a=(3/2)x mytape=tf.GradientTape() mytape.__enter__() # 기록 시작 y=a*x**2 ## y=ax^2 = (3/2)x^3 mytape.__exit__(None,None,None) # 기록 끝 mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . - 예제4: with문과 함께 쓰는 tf.GradientTape() . x=tf.Variable(2.0) a=(x/2)*3 . with tf.GradientTape() as mytape: ## with문 시작 y=a*x**2 ## with문 끝 . mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=12.0&gt; . (문법해설) . 아래와 같이 쓴다. . with expression as myname: ## with문 시작: myname.__enter__() blabla ~ yadiyadi !! ## with문 끝: myname.__exit__() . (1) expression 의 실행결과 오브젝트가 생성, 생성된 오브젝트는 myname라고 이름붙임. 이 오브젝트는 .__enter__()와 .__exit__()를 숨겨진 기능으로 포함해야 한다. . (2) with문이 시작되면서 myname.__enter__()이 실행된다. . (3) 블라블라와 야디야디가 실행된다. . (4) with문이 종료되면서 myname.__exit__()이 실행된다. . - 예제5: 예제2를 with문과 함께 구현 . x=tf.Variable(2.0) with tf.GradientTape() as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 mytape.gradient(y,x) # y를 x로 미분하라. . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 예제6: persistent = True . (관찰1) . x=tf.Variable(2.0) with tf.GradientTape() as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . mytape.gradient(y,x) # 2번이상 실행해서 에러를 관측하라 . RuntimeError Traceback (most recent call last) Input In [90], in &lt;cell line: 1&gt;() -&gt; 1 mytape.gradient(y,x) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1032, in GradientTape.gradient(self, target, sources, output_gradients, unconnected_gradients) 1002 &#34;&#34;&#34;Computes the gradient using operations recorded in context of this tape. 1003 1004 Note: Unless you set `persistent=True` a GradientTape can only be used to (...) 1029 called with an unknown value. 1030 &#34;&#34;&#34; 1031 if self._tape is None: -&gt; 1032 raise RuntimeError(&#34;A non-persistent GradientTape can only be used to &#34; 1033 &#34;compute one set of gradients (or jacobians)&#34;) 1034 if self._recording: 1035 if not self._persistent: RuntimeError: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians) . (관찰2) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True) as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . mytape.gradient(y,x) # 2번이상실행해도 에러가 나지않음 . &lt;tf.Tensor: shape=(), dtype=float32, numpy=18.0&gt; . - 예제7: watch . (관찰1) . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape: a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . None . (관찰2) . x=tf.constant(2.0) with tf.GradientTape(persistent=True) as mytape: mytape.watch(x) # 수동감시 a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . tf.Tensor(18.0, shape=(), dtype=float32) . (관찰3) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제 a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . None . (관찰4) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True,watch_accessed_variables=False) as mytape: # 자동감시 모드 해제 mytape.watch(x) a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . tf.Tensor(18.0, shape=(), dtype=float32) . (관찰5) . x=tf.Variable(2.0) with tf.GradientTape(persistent=True) as mytape: mytape.watch(x) a=(x/2)*3 ## a=(3/2)x y=a*x**2 ## y=ax^2 = (3/2)x^3 . print(mytape.gradient(y,x)) . tf.Tensor(18.0, shape=(), dtype=float32) . - 예제9: 카페예제로 돌아오자. . - 예제10: 카페예제의 매트릭스 버전 . - 예제11: 위의 예제에서 이론적인 $ boldsymbol{ beta}$의 최적값을 찾아보고 (즉 $ hat{ boldsymbol{ beta}}$을 찾고) 그곳에서 loss의 미분을 구하라. 구한결과가 $ begin{bmatrix}0 0 end{bmatrix}$ 임을 확인하라. .",
            "url": "https://guebin.github.io/STBDA2022/2022/03/21/(3%EC%A3%BC%EC%B0%A8)-3%EC%9B%9421%EC%9D%BC.html",
            "relUrl": "/2022/03/21/(3%EC%A3%BC%EC%B0%A8)-3%EC%9B%9421%EC%9D%BC.html",
            "date": " • Mar 21, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "(2주차) 3월14일",
            "content": "&#44053;&#51032;&#45432;&#53944; . . import . import tensorflow as tf import numpy as np . tf.config.experimental.list_physical_devices(&#39;GPU&#39;) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . tf.constant . &#50696;&#48708;&#54617;&#49845;: &#51473;&#52393;&#47532;&#49828;&#53944; . - 리스트 . lst = [1,2,4,5,6] lst . [1, 2, 4, 5, 6] . lst[1] # 두번쨰원소 . 2 . lst[-1] # 마지막원소 . 6 . - (2,2) matrix 느낌의 list . lst= [[1,2],[3,4]] lst . [[1, 2], [3, 4]] . 위를 아래와 같은 매트릭스로 생각할수 있다. . 1 2 3 4 . print(lst[0][0]) # (1,1) print(lst[0][1]) # (1,2) print(lst[1][0]) # (2,1) print(lst[1][1]) # (2,2) . 1 2 3 4 . - (4,1) matrix 느낌의 list . lst=[[1],[2],[3],[4]] # (4,1) matrix = 길이가 4인 col-vector lst . [[1], [2], [3], [4]] . - (1,4) matrix 느낌의 list . lst=[[1,2,3,4]] # (1,4) matrix = 길이가 4인 row-vector lst . [[1, 2, 3, 4]] . &#49440;&#50616; . - 스칼라 . tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt; . tf.constant(3.14)+tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=6.28&gt; . - 벡터 . _vector=tf.constant([1,2,3]) . _vector[-1] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=3&gt; . - 매트릭스 . _matrix= tf.constant([[1,0],[0,1]]) _matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 1]], dtype=int32)&gt; . - array . tf.constant([[[0,1,1],[1,2,-1]],[[0,1,2],[1,2,-1]]]) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 0, 1, 1], [ 1, 2, -1]], [[ 0, 1, 2], [ 1, 2, -1]]], dtype=int32)&gt; . &#53440;&#51077; . type(tf.constant(3.14)) . tensorflow.python.framework.ops.EagerTensor . &#51064;&#45937;&#49905; . _matrix = tf.constant([[1,2],[3,4]]) _matrix . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . _matrix[0][0] . &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt; . _matrix[0] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . _matrix[0,:] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt; . _matrix[:,0] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3], dtype=int32)&gt; . tf.constant&#45716; &#48520;&#54200;&#54616;&#45796;. . - 불편한점 . 모든 원소가 같은 dtype을 가지고 있어야함. | 원소 수정이 불가능함. | 묵시적 형변환이 불가능하다. | - 원소수정이 불가능함 . a=tf.constant([1,22,33]) a . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1, 22, 33], dtype=int32)&gt; . a[0]=11 . TypeError Traceback (most recent call last) Input In [24], in &lt;cell line: 1&gt;() -&gt; 1 a[0]=11 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . - 묵시적 형변환이 불가능하다 . tf.constant(1)+tf.constant(3.14) . InvalidArgumentError Traceback (most recent call last) Input In [25], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant(1)+tf.constant(3.14) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2] . tf.constant(1.0)+tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.1400003&gt; . - 같은 float도 안되는 경우가 있음 . tf.constant(1.0,dtype=tf.float64) . &lt;tf.Tensor: shape=(), dtype=float64, numpy=1.0&gt; . tf.constant(3.14) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt; . tf.constant(1.0,dtype=tf.float64)+tf.constant(3.14) . InvalidArgumentError Traceback (most recent call last) Input In [29], in &lt;cell line: 1&gt;() -&gt; 1 tf.constant(1.0,dtype=tf.float64)+tf.constant(3.14) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:AddV2] . tf.constant $ to$ &#45336;&#54028;&#51060; . np.array(tf.constant(1)) # 방법1 . array(1, dtype=int32) . a=tf.constant([3.14,-3.14]) type(a) . tensorflow.python.framework.ops.EagerTensor . a.numpy() . array([ 3.14, -3.14], dtype=float32) . &#50672;&#49328; . - 더하기 . a=tf.constant([1,2]) b=tf.constant([3,4]) a+b . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . tf.add(a,b) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt; . - 곱하기 . a=tf.constant([[1,2],[3,4]]) b=tf.constant([[5,6],[7,8]]) a*b . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 5, 12], [21, 32]], dtype=int32)&gt; . tf.multiply(a,b) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 5, 12], [21, 32]], dtype=int32)&gt; . - 매트릭스의곱 . a=tf.constant([[1,0],[0,1]]) # (2,2) b=tf.constant([[5],[7]]) # (2,1) a@b . &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[5], [7]], dtype=int32)&gt; . tf.matmul(a,b) . &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[5], [7]], dtype=int32)&gt; . - 역행렬 . a=tf.constant([[1,0],[0,2]]) a . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 0], [0, 2]], dtype=int32)&gt; . tf.linalg.inv(a) . InvalidArgumentError Traceback (most recent call last) Input In [40], in &lt;cell line: 1&gt;() -&gt; 1 tf.linalg.inv(a) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/ops/gen_linalg_ops.py:1506, in matrix_inverse(input, adjoint, name) 1504 return _result 1505 except _core._NotOkStatusException as e: -&gt; 1506 _ops.raise_from_not_ok_status(e, name) 1507 except _core._FallbackException: 1508 pass File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: Value for attr &#39;T&#39; of int32 is not in the list of allowed values: double, float, half, complex64, complex128 ; NodeDef: {{node MatrixInverse}}; Op&lt;name=MatrixInverse; signature=input:T -&gt; output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:MatrixInverse] . a=tf.constant([[1.0,0.0],[0.0,2.0]]) tf.linalg.inv(a) . &lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy= array([[1. , 0. ], [0. , 0.5]], dtype=float32)&gt; . - tf.linalg. + tab을 누르면 좋아보이는 연산들 많음 . a=tf.constant([[1.0,2.0],[3.0,4.0]]) print(a) tf.linalg.det(a) . tf.Tensor( [[1. 2.] [3. 4.]], shape=(2, 2), dtype=float32) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt; . tf.linalg.trace(a) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt; . &#54805;&#53468;&#48320;&#54872; . - 기본: tf.reshape() 를 이용 . a=tf.constant([1,2,3,4]) a . &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt; . tf.reshape(a,(4,1)) . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]], dtype=int32)&gt; . tf.reshape(a,(2,2)) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . tf.reshape(a,(2,2,1)) . &lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy= array([[[1], [2]], [[3], [4]]], dtype=int32)&gt; . - 다차원 . a=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12]) a . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . tf.reshape(a,(2,2,3)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . tf.reshape(a,(4,3)) . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . - tf.resh . a=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12]) a . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . tf.reshape(a,(4,-1)) . &lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy= array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . tf.reshape(a,(2,2,-1)) . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . b=tf.reshape(a,(2,2,-1)) b . &lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy= array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]], dtype=int32)&gt; . tf.reshape(b,-1) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype=int32)&gt; . &#49440;&#50616;&#44256;&#44553; . - 다른 자료형 (리스트나 넘파이)로 만들고 바꾸는것도 좋다. . np.diag([1,2,3,4]) . array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]]) . tf.constant(np.diag([1,2,3,4])) . &lt;tf.Tensor: shape=(4, 4), dtype=int64, numpy= array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]])&gt; . - tf.ones, tf.zeros . tf.zeros([3,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy= array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], dtype=float32)&gt; . tf.reshape(tf.constant([0]*9),(3,3)) . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=int32)&gt; . - range(10) . a=range(0,12) tf.constant(a) . &lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype=int32)&gt; . tf.constant(range(1,20,3)) . &lt;tf.Tensor: shape=(7,), dtype=int32, numpy=array([ 1, 4, 7, 10, 13, 16, 19], dtype=int32)&gt; . - tf.linspace . tf.linspace(0,1,10) . &lt;tf.Tensor: shape=(10,), dtype=float64, numpy= array([0. , 0.11111111, 0.22222222, 0.33333333, 0.44444444, 0.55555556, 0.66666667, 0.77777778, 0.88888889, 1. ])&gt; . tf.concat . - (2,1) concat (2,1) =&gt; (2,2) . 두번째 축이 바뀌었다. =&gt; axis=1 | . a=tf.constant([[1],[2]]) b=tf.constant([[3],[4]]) a,b . (&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[1], [2]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[3], [4]], dtype=int32)&gt;) . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 3], [2, 4]], dtype=int32)&gt; . - (2,1) concat (2,1) =&gt; (4,1) . 첫번째 축이 바뀌었다. =&gt; axis=0 | . a=tf.constant([[1],[2]]) b=tf.constant([[3],[4]]) a,b . (&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[1], [2]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy= array([[3], [4]], dtype=int32)&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy= array([[1], [2], [3], [4]], dtype=int32)&gt; . - (1,2) concat (1,2) =&gt; (2,2) . 첫번째 // axis=0 | . a=tf.constant([[1,2]]) b=tf.constant([[3,4]]) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[1, 2], [3, 4]], dtype=int32)&gt; . - (1,2) concat (1,2) =&gt; (1,4) . 첫번째 // axis=0 | . - (2,3,4,5) concat (2,3,4,5) =&gt; (4,3,4,5) . 첫번째 // axis=0 | . a=tf.reshape(tf.constant(range(120)),(2,3,4,5)) b=-a . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(4, 3, 4, 5), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]]], [[[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]], dtype=int32)&gt; . - (2,3,4,5) concat (2,3,4,5) =&gt; (2,6,4,5) . 두번째 // axis=1 | . a=tf.reshape(tf.constant(range(120)),(2,3,4,5)) b=-a . tf.concat([a,b],axis=1) . &lt;tf.Tensor: shape=(2, 6, 4, 5), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59]], [[ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119]], [[ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]], dtype=int32)&gt; . - (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,8,5) . 세번째 // axis=2 | . a=tf.reshape(tf.constant(range(120)),(2,3,4,5)) b=-a . tf.concat([a,b],axis=2) . &lt;tf.Tensor: shape=(2, 3, 8, 5), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19], [ 0, -1, -2, -3, -4], [ -5, -6, -7, -8, -9], [ -10, -11, -12, -13, -14], [ -15, -16, -17, -18, -19]], [[ 20, 21, 22, 23, 24], [ 25, 26, 27, 28, 29], [ 30, 31, 32, 33, 34], [ 35, 36, 37, 38, 39], [ -20, -21, -22, -23, -24], [ -25, -26, -27, -28, -29], [ -30, -31, -32, -33, -34], [ -35, -36, -37, -38, -39]], [[ 40, 41, 42, 43, 44], [ 45, 46, 47, 48, 49], [ 50, 51, 52, 53, 54], [ 55, 56, 57, 58, 59], [ -40, -41, -42, -43, -44], [ -45, -46, -47, -48, -49], [ -50, -51, -52, -53, -54], [ -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64], [ 65, 66, 67, 68, 69], [ 70, 71, 72, 73, 74], [ 75, 76, 77, 78, 79], [ -60, -61, -62, -63, -64], [ -65, -66, -67, -68, -69], [ -70, -71, -72, -73, -74], [ -75, -76, -77, -78, -79]], [[ 80, 81, 82, 83, 84], [ 85, 86, 87, 88, 89], [ 90, 91, 92, 93, 94], [ 95, 96, 97, 98, 99], [ -80, -81, -82, -83, -84], [ -85, -86, -87, -88, -89], [ -90, -91, -92, -93, -94], [ -95, -96, -97, -98, -99]], [[ 100, 101, 102, 103, 104], [ 105, 106, 107, 108, 109], [ 110, 111, 112, 113, 114], [ 115, 116, 117, 118, 119], [-100, -101, -102, -103, -104], [-105, -106, -107, -108, -109], [-110, -111, -112, -113, -114], [-115, -116, -117, -118, -119]]]], dtype=int32)&gt; . - (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,4,10) . 네번째 // axis=3 # 0,1,2,3 // -4 -3 -2 -1 | . a=tf.reshape(tf.constant(range(120)),(2,3,4,5)) b=-a . tf.concat([a,b],axis=-1) . &lt;tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy= array([[[[ 0, 1, 2, 3, 4, 0, -1, -2, -3, -4], [ 5, 6, 7, 8, 9, -5, -6, -7, -8, -9], [ 10, 11, 12, 13, 14, -10, -11, -12, -13, -14], [ 15, 16, 17, 18, 19, -15, -16, -17, -18, -19]], [[ 20, 21, 22, 23, 24, -20, -21, -22, -23, -24], [ 25, 26, 27, 28, 29, -25, -26, -27, -28, -29], [ 30, 31, 32, 33, 34, -30, -31, -32, -33, -34], [ 35, 36, 37, 38, 39, -35, -36, -37, -38, -39]], [[ 40, 41, 42, 43, 44, -40, -41, -42, -43, -44], [ 45, 46, 47, 48, 49, -45, -46, -47, -48, -49], [ 50, 51, 52, 53, 54, -50, -51, -52, -53, -54], [ 55, 56, 57, 58, 59, -55, -56, -57, -58, -59]]], [[[ 60, 61, 62, 63, 64, -60, -61, -62, -63, -64], [ 65, 66, 67, 68, 69, -65, -66, -67, -68, -69], [ 70, 71, 72, 73, 74, -70, -71, -72, -73, -74], [ 75, 76, 77, 78, 79, -75, -76, -77, -78, -79]], [[ 80, 81, 82, 83, 84, -80, -81, -82, -83, -84], [ 85, 86, 87, 88, 89, -85, -86, -87, -88, -89], [ 90, 91, 92, 93, 94, -90, -91, -92, -93, -94], [ 95, 96, 97, 98, 99, -95, -96, -97, -98, -99]], [[ 100, 101, 102, 103, 104, -100, -101, -102, -103, -104], [ 105, 106, 107, 108, 109, -105, -106, -107, -108, -109], [ 110, 111, 112, 113, 114, -110, -111, -112, -113, -114], [ 115, 116, 117, 118, 119, -115, -116, -117, -118, -119]]]], dtype=int32)&gt; . - (4,) concat (4,) =&gt; (8,) . 첫번째축? // axis=0 | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.concat([a,b],axis=0) . &lt;tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1, 2, 3, 4, -1, -2, -3, -4], dtype=int32)&gt; . - (4,) concat (4,) =&gt; (4,2) . 두번째축? // axis=1 ==&gt; 이런거없다.. | . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.concat([a,b],axis=1) . InvalidArgumentError Traceback (most recent call last) Input In [80], in &lt;cell line: 1&gt;() -&gt; 1 tf.concat([a,b],axis=1) File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb File ~/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7107, in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None InvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat . tf.stack . a=tf.constant([1,2,3,4]) b=-a a,b . (&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;) . tf.stack([a,b],axis=0) . &lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy= array([[ 1, 2, 3, 4], [-1, -2, -3, -4]], dtype=int32)&gt; . tf.stack([a,b],axis=1) . &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy= array([[ 1, -1], [ 2, -2], [ 3, -3], [ 4, -4]], dtype=int32)&gt; . tnp . - tf는 넘파이에 비하여 텐서만들기가 너무힘듬 . np.diag([1,2,3]).reshape(-1) . array([1, 0, 0, 0, 2, 0, 0, 0, 3]) . 넘파이는 이런식으로 np.diag()도 쓸수 있고 reshape을 메소드로 쓸 수도 있는데... | . tnp &#49324;&#50857;&#48169;&#48277; (&#48520;&#47564;&#54644;&#44208;&#48169;&#48277;) . import tensorflow.experimental.numpy as tnp tnp.experimental_enable_numpy_behavior() . type(tnp.array([1,2,3])) . tensorflow.python.framework.ops.EagerTensor . - int와 float을 더할 수 있음 . tnp.array([1,2,3])+tnp.array([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt; . tf.constant([1,2,3])+tf.constant([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt; . tnp.array(1)+tnp.array([1.0,2.0,3.0]) . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 3., 4.])&gt; . tnp.diag([1,2,3]) . &lt;tf.Tensor: shape=(3, 3), dtype=int64, numpy= array([[1, 0, 0], [0, 2, 0], [0, 0, 3]])&gt; . a=tnp.diag([1,2,3]) type(a) . tensorflow.python.framework.ops.EagerTensor . a=tf.constant([1,2,3]) a.reshape(3,1) . &lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy= array([[1], [2], [3]], dtype=int32)&gt; . &#49440;&#50616;&#44256;&#44553; . np.random.randn(5) . array([0.67533519, 0.18494521, 0.76946432, 0.94461951, 1.15058192]) . tnp.random.randn(5) # 넘파이가 되면 나도 된다. . &lt;tf.Tensor: shape=(5,), dtype=float64, numpy=array([-0.37112581, -0.31535817, -0.92963552, -0.68741888, -0.54859424])&gt; . &#53440;&#51077; . type(tnp.random.randn(5)) . tensorflow.python.framework.ops.EagerTensor . tf.contant&#47196; &#47564;&#46308;&#50612;&#46020; &#47560;&#52824; &#45336;&#54028;&#51060;&#51064;&#46319; &#50416;&#45716; &#44592;&#45733;&#46308; . - 묵시적형변환이 가능 . tf.constant([1,1])+tf.constant([2.2,3.3]) . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([3.20000005, 4.29999995])&gt; . - 메소드를 쓸수 있음. . a= tnp.array([[1,2,3,4]]) a.T . &lt;tf.Tensor: shape=(4, 1), dtype=int64, numpy= array([[1], [2], [3], [4]])&gt; . &#44536;&#47111;&#51648;&#47564; np.array&#45716; &#50500;&#45784; . - 원소를 할당하는것은 불가능 . a=tf.constant([1,2,3]) a . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt; . a[0]=11 . TypeError Traceback (most recent call last) Input In [101], in &lt;cell line: 1&gt;() -&gt; 1 a[0]=11 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment .",
            "url": "https://guebin.github.io/STBDA2022/2022/03/14/(2%EC%A3%BC%EC%B0%A8)-3%EC%9B%9414%EC%9D%BC.html",
            "relUrl": "/2022/03/14/(2%EC%A3%BC%EC%B0%A8)-3%EC%9B%9414%EC%9D%BC.html",
            "date": " • Mar 14, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "(1주차) 3월7일",
            "content": "&#44053;&#51032;&#50689;&#49345; . . &#44053;&#51032;&#48372;&#52649;&#51088;&#47308; . - https://github.com/guebin/STBDA2022/blob/master/_notebooks/2022-03-07-supp1.pdf . - https://github.com/guebin/STBDA2022/blob/master/_notebooks/2022-03-07-supp2.pdf . &#47196;&#46300;&#47605; . - 오늘수업할내용: 단순선형회귀 . - 단순선형회귀를 배우는 이유? . 우리가 배우고싶은것: 심층신경망(DNN) $ to$ 합성곱신경망(CNN) $ to$ 적대적생성신경망(GAN) | 심층신경망을 바로 이해하기 어려움 | 다음의 과정으로 이해해야함: (선형대수학 $ to$) 회귀분석 $ to$ 로지스틱회귀분석 $ to$ 심층신경망 | . &#49440;&#54805;&#54924;&#44480; . - 상황극 . 나는 동네에 커피점을 하나 차렸음. | 장사를 하다보니까 날이 더울수록 아이스아메리카노의 판매량이 증가한다는 사실을 깨달았다. | 일기예보는 미리 나와있으니까 그 정보를 잘 이용하면 &#39;온도 -&gt; 아이스아메리카노 판매량 예측&#39; 이 가능할것 같다. (내가 앞으로 얼마나 벌지 예측가능) | . - 가짜자료 생성 . import matplotlib.pyplot as plt import tensorflow as tf . 온도 ${ bf x}$가 아래와 같다고 하자. . x=tf.constant([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) # 기온 x . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4], dtype=float32)&gt; . 아이스아메리카노의 판매량 ${ bf y}$이 아래와 같다고 하자. (판매량은 정수로 나오겠지만 편의상 소수점도 가능하다고 생각하자) . $${ bf y} approx 10.2 +2.2 { bf x}$$ . 여기에서 10.2, 2.2 의 숫자는 제가 임의로 정한것임 | 식의의미: 온도가 0일때 10.2잔정도 팔림 + 온도가 1도 증가하면 2.2잔정도 더 팔림 | 물결의의미: 현실반영. 세상은 꼭 수식대로 정확하게 이루어지지 않음. | . tf.random.set_seed(43052) epsilon=tf.random.normal([10]) y=10.2 + 2.2*x + epsilon y . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([55.418365, 58.194283, 61.230827, 62.312557, 63.107002, 63.69569 , 67.247055, 71.4365 , 73.1013 , 77.84988 ], dtype=float32)&gt; . - 우리는 아래와 같은 자료를 모았다고 생각하자. . tf.transpose(tf.concat([[x],[y]],0)) . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . - 그려보자. . plt.plot(x,y,&#39;.&#39;) # 파란점, 관측한 데이터 plt.plot(x,10.2 + 2.2*x, &#39;--&#39;) # 주황색점선, 세상의 법칙 . [&lt;matplotlib.lines.Line2D at 0x7f114c62d7e0&gt;] . - 우리의 목표: 파란색점 $ to$ 주황색점선을 추론 // 데이터를 바탕으로 세상의 법칙을 추론 . - 아이디어: 데이터를 보니까 $x$와 $y$가 선형의 관계에 있는듯 보인다. 즉 모든 $i=1,2, dots, 10$에 대하여 아래를 만족하는 적당한 a,b (혹은 $ beta_0, beta_1$) 가 존재할것 같다. . $y_{i} approx ax_{i}+b$ | $y_{i} approx beta_1 x_{i}+ beta_0$ | . - 어림짐작으로 $a,b$를 알아내보자. . 데이터를 살펴보자. . tf.transpose(tf.concat([[x],[y]],0)) . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[20.1 , 55.418365], [22.2 , 58.194283], [22.7 , 61.230827], [23.3 , 62.312557], [24.4 , 63.107002], [25.1 , 63.69569 ], [26.2 , 67.247055], [27.3 , 71.4365 ], [28.4 , 73.1013 ], [30.4 , 77.84988 ]], dtype=float32)&gt; . 적당히 왼쪽*2+15 = 오른쪽의 관계가 성립하는것 같다. . 따라서 $a=2, b=15$ 혹은 $ beta_0=15, beta_1=2$ 로 추론할 수 있겠다. . - 누군가가 $( beta_0, beta_1)=(14,2)$ 이라고 주장할 수 있다. (어차피 지금은 감각으로 추론하는 과정이니까) . - 새로운 주장으로 인해서 $( beta_0, beta_1)=(15,2)$ 로 볼 수도 있고 $( beta_0, beta_1)=(14,2)$ 로 볼 수도 있다. 이중에서 어떠한 추정치가 좋은지 판단할 수 있을까? . 후보1: $( beta_0, beta_1)=(15,2)$ | 후보2: $( beta_0, beta_1)=(14,2)$ | . - 가능한 $y_i approx beta_0 + beta_1 x_i$ 이 되도록 만드는 $( beta_0, beta_1)$ 이 좋을 것이다. $ to$ 후보 1,2를 비교해보자. . (관찰에 의한 비교) . 후보1에 대해서 $i=1,2$를 넣고 관찰하여 보자. . 20.1 * 2 + 15 , 55.418365 # i=1 . (55.2, 55.418365) . 22.2 * 2 + 15 , 58.194283 # i=2 . (59.4, 58.194283) . 후보2에 대하여 $i=1,2$를 넣고 관찰하여 보자. . 20.1 * 2 + 14 , 55.418365 # i=1 . (54.2, 55.418365) . 22.2 * 2 + 14 , 58.194283 # i=2 . (58.4, 58.194283) . $i=1$인 경우에는 후보1이 더 잘맞는것 같은데 $i=2$인 경우는 후보2가 더 잘맞는것 같다. . (좀 더 체계적인 비교) . $i=1,2,3, dots, 10$ 에서 후보1과 후보2중 어떤것이 더 좋은지 비교하는 체계적인 방법을 생각해보자. . 후보 1,2에 대하여 $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$를 계산하여 비교해보자. . sum1=0 for i in range(10): sum1=sum1+(y[i]-15-2*x[i])**2 . sum2=0 for i in range(10): sum2=sum2+(y[i]-14-2*x[i])**2 . sum1,sum2 . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=14.734169&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=31.521086&gt;) . 후보1이 더 $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$의 값이 작다. . 후보1이 종합적으로 후보2에 비하여 좋다. 이 과정을 무한번 반복하면 최적의 추정치를 찾을 수 있다. . - 그런데 이 알고리즘은 현실적으로 구현이 불가능하다. (무한번 계산하기도 힘들고, 언제 멈출지도 애매함) . - 수학을 이용해서 좀 더 체계적으로 찾아보자. 결국 아래식을 가장 작게 만드는 $ beta_0, beta_1$을 찾으면 된다. . $ sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2$ . 그런데 결국 $ beta_0, beta_1$에 대한 이차식인데 이 식을 최소화하는 $ beta_0, beta_1$을 구하기 위해서는 아래를 연립하여 풀면된다. . $ begin{cases} frac{ partial}{ partial beta_0} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 frac{ partial}{ partial beta_1} sum_{i=1}^{10} (y_i - beta_0 - beta_1 x_i)^2=0 end{cases}$ . - 풀어보자. . $ begin{cases} sum_{i=1}^{10} -2(y_i - beta_0 - beta_1 x_i)=0 sum_{i=1}^{10} -2x_i(y_i - beta_0 - beta_1 x_i)=0 end{cases}$ . 정리하면 . $$ hat{ beta}_0= bar{y}- hat{ beta}_1 bar{x}$$ . $$ hat{ beta}_1= frac{S_{xy}}{S_{xx}}= frac{ sum_{i=1}^{n}(x_i- bar{x})(y_i- bar{y})}{ sum_{i=1}^{n}(x_i- bar{x})^2}$$ . - 따라서 최적의 추정치 $( hat{ beta}_0, hat{ beta}_1)$를 이용한 추세선을 아래와 같이 계산할 수 있음. . Sxx= sum((x-sum(x)/10)**2) Sxx . &lt;tf.Tensor: shape=(), dtype=float32, numpy=87.84898&gt; . Sxy= sum((x-sum(x)/10)*(y-sum(y)/10)) Sxy . &lt;tf.Tensor: shape=(), dtype=float32, numpy=194.64737&gt; . beta1_estimated = Sxy/Sxx beta1_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157042&gt; . beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 beta0_estimated . &lt;tf.Tensor: shape=(), dtype=float32, numpy=9.94458&gt; . plt.plot(x,y,&#39;.&#39;) plt.plot(x,beta0_estimated + beta1_estimated * x, &#39;--&#39;) # 주황색선: 세상의 법칙을 추정한선 plt.plot(x,10.2 + 2.2* x, &#39;--&#39;) # 초록색선: ture, 세상의법칙 . [&lt;matplotlib.lines.Line2D at 0x7f1128716d70&gt;] . . Note: 샘플수가 커질수록 주황색선은 점점 초록색선으로 가까워진다. . - 꽤 훌륭한 도구임. 그런데 약간의 단점이 존재한다. . (1) 공식이 좀 복잡함.. . (2) $x$가 여러개일 경우 확장이 어려움 . - 단점을 극복하기 위해서 우리가 지금까지 했던논의를 매트릭스로 바꾸어서 다시 써보자. . - 모형의 매트릭스화 . 우리의 모형은 아래와 같다. . $y_i = beta_0 + beta_1 x_i + epsilon_i, quad i=1,2, dots,10$ . 풀어서 쓰면 . $ begin{cases} y_1 = beta_0 + beta_1 x_1 + epsilon_1 y_2 = beta_0 + beta_1 x_2 + epsilon_2 dots y_{10} = beta_0 + beta_1 x_{10} + epsilon_{10} end{cases}$ . 아래와 같이 쓸 수 있다. . $ begin{bmatrix} y_1 y_2 dots y_{10} end{bmatrix} = begin{bmatrix} 1 &amp; x_1 1 &amp; x_2 dots &amp; dots 1 &amp; x_{10} end{bmatrix} begin{bmatrix} beta_0 beta_1 end{bmatrix} + begin{bmatrix} epsilon_1 epsilon_2 dots epsilon_{10} end{bmatrix} $ . 벡터와 매트릭스 형태로 정리하면 . ${ bf y} = { bf X} { boldsymbol beta} + boldsymbol{ epsilon}$ . - 손실함수의 매트릭스화: 우리가 최소화 하려던 손실함수는 아래와 같다. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2$ . 이것을 벡터표현으로 하면 아래와 같다. . $loss= sum_{i=1}^{n}(y_i- beta_0- beta_1x_i)^2=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})$ . 풀어보면 . $loss=({ bf y}-{ bf X}{ boldsymbol beta})^ top({ bf y}-{ bf X}{ boldsymbol beta})={ bf y}^ top { bf y} - { bf y}^ top { bf X}{ boldsymbol beta} - { boldsymbol beta}^ top { bf X}^ top { bf y} + { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . - 미분하는 과정의 매트릭스화 . loss를 최소화하는 ${ boldsymbol beta}$를 구해야하므로 loss를 ${ boldsymbol beta}$로 미분한식을 0이라고 놓고 풀면 된다. . $ frac{ partial}{ partial boldsymbol{ beta}} loss = frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf y} - frac{ partial}{ partial boldsymbol{ beta}} { bf y}^ top { bf X}{ boldsymbol beta} - frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf y} + frac{ partial}{ partial boldsymbol{ beta}} { boldsymbol beta}^ top { bf X}^ top { bf X} { boldsymbol beta}$ . $= 0 - { bf X}^ top { bf y}- { bf X}^ top { bf y} + 2{ bf X}^ top { bf X}{ boldsymbol beta} $ . 따라서 $ frac{ partial}{ partial boldsymbol{ beta}}loss=0$을 풀면 아래와 같다. . $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ . - 공식도 매트릭스로 표현하면: $ boldsymbol{ hat beta}= ({ bf X}^ top { bf X})^{-1}{ bf X}^ top { bf y} $ &lt;-- 외우세요 . - 적용을 해보자. . (X를 만드는 방법1) . X=tf.transpose(tf.concat([[[1.0]*10],[x]],0)) # X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . (X를 만드는 방법2) . from tensorflow.python.ops.numpy_ops import np_config np_config.enable_numpy_behavior() . X=tf.concat([[[1.0]*10],[x]],0).T X . &lt;tf.Tensor: shape=(10, 2), dtype=float32, numpy= array([[ 1. , 20.1], [ 1. , 22.2], [ 1. , 22.7], [ 1. , 23.3], [ 1. , 24.4], [ 1. , 25.1], [ 1. , 26.2], [ 1. , 27.3], [ 1. , 28.4], [ 1. , 30.4]], dtype=float32)&gt; . tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([9.945175 , 2.2156773], dtype=float32)&gt; . - 잘 구해진다. . - 그런데.. . beta0_estimated,beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=9.94458&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.2157042&gt;) . 값이 좀 다르다..? . - 같은 값입니다! 신경쓰지 마세요! 텐서플로우가 좀 대충계산합니다. . import tensorflow.experimental.numpy as tnp . x=tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4]) y=10.2 + 2.2*x + epsilon . beta1_estimated = sum((x-sum(x)/10)*(y-sum(y)/10)) / sum((x-sum(x)/10)**2) beta0_estimated = sum(y)/10 - beta1_estimated * sum(x)/10 . beta0_estimated, beta1_estimated . (&lt;tf.Tensor: shape=(), dtype=float64, numpy=9.944573243234018&gt;, &lt;tf.Tensor: shape=(), dtype=float64, numpy=2.215704607783491&gt;) . X=tnp.concatenate([[tnp.array([1.0]*10)],[x]],0).T tf.linalg.inv(X.T @ X) @ X.T @ y . &lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([9.94457324, 2.21570461])&gt; . &#50526;&#51004;&#47196; &#54624;&#44163; . - 선형대수학의 미분이론.. . - 실습 (tensorflow에서 매트릭스를 자유롭게 다루비) .",
            "url": "https://guebin.github.io/STBDA2022/2022/03/07/(1%EC%A3%BC%EC%B0%A8)-3%EC%9B%947%EC%9D%BC.html",
            "relUrl": "/2022/03/07/(1%EC%A3%BC%EC%B0%A8)-3%EC%9B%947%EC%9D%BC.html",
            "date": " • Mar 7, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "최규빈 . guebin@jbnu.ac.kr | 자연과학대학교 본관 205호 | 카카오톡 오픈채널 1 | . 2022년 1학기 종료후 폐쇄예정 &#8617; . |",
          "url": "https://guebin.github.io/STBDA2022/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://guebin.github.io/STBDA2022/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}